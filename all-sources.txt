// src/app.ts
// src/app.ts - FIXED: Dynamic host configuration
import express from "express";
import helmet from "helmet";
import { getConfigAsync } from "./config/envConfig";
import { validateConfig } from "./config/config-validator";
import { initServiceContainer } from "./services/serviceContainer";
import { createAuthenticateJWT } from "./middlewares/authenticateJWT";
import { authorizeAdmin } from "./middlewares";
import authRoutes from "./routes/authRoutes";
import emailPublicRoutes from "./services/email/publicRoutes";
import pdfRoutes from "./services/pdf/routes";
import zplRoutes from "./services/zpl/routes";
import logger from "./utils/logging";
import gdprRoutes from "./routes/gdprRoutes";
import { initGDPRService } from "./services/gdpr/gdprTokenService";

const startApp = async () => {
  const config = await getConfigAsync();
  logger.system("Config loaded successfully", {
    config_source: config.config_source || 'unknown',
    has_required_fields: !!(config.clientId && config.senderEmail && config.jwtSecret)
  });

  validateConfig(config);
  logger.system("Config validation passed", {
    validated_fields: ['clientId', 'clientSecret', 'tenantId', 'senderEmail'],
    config_mode: config.config_source
  });

  await initServiceContainer(config);
  logger.container("Service container initialized successfully", {
    email_config: !!config.senderEmail,
    pdf_config: !!config.certPdfSignPath,
    jwt_config: !!config.jwtSecret
  });

  // After initServiceContainer:
  initGDPRService();
  logger.system("GDPR service initialized", {
    default_token_expiry: '24 hours',
    cleanup_interval: '1 hour'
  });

  logger.startMetrics(30000); // Start metrics collection every 30 seconds
  logger.system("Metrics collection started", {
    interval_ms: 30000,
    metrics_file: 'core-services-metrics-*.log'
  });

  // Create JWT middleware with loaded config
  const authenticateJWT = createAuthenticateJWT(config.jwtSecret);
  logger.auth("JWT middleware created", {
    jwt_secret_length: config.jwtSecret?.length || 0
  });

  const app = express();
  app.use(helmet());
  app.use(express.json());

  // Auth routes - for login capability
  app.use("/auth", authRoutes);

  // EMAIL ROUTES - TESTING MODE (authentication disabled)
  app.use("/api/email", /*authenticateJWT, authorizeAdmin,*/ emailPublicRoutes);
  app.use("/api/gdpr", gdprRoutes);

  // Public routes for pdf and zpl services - TESTING MODE (authentication disabled)
  app.use("/generate-pdf", /*authenticateJWT, authorizeAdmin,*/ pdfRoutes);
  app.use("/generate-zpl", /*authenticateJWT, authorizeAdmin,*/ zplRoutes);

  app.get("/health", (_, res) => {
    res.status(200).send("OK");
  });

  logger.system("Express server configuration completed", {
    routes: ['/auth', '/api/email', '/api/gdpr', '/generate-pdf', '/generate-zpl', '/health'],
    middleware: ['helmet', 'express.json'],
    authentication: 'DISABLED_FOR_TESTING'
  });

  // FIXED: Dynamic host and port configuration
  const PORT = config.servicesPort || 3001;
  const HOST = process.env.HOST || '0.0.0.0'; // Default to listen on all interfaces
  
  // Build dynamic base URL for API endpoints
  const getBaseUrl = () => {
    // Priority: explicit config > environment > fallback
    if (config.coreApiHost) {
      return config.coreApiHost;
    }
    
    if (process.env.API_BASE_URL) {
      return process.env.API_BASE_URL;
    }
    
    // Fallback for development
    const protocol = process.env.NODE_ENV === 'production' ? 'https' : 'http';
    const hostname = process.env.HOSTNAME || 'localhost';
    return `${protocol}://${hostname}`;
  };

  const baseUrl = getBaseUrl();
  const fullApiUrl = `${baseUrl}:${PORT}`;

  app.listen(PORT, HOST, () => {
    logger.system(`Core Services API started successfully`, {
      host: HOST,
      port: PORT,
      base_url: baseUrl,
      full_api_url: fullApiUrl,
      listen_address: `${HOST}:${PORT}`,
      api_endpoints: {
        health: `${fullApiUrl}/health`,
        auth: `${fullApiUrl}/auth/login`,
        email: `${fullApiUrl}/api/email/send-with-consent`,
        pdf: `${fullApiUrl}/generate-pdf`,
        zpl: `${fullApiUrl}/generate-zpl`,
        gdpr: `${fullApiUrl}/api/gdpr/generate-token`
      },
      configuration: {
        environment: process.env.NODE_ENV || 'development',
        config_source: config.config_source,
        api_host_from: config.coreApiHost ? 'config' : 
                      process.env.API_BASE_URL ? 'env_API_BASE_URL' : 
                      'fallback'
      },
      features: [
        'ISO 27001 compliant emails',
        'PDF generation with signing',
        'ZPL label generation',
        'GDPR consent management'
      ],
      ready: true
    });
  });
};

logger.system("Starting Core Services application", {
  node_version: process.version,
  platform: process.platform,
  environment: process.env.NODE_ENV || 'development',
  hostname: process.env.HOSTNAME || 'localhost',
  host_binding: process.env.HOST || '0.0.0.0'
});

startApp().catch(error => {
  logger.error("Failed to start Core Services application", {
    error: error.message,
    stack: error.stack
  });
  process.exit(1);
});
// src/config/browserPool.ts
// src/config/browserPool.ts
import puppeteer, { Browser, Page } from 'puppeteer';
import fs from 'fs';
import os from 'os';
import logger from '../utils/logging';
import { BrowserPoolConfig  } from '../services/serviceContainer'; // O como obtengas el contenedor de servicios

// vars to config del container
let MAX_BROWSERS: number;
let MAX_PAGES_PER_BROWSER: number;
let PAGE_IDLE_TIMEOUT: number;

interface PageInfo {
  page: Page;
  lastUsed: number;
  browser: Browser;
}

const browsers: Browser[] = [];
const availablePages: PageInfo[] = [];
let cleanupInterval: NodeJS.Timeout | null = null;

function getExecutablePath(): string | undefined {
  const platform = os.platform();
  if (platform === 'linux') {
    const chromePath = '/usr/bin/google-chrome';
    if (fs.existsSync(chromePath)) return chromePath;
  }
  return undefined;
}

export async function initializeBrowserPool(config?: BrowserPoolConfig) {

  // Load configuration from environment variables or provided config
  MAX_BROWSERS = config?.maxBrowsers || parseInt(process.env.MAX_BROWSERS || '2');
  MAX_PAGES_PER_BROWSER = config?.maxPagesPerBrowser || parseInt(process.env.MAX_PAGES_PER_BROWSER || '3');
  PAGE_IDLE_TIMEOUT = config?.pageIdleTimeout || parseInt(process.env.PAGE_IDLE_TIMEOUT || '300000');
  
  logger.system('Initializing browser pool', {
    max_browsers: MAX_BROWSERS,
    max_pages_per_browser: MAX_PAGES_PER_BROWSER,
    page_idle_timeout_ms: PAGE_IDLE_TIMEOUT
  });

  // validate configuration
  for (let i = 0; i < MAX_BROWSERS; i++) {
    
    try {
      const browser = await puppeteer.launch({
        headless: 'new',
        args: [
          '--no-sandbox', 
          '--disable-setuid-sandbox',
          '--disable-dev-shm-usage', // Evita problemas de memoria compartida
          '--disable-gpu',
          '--no-first-run',
          '--no-default-browser-check',
          '--disable-default-apps'
        ],
        executablePath: getExecutablePath(),
      });
      
      browsers.push(browser);
      
      logger.system(`Browser ${i + 1} initialized`, {
        browser_index: i,
        total_browsers: browsers.length
      });
    } catch (error) {
      logger.error(`Failed to initialize browser ${i + 1}`, {
        browser_index: i,
        error: (error as Error).message
      });
      throw error;
    }
  }
  
  // Cleanup idle pages periodically
  startPageCleanup();
  
  logger.system('Browser pool initialization completed', {
    browsers_created: browsers.length,
    cleanup_interval_ms: 60000
  });
}

export async function acquirePage(): Promise<Page> {
  const startTime = Date.now();
  
  //Try to acquire a page from the pool
  if (availablePages.length > 0) {
    const pageInfo = availablePages.pop()!;
    pageInfo.lastUsed = Date.now();
    
    logger.pdf('Page acquired from pool', {
      duration_ms: Date.now() - startTime,
      pages_remaining: availablePages.length,
      source: 'pool'
    });
    
    return pageInfo.page;
  }

  // Create a new page if we have not reached the maximum limit
  const totalPages = getTotalActivePages();
  const maxTotalPages = MAX_BROWSERS * MAX_PAGES_PER_BROWSER;
  
  if (totalPages < maxTotalPages) {
    const browser = getLeastLoadedBrowser();
    const page = await browser.newPage();
    
    // Setup default timeouts
    page.setDefaultTimeout(30000);
    page.setDefaultNavigationTimeout(30000);
    
    logger.pdf('New page created', {
      duration_ms: Date.now() - startTime,
      total_active_pages: totalPages + 1,
      max_pages: maxTotalPages,
      source: 'new'
    });
    
    return page;
  }

  // if we reach here, it means we have no available pages and reached the max limit
  logger.warn('Page pool exhausted, waiting for available page', {
    total_active_pages: totalPages,
    max_pages: maxTotalPages,
    available_pages: availablePages.length
  });
  
  //wait for an available page
  const maxWaitTime = 30000;
  const pollInterval = 100;
  let waitTime = 0;
  
  while (waitTime < maxWaitTime) {
    if (availablePages.length > 0) {
      const pageInfo = availablePages.pop()!;
      pageInfo.lastUsed = Date.now();
      
      logger.pdf('Page acquired after waiting', {
        duration_ms: Date.now() - startTime,
        wait_time_ms: waitTime,
        source: 'pool_after_wait'
      });
      
      return pageInfo.page;
    }
    
    await new Promise(resolve => setTimeout(resolve, pollInterval));
    waitTime += pollInterval;
  }
  
  // if there are still no available pages after waiting, create a new one
  logger.error('Page pool timeout, forcing new page creation', {
    wait_time_ms: waitTime,
    total_active_pages: getTotalActivePages()
  });
  
  const browser = getLeastLoadedBrowser();
  const page = await browser.newPage();
  page.setDefaultTimeout(30000);
  page.setDefaultNavigationTimeout(30000);
  
  return page;
}

export async function releasePage(page: Page) {
  const startTime = Date.now();
  
  try {
    // Clean up the page before releasing it
    await page.goto('about:blank', { waitUntil: 'domcontentloaded', timeout: 5000 });
    
    // CLean up cookies
    await page.evaluate(() => {
      // Clean up session storage
      if (typeof Storage !== 'undefined') {
        localStorage.clear();
        sessionStorage.clear();
      }
    });
    
    // Find the browser instance for this page
    const browser = page.browser();
    
    // Add a new entry to the available pages pool
    if (availablePages.length < MAX_PAGES_PER_BROWSER * MAX_BROWSERS) {
      availablePages.push({
        page,
        lastUsed: Date.now(),
        browser
      });
      
      logger.pdf('Page released to pool', {
        duration_ms: Date.now() - startTime,
        available_pages: availablePages.length,
        status: 'pooled'
      });
    } else {
      // If we have reached the maximum number of pages, close the page
      await page.close();
      
      logger.pdf('Page closed (pool full)', {
        duration_ms: Date.now() - startTime,
        available_pages: availablePages.length,
        status: 'closed'
      });
    }
    
  } catch (error) {
    // if cleanup fails, log the error and close the page
    try {
      await page.close();
      logger.warn('Page cleanup failed, page closed', {
        duration_ms: Date.now() - startTime,
        error: (error as Error).message,
        status: 'closed_after_error'
      });
    } catch (closeError) {
      logger.error('Failed to close page after cleanup error', {
        duration_ms: Date.now() - startTime,
        cleanup_error: (error as Error).message,
        close_error: (closeError as Error).message
      });
    }
  }
}

/**
 * Get the least loaded browser from the pool
 * This is a simple round-robin selection for now.
 * In the future, we could implement a more sophisticated load balancing strategy.
 */
function getLeastLoadedBrowser(): Browser {
  if (browsers.length === 0) {
    throw new Error('No browsers available in pool');
  }
  
  // rotate through browsers to balance load
  return browsers[Math.floor(Math.random() * browsers.length)];
}

/**
 * Count total active pages in the pool
 */
function getTotalActivePages(): number {
  // this is a simple count of all pages in the availablePages array
  return availablePages.length;
}

/**
 * Cleanup idle pages periodically
 */
function startPageCleanup() {
  if (cleanupInterval) return;
  
  cleanupInterval = setInterval(async () => {
    const now = Date.now();
    const pagesToClose: PageInfo[] = [];
    const pagesToKeep: PageInfo[] = [];
    
    // Separate pages into those to close and those to keep
    for (const pageInfo of availablePages) {
      if (now - pageInfo.lastUsed > PAGE_IDLE_TIMEOUT) {
        pagesToClose.push(pageInfo);
      } else {
        pagesToKeep.push(pageInfo);
      }
    }
    
    // Cerrar p√°ginas idle
    if (pagesToClose.length > 0) {
      logger.system('Cleaning up idle pages', {
        pages_to_close: pagesToClose.length,
        pages_to_keep: pagesToKeep.length,
        idle_timeout_ms: PAGE_IDLE_TIMEOUT
      });
      
      for (const pageInfo of pagesToClose) {
        try {
          await pageInfo.page.close();
        } catch (error) {
          logger.warn('Failed to close idle page', {
            error: (error as Error).message
          });
        }
      }
      
      // Update availablePages array
      availablePages.length = 0;
      availablePages.push(...pagesToKeep);
    }
    
  }, 60000); // Cleanup every 60 seconds
}

export async function closeAllBrowsers() {
  logger.system('Closing all browsers', {
    browsers_count: browsers.length,
    available_pages: availablePages.length
  });
  
  // Stop the cleanup interval if it's running
  if (cleanupInterval) {
    clearInterval(cleanupInterval);
    cleanupInterval = null;
  }
  
  // CLose all available pages
  for (const pageInfo of availablePages) {
    try {
      await pageInfo.page.close();
    } catch (error) {
      logger.warn('Failed to close page during shutdown', {
        error: (error as Error).message
      });
    }
  }
  availablePages.length = 0;
  
  // CLose all browsers
  for (const browser of browsers) {
    try {
      await browser.close();
    } catch (error) {
      logger.warn('Failed to close browser during shutdown', {
        error: (error as Error).message
      });
    }
  }
  browsers.length = 0;
  
  logger.system('All browsers closed successfully');
}

/**
 * Get statistics about the browser pool
 */
export function getBrowserPoolStats() {
  return {
    browsers: browsers.length,
    availablePages: availablePages.length,
    maxBrowsers: MAX_BROWSERS,
    maxPagesPerBrowser: MAX_PAGES_PER_BROWSER,
    pageIdleTimeout: PAGE_IDLE_TIMEOUT
  };
}
// src/config/config-validator.ts
import logger from '../utils/logging';

export function validateConfig(config: any): void {
  const requiredVars: { key: string; label: string }[] = [
    { key: "clientId", label: "CLIENT_ID" },
    { key: "clientSecret", label: "CLIENT_SECRET" },
    { key: "tenantId", label: "TENANT_CLIENT_ID" },
    { key: "senderEmail", label: "SENDER_EMAIL" },
  ];

  // SECURE: Only log config in verbose mode, and sanitize sensitive data
  logger.debug('Config validation started', {
    config_keys: config ? Object.keys(config) : null,
    config_size: config ? Object.keys(config).length : 0,
    // Only include non-sensitive config parts in verbose mode
    ...(logger.isVerbose() && {
      non_sensitive_config: {
        tenantClientId: config?.tenantClientId,
        coreApiHost: config?.coreApiHost,
        servicesPort: config?.servicesPort,
        backendPort: config?.backendPort,
        has_clientId: !!config?.clientId,
        has_clientSecret: !!config?.clientSecret,
        has_tenantId: !!config?.tenantId,
        has_senderEmail: !!config?.senderEmail,
        has_jwtSecret: !!config?.jwtSecret
      }
    })
  });

  // Handle null config (can happen in standalone mode if not properly loaded)
  if (!config) {
    logger.error("Configuration is null or undefined", {
      operation: 'SYSTEM',
      error_code: 'CONFIG_NULL'
    });
    process.exit(1);
  }

  const missing = requiredVars.filter(({ key }) => !config[key]);

  if (missing.length > 0) {
    const missingLabels = missing.map(({ label }) => label);
    
    logger.error("Missing required environment variables", {
      operation: 'SYSTEM',
      error_code: 'MISSING_ENV_VARS',
      missing_variables: missingLabels,
      missing_count: missing.length,
      total_required: requiredVars.length
    });
    
    process.exit(1);
  }

  // Success logging with secure details
  logger.system("Configuration validation successful", {
    required_vars_count: requiredVars.length,
    validated_keys: requiredVars.map(v => v.key),
    config_source: config.standalone_mode ? 'SOPS' : 'Environment',
    ...(logger.isVerbose() && {
      // Additional details only in verbose mode
      client_id_length: config.clientId?.length || 0,
      sender_email: config.senderEmail,
      tenant_id: config.tenantId
    })
  });
}
// src/config/envConfig.ts
// config/envConfig.ts - SIMPLIFIED VERSION - SOPS ONLY
import { spawn } from 'child_process';
import fs from 'fs';
import path from 'path';
import yaml from 'js-yaml';
import logger from "../utils/logging";

/**
 * SIMPLIFIED CONFIG LOADER - SOPS ONLY
 * 
 * This version removes all .env complexity and focuses on SOPS decryption.
 * Clean, simple, and maintainable.
 */

/**
 * Get client ID from CLI args or environment
 */
const getClientId = (): string => {
  const cliArgs = process.argv.slice(2);
  const cliClientId = cliArgs.find(arg => !arg.startsWith('--'));
  
  if (cliClientId) {
    logger.system(`Using CLIENT_ID from CLI: ${cliClientId}`);
    return cliClientId;
  }
  
  if (process.env.CLIENT_ID) {
    logger.system(`Using CLIENT_ID from ENV: ${process.env.CLIENT_ID}`);
    return process.env.CLIENT_ID;
  }
  
  logger.system(`Using default CLIENT_ID: core-dev`);
  return 'core-dev';
};

/**
 * Get GPG passphrase from environment or CLI argument
 */
const getGPGPassphrase = (): string => {
  if (process.env.GPG_PASSPHRASE) {
    logger.system('Using GPG passphrase from environment');
    return process.env.GPG_PASSPHRASE;
  }
  
  const passphraseArg = process.argv.find(arg => arg.startsWith('--gpg-passphrase='));
  if (passphraseArg) {
    logger.system('Using GPG passphrase from CLI argument');
    return passphraseArg.split('=')[1];
  }
  
  throw new Error('GPG passphrase not found. Set GPG_PASSPHRASE env var or use --gpg-passphrase=xxx');
};

/**
 * Get SOPS binary path for Windows
 */
const getSopsPath = (envsRepoPath: string): string => {
  const winPath = path.join(envsRepoPath, 'tools/win64/sops.exe');
  if (!fs.existsSync(winPath)) {
    throw new Error(`SOPS Windows binary not found at: ${winPath}`);
  }
  return winPath;
};

/**
 * Decrypt SOPS file using spawn for better process control
 */
const decryptSopsAsync = async (sopsPath: string, secretsPath: string, gpgPassphrase: string): Promise<string> => {
  logger.system('üîê Decrypting SOPS file...');
  
  return new Promise((resolve, reject) => {
    const gnupgHome = process.env.GNUPGHOME || path.join(process.env.APPDATA!, 'gnupg');
    
    logger.debug('SOPS Environment details', {
      sops_path: sopsPath,
      secrets_path: secretsPath,
      gnupg_home: gnupgHome,
      ...(logger.isVerbose() && {
        verbose_full_env: {
          GNUPGHOME: gnupgHome,
          GPG_TTY: process.platform === 'win32' ? undefined : '/dev/null',
          GPG_BATCH: '1'
        }
      })
    });
    
    const sopsProcess = spawn(sopsPath, [
      '-d', 
      '--output-type', 
      'json', 
      secretsPath
    ], {
      stdio: ['pipe', 'pipe', 'pipe'],
      env: { 
        ...process.env,
        GNUPGHOME: gnupgHome,
        GPG_PASSPHRASE: gpgPassphrase,
        GPG_TTY: process.platform === 'win32' ? undefined : '/dev/null',
        GPG_BATCH: '1'
      },
      shell: true,
      windowsHide: true
    });

    let stdout = '';
    let stderr = '';

    sopsProcess.stdout?.on('data', (data) => {
      stdout += data.toString();
    });

    sopsProcess.stderr?.on('data', (data) => {
      stderr += data.toString();
    });

    sopsProcess.on('close', (code) => {
      if (code === 0) {
        logger.system('SOPS decryption successful');
        resolve(stdout);
      } else {
        logger.error('SOPS decryption failed', {
          exit_code: code,
          stderr: stderr
        });
        reject(new Error(`SOPS decryption failed: ${stderr}`));
      }
    });

    sopsProcess.on('error', (error) => {
      logger.error('SOPS process error', {
        error: error.message
      });
      reject(error);
    });
  });
};

/**
 * Load configuration using SOPS only
 */
const loadConfig = async () => {
  console.log('üîß Loading config via SOPS');
  
  const clientId = getClientId();
  const gpgPassphrase = getGPGPassphrase();
  
  const envsRepoPath = path.resolve(__dirname, '../../../core-envs-private');
  
  if (!fs.existsSync(envsRepoPath)) {
    throw new Error(`core-envs-private repo not found at: ${envsRepoPath}`);
  }
  
  console.log(`üìÇ Using repo: ${envsRepoPath}`);
  
  try {
    // 1. Load config.yaml (public configuration)
    const yamlPath = path.join(envsRepoPath, `clients/${clientId}/config.yaml`);
    if (!fs.existsSync(yamlPath)) {
      throw new Error(`Client config.yaml not found: ${clientId}`);
    }
    
    const yamlFile = fs.readFileSync(yamlPath, 'utf8');
    const yamlParsed = yaml.load(yamlFile) as Record<string, any>;
    
    // Convert snake_case to camelCase for consistency
    const yamlConfig = Object.entries(yamlParsed).reduce((acc, [key, value]) => {
      const camelKey = key.replace(/_([a-z])/g, (_, c) => c.toUpperCase());
      acc[camelKey] = value;
      return acc;
    }, {} as Record<string, any>);
    
    console.log('‚úÖ config.yaml loaded');
    
    // 2. Decrypt secrets.sops.yaml
    const secretsPath = path.join(envsRepoPath, `clients/${clientId}/secrets.sops.yaml`);
    if (!fs.existsSync(secretsPath)) {
      throw new Error(`Client secrets.sops.yaml not found: ${clientId}`);
    }
    
    console.log(`üîê Decrypting secrets for: ${clientId}`);
    
    const sopsPath = getSopsPath(envsRepoPath);
    console.log(`üîß Using SOPS: ${sopsPath}`);
    
    const decryptOutput = await decryptSopsAsync(sopsPath, secretsPath, gpgPassphrase);
    const secretsConfig = JSON.parse(decryptOutput);
    console.log('‚úÖ Secrets decrypted successfully');
    
    // 3. Merge configs with proper field mapping
    const mergedConfig = {
      ...yamlConfig,
      ...secretsConfig,
      
      // Map snake_case to camelCase for key fields
      senderEmail: secretsConfig.sender_email ?? yamlConfig.senderEmail ?? '',
      clientId: secretsConfig.client_id ?? yamlConfig.clientId ?? '',
      clientSecret: secretsConfig.client_secret ?? yamlConfig.clientSecret ?? '',
      tenantId: secretsConfig.tenant_id ?? yamlConfig.tenantId ?? '',
      refreshToken: secretsConfig.refresh_token ?? yamlConfig.refreshToken ?? '',
      tenantClientId: secretsConfig.tenant_client_id ?? yamlConfig.tenantClientId ?? '',
      tokenEndpoint: secretsConfig.token_endpoint ?? yamlConfig.tokenEndpoint ?? 'https://login.microsoftonline.com',
      jwtSecret: secretsConfig.jwt_secret ?? yamlConfig.jwtSecret ?? '',
      internalJwtSecret: secretsConfig.internal_jwt_secret ?? yamlConfig.internalJwtSecret ?? '',
      authUsername: secretsConfig.auth_username ?? yamlConfig.authUsername,
      authPassword: secretsConfig.auth_password ?? yamlConfig.authPassword,
      
      // Browser Pool Configuration
      maxBrowsers: yamlConfig.maxBrowsers ?? secretsConfig.max_browsers ?? 2,
      maxPagesPerBrowser: yamlConfig.maxPagesPerBrowser ?? secretsConfig.max_pages_per_browser ?? 3,
      pageIdleTimeout: yamlConfig.pageIdleTimeout ?? secretsConfig.page_idle_timeout ?? 300000,      

      // Build URLs from config
      coreApiHost: yamlConfig.coreApiHost ?? '',
      servicesPort: yamlConfig.servicesPort ?? '',
      authUrl: yamlConfig.authUrl ?? '',
      backendUrl: yamlConfig.backendUrl ?? '',
      apiUrl: `${yamlConfig.coreApiHost}:${yamlConfig.servicesPort}${yamlConfig.backendUrl}`,
      authFullUrl: `${yamlConfig.coreApiHost}:${yamlConfig.servicesPort}${yamlConfig.authUrl}`,
      
      // Certificate configuration
      certPdfSignType: yamlConfig.certPdfSignType ?? 'p12',
      certPdfSignPath: secretsConfig.cert_pdf_sign_path ?? yamlConfig.certPdfSignPath ?? '',
      certPdfSignPassword: secretsConfig.cert_pdf_sign_password ?? yamlConfig.certPdfSignPassword ?? '',

      
      
      // Metadata
      config_source: 'SOPS_ONLY'
    };
    
    console.log('‚úÖ Config loaded successfully via SOPS');
    
    return mergedConfig;
    
  } catch (error) {
    console.error('‚ùå Config loading failed:', (error as Error).message);
    throw error;
  }
};

// Exports
export const getConfig = loadConfig;
export const getConfigAsync = loadConfig;
export default null;
// src/config/internalToken.ts
import jwt from 'jsonwebtoken';

const SECRET = process.env.JWT_SECRET || 'default_secret_dangerous';

export const getInternalToken = (): string => {
  return jwt.sign(
    { username: 'core_services', role: 'admin' },
    SECRET,
    { expiresIn: '1h' }
  );
};

// src/config/paths.ts
// config/paths.ts
import path from 'path';

/**
 * Static paths configuration
 * These paths are fixed and don't depend on client configuration
 */
export const paths = {
  pdf: {
    templatePath: path.resolve(__dirname, '../../../reports_templates/templates'),
    cssPath: path.resolve(__dirname, '../../../reports_templates/css')
  },
  zpl: {
    templatePath: path.resolve(__dirname, '../../../reports_templates/templates')
  }
};
// src/controllers/authController.ts
import { Request, Response } from 'express';
import jwt from 'jsonwebtoken';
import bcrypt from 'bcrypt';
import rawUsers from '../config/users.json';
import { getServiceContainer } from '../services/serviceContainer';

// Define the shape of each user record
interface UserRecord {
  password: string;  // hashed password
  role: string;      // user role, e.g., 'admin', 'viewer'
}

// Apply type to the users object
const users: Record<string, UserRecord> = rawUsers;

export const login = async (req: Request, res: Response) => {
  const { username, password } = req.body;

  // Validate input presence
  if (!username || !password) {
    return res.status(400).json({ error: 'Username and password are required' });
  }

  const user = users[username];

  // Check if user exists
  if (!user) {
    return res.status(401).json({ error: 'Invalid credentials' });
  }

  // Validate password using bcrypt
  const passwordMatch = await bcrypt.compare(password, user.password);
  if (!passwordMatch) {
    return res.status(401).json({ error: 'Invalid credentials' });
  }

  try {
    // Get JWT secret from service container (works in both modes)
    const container = getServiceContainer();
    const jwtSecret = container.getJwtSecret();

    // Generate JWT token with role using the same secret as middleware
    const token = jwt.sign({ username, role: user.role }, jwtSecret, { expiresIn: '15m' });

    // Return the token to the client
    res.json({ token });
    
  } catch (error) {
    console.error('Failed to generate JWT token:', error);
    return res.status(500).json({ error: 'Failed to generate authentication token' });
  }
};
// src/controllers/email/abacSend.ts
// src/controllers/email/abacSend.ts

import { Request, Response, NextFunction } from "express";
import { enforceEmailPolicy } from "../../services/pep";
import { EmailParams } from "../../services/email/emailService";
import { sendEmailWithConfig } from "../../services/email/emailServiceHelpers";
import {
  signPDFAttachments,
  getSignedAttachments,
  validateSigningResults,
} from "../../services/email/pdfSigningService";
import logger from "../../utils/logging";
import { v4 as uuidv4 } from "uuid";
import {
  ISO27001Classification,
  getSecurityControls,
} from "../../types/iso27001";

/**
 * ISO 27001 Compliant ABAC Email Send Controller
 *
 * This controller implements Zero Trust architecture with ISO 27001 compliance:
 * - A.8.2.1: Information classification handling
 * - A.9.4.1: Information access restriction
 * - A.12.4.1: Event logging and audit trail
 * - A.13.2.1: Information transfer policies
 * - A.13.2.3: Electronic messaging with digital signatures
 *
 * Security Flow based on ISO classification:
 * - internal: Single GDPR validation
 * - confidential: Double GDPR validation
 * - restricted: Double validation + digital PDF signing
 *
 * Zero Trust Principle: Don't trust the caller, don't trust yourself.
 */
export const abacSend = async (
  req: Request,
  res: Response,
  next: NextFunction
) => {
  const trace_id = uuidv4();
  const gdpr_token = req.headers["gdpr-token"] || req.body?.gdpr_token;
  const classification: ISO27001Classification =
    req.body?.classification || "restricted";

  logger.info("ISO 27001 compliant ABAC email process started", {
    trace_id,
    step: "PROCESS_START",
    classification,
    has_gdpr_token: !!gdpr_token,
    iso_control: "A.8.2.1", // Information classification
  });

  // Validate GDPR token presence (A.9.4.1 - Information access restriction)
  if (typeof gdpr_token !== "string") {
    logger.warn("Missing or invalid gdpr_token", {
      trace_id,
      step: "GDPR_TOKEN_VALIDATION_FAILED",
      iso_control: "A.9.4.1",
    });

    return res.status(400).json({
      trace_id,
      error: "Missing or invalid gdpr_token",
      iso_control: "A.9.4.1",
    });
  }

  // Get ISO 27001 security controls for this classification level
  const securityControls = getSecurityControls(classification);

  logger.info("ISO 27001 security controls determined", {
    trace_id,
    classification,
    security_controls: securityControls,
    iso_control: "A.8.2.1",
  });

  try {
    // STEP 1: First GDPR validation (A.9.4.1 - Information access restriction)
    logger.info("Starting first GDPR validation", {
      trace_id,
      step: "FIRST_VALIDATION_START",
      iso_control: "A.9.4.1",
    });

    const firstValidation = enforceEmailPolicy(req.body, gdpr_token);

    if (!firstValidation.allowed) {
      logger.warn("First validation failed - Original payload rejected", {
        trace_id,
        step: "FIRST_VALIDATION_FAILED",
        reason: firstValidation.reason,
        hash: firstValidation.hash,
        iso_control: "A.9.4.1",
      });

      return res.status(403).json({
        trace_id,
        error: "Email not allowed by policy (first validation)",
        reason: firstValidation.reason,
        iso_control: "A.9.4.1",
      });
    }

    logger.info("First validation passed - Original payload approved", {
      trace_id,
      step: "FIRST_VALIDATION_SUCCESS",
      hash: firstValidation.hash,
      iso_control: "A.9.4.1",
    });

    let finalPayload = req.body;
    let secondValidation = firstValidation; // Default for 'internal' classification

    // STEP 2: PDF Signing and Double Validation (based on ISO classification)
    if (securityControls.electronicMessaging) {
      // A.13.2.3 - Electronic messaging: Digital signatures required for 'restricted'
      logger.info(
        "Starting PDF signing process for restricted classification",
        {
          trace_id,
          step: "PDF_SIGNING_START",
          attachment_count: req.body.attachments?.length || 0,
          iso_control: "A.13.2.3",
        }
      );

      finalPayload = await createPayloadWithSignedPDFs(req.body, trace_id);

      logger.info("PDF signing completed", {
        trace_id,
        step: "PDF_SIGNING_SUCCESS",
        signed_attachment_count: finalPayload.attachments?.length || 0,
        iso_control: "A.13.2.3",
      });
    }

    if (securityControls.informationTransfer) {
      // A.13.2.1 - Information transfer: Double validation for 'confidential' and 'restricted'
      logger.info("Starting second GDPR validation for enhanced security", {
        trace_id,
        step: "SECOND_VALIDATION_START",
        iso_control: "A.13.2.1",
      });

      secondValidation = enforceEmailPolicy(finalPayload, gdpr_token);

      if (!secondValidation.allowed) {
        logger.error("Second validation failed - Enhanced payload rejected", {
          trace_id,
          step: "SECOND_VALIDATION_FAILED",
          reason: secondValidation.reason,
          hash: secondValidation.hash,
          original_hash: firstValidation.hash,
          iso_control: "A.13.2.1",
        });

        return res.status(403).json({
          trace_id,
          error: "Email not allowed by policy (second validation)",
          reason: secondValidation.reason,
          details: "Enhanced payload differs from consented content",
          iso_control: "A.13.2.1",
        });
      }

      logger.info("Second validation passed - Enhanced payload approved", {
        trace_id,
        step: "SECOND_VALIDATION_SUCCESS",
        hash: secondValidation.hash,
        iso_control: "A.13.2.1",
      });
    }

    // STEP 3: Send Email with ISO classification (A.12.4.1 - Event logging)
    logger.info("Starting ISO 27001 compliant email send", {
      trace_id,
      step: "EMAIL_SEND_START",
      classification,
      iso_control: "A.12.4.1",
    });

    // Add classification to payload for email service
    const emailPayload: EmailParams = {
      ...finalPayload,
      classification,
    };

    // Use the clean DI helper function
    const emailStatus = await sendEmailWithConfig(emailPayload, trace_id);

    // A.12.4.1 - Event logging: Complete audit trail
    logger.info("ISO 27001 compliant email sent successfully", {
      trace_id,
      step: "EMAIL_SEND_SUCCESS",
      user_id: process.env.TENANT_CLIENT_ID,
      gdpr_token,
      classification,
      security_controls: securityControls,
      first_hash: firstValidation.hash,
      second_hash: secondValidation.hash,
      email_status: emailStatus,
      status: "DELIVERED",
      iso_controls: ["A.8.2.1", "A.9.4.1", "A.12.4.1", "A.13.2.1"],
    });

    res.status(200).json({
      trace_id,
      message: "Email sent with ISO 27001 compliance",
      status: "success",
      classification,
      security_controls: securityControls,
      validations: {
        first_hash: firstValidation.hash,
        second_hash: secondValidation.hash,
        both_passed: true,
        double_validation_applied: securityControls.informationTransfer,
      },
      email_status: emailStatus,
      iso_controls: ["A.8.2.1", "A.9.4.1", "A.12.4.1", "A.13.2.1"],
    });
  } catch (err) {
    // A.12.4.1 - Event logging: Critical error logging
    logger.error("Critical error in ISO 27001 compliant ABAC email process", {
      trace_id,
      step: "CRITICAL_ERROR",
      classification,
      error: (err as Error).message,
      stack: (err as Error).stack,
      iso_control: "A.12.4.1",
    });

    logger.debug("Critical error details", { trace_id, error_details: err });

    // üéØ VERIFICAR SI ES UN ERROR ESTRUCTURADO DEL EMAIL SERVICE
    const error = err as any;
    const statusCode = error.statusCode || 500;

    if (error.details && error.userAction) {
      // Error estructurado con informaci√≥n √∫til
      return res.status(statusCode === 403 ? 403 : 500).json({
        trace_id,
        error: error.message,
        details: error.details,
        user_action: error.userAction,
        ...(error.graphErrorCode && { graph_error_code: error.graphErrorCode }),
        iso_control: "A.12.4.1",
        timestamp: new Date().toISOString(),
      });
    } else {
      // Error gen√©rico sin estructura
      return res.status(500).json({
        trace_id,
        error: (err as Error).message,
        details: `An unexpected error occurred. Check logs for trace_id: ${trace_id}`,
        user_action: "Contact system administrator if the problem persists",
        iso_control: "A.12.4.1",
        timestamp: new Date().toISOString(),
      });
    }
  }
};

/**
 * Creates a new email payload with all PDF attachments digitally signed
 *
 * This function implements ISO 27001 A.13.2.3 (Electronic messaging)
 * by applying digital signatures to PDF documents for data integrity
 * and authenticity verification.
 *
 * @param originalPayload - Original email payload from PLSQL call
 * @param trace_id - Trace ID for logging and audit trail (A.12.4.1)
 * @returns New payload with signed PDF attachment paths
 * @throws Error if PDF signing fails (fail-fast for security)
 */
async function createPayloadWithSignedPDFs(
  originalPayload: EmailParams,
  trace_id: string
): Promise<EmailParams> {
  // If no attachments, return original payload unchanged
  if (
    !originalPayload.attachments ||
    originalPayload.attachments.length === 0
  ) {
    logger.info("No attachments to process for digital signing", {
      trace_id,
      step: "NO_ATTACHMENTS",
      iso_control: "A.13.2.3",
    });
    return originalPayload;
  }

  logger.info("Processing attachments for ISO 27001 compliant PDF signing", {
    trace_id,
    step: "PROCESSING_ATTACHMENTS",
    original_attachment_count: originalPayload.attachments.length,
    iso_control: "A.13.2.3",
  });

  // Sign all PDF attachments using the dedicated PDF signing service
  const signingResults = await signPDFAttachments(
    originalPayload.attachments,
    trace_id
  );

  // Validate that all expected PDFs were signed successfully (A.13.2.3)
  // This will throw an error if any PDF that should have been signed wasn't
  validateSigningResults(signingResults, trace_id);

  // Extract the final list of attachments (signed PDFs + unchanged non-PDFs)
  const finalAttachments = getSignedAttachments(signingResults);

  logger.info("ISO 27001 PDF signing validation completed", {
    trace_id,
    step: "SIGNING_VALIDATION_SUCCESS",
    original_count: originalPayload.attachments.length,
    final_count: finalAttachments.length,
    signed_pdfs: signingResults.filter((r) => r.wasSigned).length,
    iso_control: "A.13.2.3",
  });

  // Return new payload with signed attachment paths
  // This will generate a different hash than the original payload for second validation
  return {
    ...originalPayload,
    attachments: finalAttachments,
  };
}

// src/middlewares/allowGetPostOnly.ts
import { Request, Response, NextFunction } from 'express';

export function allowGetPostOnly(req: Request, res: Response, next: NextFunction) {
  if (req.method !== 'GET' && req.method !== 'POST') {
    return res.status(405).json({ error: 'Method Not Allowed' });
  }
  next();
}

// src/middlewares/authenticateJWT.ts
import { Request, Response, NextFunction } from 'express';
import jwt from 'jsonwebtoken';

/**
 * JWT Authentication middleware factory
 * Returns a middleware function configured with the provided JWT secret
 */
export function createAuthenticateJWT(jwtSecret: string) {
  return function authenticateJWT(req: Request, res: Response, next: NextFunction) {
    const authHeader = req.headers.authorization;
    
    if (!authHeader || !authHeader.startsWith('Bearer ')) {
      return res.status(401).json({ message: 'Missing or invalid token' });
    }
    
    const token = authHeader.split(' ')[1];
    
    try {
      const user = jwt.verify(token, jwtSecret);
      (req as any).user = user;
      next();
    } catch {
      res.status(403).json({ message: 'Token verification failed' });
    }
  };
}
// src/middlewares/authorizeAdmin.ts
import { Request, Response, NextFunction } from 'express';

export function authorizeAdmin(req: Request, res: Response, next: NextFunction) {
  const user = (req as any).user;
  if (!user || user.role !== 'admin') {
    return res.status(403).json({ message: 'Access denied' });
  }
  next();
}

// src/middlewares/errorHandler.ts
import { Request, Response, NextFunction } from 'express';

export function errorHandler(err: any, req: Request, res: Response, next: NextFunction) {
  console.error(`[ErrorHandler] ${err.message || err}`);
  res.status(500).json({ error: 'Internal server error' });
}

// src/middlewares/index.ts
export { traceId } from './traceId';
export { validateToken } from './validateToken';
export { rateLimiter } from './rateLimiter';
export { validateBody } from './validateBody';
export { securityHeaders } from './securityHeaders';
export { errorHandler } from './errorHandler';
export { allowGetPostOnly } from './allowGetPostOnly';
export { createAuthenticateJWT } from './authenticateJWT';
export { authorizeAdmin } from './authorizeAdmin';
// src/middlewares/rateLimiter.ts
import rateLimit from 'express-rate-limit';

export const rateLimiter = rateLimit({
  windowMs: 60 * 1000, // 1 minuto
  max: 100,             // 100 peticiones por minuto
  standardHeaders: true,
  legacyHeaders: false
});

// src/middlewares/securityHeaders.ts
import { Request, Response, NextFunction } from 'express';

export function securityHeaders(req: Request, res: Response, next: NextFunction) {
  res.setHeader('X-Content-Type-Options', 'nosniff');
  res.setHeader('X-Frame-Options', 'DENY');
  res.setHeader('X-XSS-Protection', '1; mode=block');
  res.setHeader('Referrer-Policy', 'no-referrer');
  next();
}

// src/middlewares/traceId.ts
import { Request, Response, NextFunction } from 'express';
import { v4 as uuidv4 } from 'uuid';

export function traceId(req: Request, res: Response, next: NextFunction) {
  const incomingTraceId = req.headers['x-trace-id'] as string;
  const newTraceId = incomingTraceId || uuidv4();
  (req as any).trace_id = newTraceId;
  res.setHeader('x-trace-id', newTraceId);
  next();
}

// src/middlewares/validateBody.ts
import { Request, Response, NextFunction } from 'express';
import { ZodSchema } from 'zod';

export function validateBody(schema: ZodSchema) {
  return (req: Request, res: Response, next: NextFunction) => {
    const result = schema.safeParse(req.body);
    if (!result.success) {
      return res.status(400).json({ error: 'Validation error', issues: result.error.issues });
    }
    next();
  };
}

// src/middlewares/validateToken.ts
import { Request, Response, NextFunction } from 'express';
import { getInternalToken } from '../config/internalToken';

export function validateToken(req: Request, res: Response, next: NextFunction) {
  const token = req.headers.authorization?.replace('Bearer ', '');
  const expectedToken = getInternalToken();

  if (token !== expectedToken) {
    return res.status(401).json({ error: 'Unauthorized' });
  }

  next();
}

// src/routes/authRoutes.ts
import express from 'express';
import { login } from '../controllers/authController';

const router = express.Router();

router.post('/login', login);

export default router;

// src/routes/gdprRoutes.ts
// routes/gdprRoutes.ts

import express from 'express';
import { getGDPRService, GDPRTokenRequest } from '../services/gdpr/gdprTokenService';
import { generatePayloadHash } from '../utils/hashUtils';
import { z } from 'zod';
import logger from '../utils/logging';
import { v4 as uuidv4 } from 'uuid';

const router = express.Router();

/**
 * Schema for GDPR token generation request
 */
const GDPRTokenRequestSchema = z.object({
  recipient_email: z.string().email('Invalid email format'),
  purpose: z.string().min(1, 'Purpose is required').default('email_notification'),
  email_payload: z.object({
    to: z.string(),
    subject: z.string(),
    body: z.string(),
    classification: z.string().optional(),
    attachments: z.array(z.object({
      name: z.string(),
      path: z.string()
    })).optional()
  }),
  expires_in_hours: z.number().min(1).max(168).default(24), // Max 1 week
  user_id: z.string().optional(),
  client_id: z.string().optional()
});



/**
 * POST /gdpr/generate-token
 * Generate a new GDPR consent token for email sending
 */
router.post('/generate-token', async (req, res) => {
  const trace_id = uuidv4();
  
  try {
    logger.system('GDPR token generation requested', {
      trace_id,
      ip: req.ip,
      user_agent: req.get('User-Agent')
    });
    
    // Validate request
    const validation = GDPRTokenRequestSchema.safeParse(req.body);
    if (!validation.success) {
      logger.warn('GDPR token generation failed - validation error', {
        trace_id,
        errors: validation.error.issues
      });
      
      return res.status(400).json({
        trace_id,
        error: 'Validation failed',
        details: validation.error.issues
      });
    }
    
    const { 
      recipient_email, 
      purpose, 
      email_payload, 
      expires_in_hours, 
      user_id, 
      client_id 
    } = validation.data;
    
    // Generate consistent hash of the email payload
    const payload_hash = generatePayloadHash(email_payload);
    
    logger.system('Generated payload hash for GDPR token', {
      trace_id,
      recipient_email,
      payload_hash: payload_hash.substring(0, 16) + '...',
      payload_keys: Object.keys(email_payload)
    });
    
    // Generate GDPR token
    const gdprService = getGDPRService();
    const tokenRequest: GDPRTokenRequest = {
      recipient_email,
      purpose,
      payload_hash,
      expires_in_hours,
      user_id,
      client_id
    };
    
    const consentRecord = gdprService.generateToken(tokenRequest);
    
    logger.system('GDPR token generated successfully', {
      trace_id,
      token_preview: consentRecord.token.substring(0, 8) + '...',
      recipient_email,
      expires_at: consentRecord.expires_at
    });
    
    // Return token and metadata
    res.status(200).json({
      trace_id,
      success: true,
      gdpr_token: consentRecord.token,
      payload_hash,
      expires_at: consentRecord.expires_at,
      recipient_email,
      purpose,
      message: 'GDPR consent token generated successfully'
    });
    
  } catch (error) {
    logger.error('GDPR token generation failed', {
      trace_id,
      error: (error as Error).message,
      stack: (error as Error).stack
    });
    
    res.status(500).json({
      trace_id,
      error: 'Failed to generate GDPR token',
      details: 'Internal server error'
    });
  }
});

/**
 * GET /gdpr/validate-token/:token
 * Validate a GDPR token (for debugging/testing)
 */
router.get('/validate-token/:token', async (req, res) => {
  const trace_id = uuidv4();
  const { token } = req.params;
  const { payload_hash, recipient_email, purpose } = req.query;
  
  try {
    if (!payload_hash || !recipient_email) {
      return res.status(400).json({
        trace_id,
        error: 'payload_hash and recipient_email query parameters are required'
      });
    }
    
    const gdprService = getGDPRService();
    const result = gdprService.validateToken({
      token,
      payload_hash: payload_hash as string,
      recipient_email: recipient_email as string,
      purpose: purpose as string
    });
    
    res.json({
      trace_id,
      valid: result.valid,
      reason: result.reason,
      hash_type: result.hash_type
    });
    
  } catch (error) {
    logger.error('GDPR token validation failed', {
      trace_id,
      error: (error as Error).message
    });
    
    res.status(500).json({
      trace_id,
      error: 'Failed to validate GDPR token'
    });
  }
});

/**
 * GET /gdpr/stats
 * Get GDPR service statistics
 */
router.get('/stats', async (req, res) => {
  const trace_id = uuidv4();
  
  try {
    const gdprService = getGDPRService();
    const stats = gdprService.getStats();
    
    res.json({
      trace_id,
      stats,
      message: 'GDPR service statistics'
    });
    
  } catch (error) {
    logger.error('Failed to get GDPR stats', {
      trace_id,
      error: (error as Error).message
    });
    
    res.status(500).json({
      trace_id,
      error: 'Failed to get statistics'
    });
  }
});

export default router;
// src/scripts/start.ts
// scripts/dev.ts - SIMPLIFIED VERSION
import { execSync } from "child_process";
import * as path from "path";
import * as fs from "fs";

// Parse arguments
const args = process.argv.slice(2);
const clientId = args.find(arg => !arg.startsWith('--'));
const gpgPassphraseArg = args.find(arg => arg.startsWith("--gpg-passphrase="));

if (!clientId) {
  console.error("‚ùå Missing CLIENT_ID. Usage: npm run dev -- core-dev [--gpg-passphrase=xxx]");
  process.exit(1);
}

console.log(`üöÄ Starting Core Services for client: ${clientId}`);
console.log(`üîê Mode: SOPS ONLY`);

// Set environment variables
process.env.CLIENT_ID = clientId;

// Set GPG passphrase if provided
if (gpgPassphraseArg) {
  const passphrase = gpgPassphraseArg.split('=')[1];
  process.env.GPG_PASSPHRASE = passphrase;
  console.log("üîë GPG passphrase set from CLI argument");
}

// Verify GPG setup
console.log("üîç Verifying GPG setup...");
try {
  execSync('gpg --list-secret-keys', { stdio: 'pipe' });
  console.log("‚úÖ GPG keys available");
} catch (error) {
  console.error("‚ùå GPG setup issue. Make sure GPG is installed and keys are imported.");
  console.error("Run: gpg --list-secret-keys");
  process.exit(1);
}

// Verify SOPS is available
const sopsPath = path.resolve(__dirname, "../../../core-envs-private/tools/win64/sops.exe");
if (!fs.existsSync(sopsPath) && process.platform.startsWith("win")) {
  console.error(`‚ùå SOPS not found at: ${sopsPath}`);
  console.error("Make sure core-envs-private repo is cloned and sops.exe is in tools/win64/");
  process.exit(1);
}

// Verify client configuration exists
const envsPath = path.resolve(__dirname, "../../../core-envs-private/clients", clientId);
const configPath = path.join(envsPath, 'config.yaml');
const secretsPath = path.join(envsPath, 'secrets.sops.yaml');

if (!fs.existsSync(configPath)) {
  console.error(`‚ùå Client config not found: ${configPath}`);
  process.exit(1);
}

if (!fs.existsSync(secretsPath)) {
  console.error(`‚ùå Client secrets not found: ${secretsPath}`);
  process.exit(1);
}

console.log("‚úÖ Client configuration verified");

// Determine if we're running compiled or TypeScript
const isCompiled = __filename.endsWith('.js');
console.log(`üîß Execution mode: ${isCompiled ? 'COMPILED (JavaScript)' : 'DEVELOPMENT (TypeScript)'}`);

// Start the application
console.log("üöÄ Starting application...");

const appPath = isCompiled 
  ? path.resolve(__dirname, "../app.js")   // Compiled version in dist/
  : path.resolve(__dirname, "../app.ts");  // TypeScript version in src/

const runCommand = isCompiled
  ? `node "${appPath}" ${clientId}`
  : `ts-node "${appPath}" ${clientId}`;

console.log(`üìÇ App path: ${appPath}`);
console.log(`‚ö° Run command: ${runCommand}`);

try {
  execSync(runCommand, { 
    stdio: "inherit", 
    shell: process.platform.startsWith("win") ? "cmd.exe" : "/bin/bash",
    env: process.env // Pass all environment variables including GPG_PASSPHRASE
  });
} catch (error) {
  console.error("‚ùå Application failed to start");
  process.exit(1);
}
// src/services/email/emailService.ts
// src/services/email/emailService.ts

import axios from 'axios';
import { promises as fs } from 'fs';
import path from 'path';
import { getAccessToken, TokenServiceConfig } from './tokenService';
import logger from '../../utils/logging';

/**
 * Email Service - Pure Email Sending with Enhanced Security
 * 
 * Single responsibility: Send emails via Microsoft Graph API.
 * This service doesn't sign PDFs, validate GDPR, or do any business logic.
 * It just takes email parameters and sends them with maximum security headers.
 */

export interface EmailAttachment {
  name: string;
  path: string;
}

import { ISO27001Classification } from '../../types/iso27001';

export interface EmailParams {
  from?: string;
  to: string;
  subject: string;
  body: string;
  attachments?: EmailAttachment[];
  // ISO 27001 Annex A.8.2 - Information Classification
  classification: ISO27001Classification;
  // Optional parameters
  importance?: 'low' | 'normal' | 'high';
  gdpr_token?: string;
}

export interface EmailServiceConfig extends TokenServiceConfig {
  senderEmail: string;
}

/**
 * Sends an email with the provided parameters via Microsoft Graph API
 * 
 * This function assumes:
 * - All PDFs are already signed (if signing was required)
 * - All GDPR validation has been completed
 * - All file paths are valid and accessible
 * 
 * Enhanced with security headers for audit trail and compliance.
 * 
 * @param emailParams - Email parameters including recipient, subject, body, and attachments
 * @param trace_id - Trace ID for logging and audit trail
 * @param config - Email service configuration (senderEmail)
 * @returns HTTP status code from Microsoft Graph API
 * @throws Error if email sending fails
 */
// services/email/emailService.ts - MANEJO DE ERRORES MEJORADO

export async function sendEmail(
  { 
    from, 
    to, 
    subject, 
    body, 
    attachments = [], 
    classification,
    importance = 'normal',
    gdpr_token
  }: EmailParams,
  trace_id: string,
  config: EmailServiceConfig
): Promise<number> {
  
  logger.info('Starting ISO 27001 compliant email send process', {
    trace_id,
    to,
    from: from || 'using_config_default',
    subject: subject.substring(0, 50),
    attachment_count: attachments.length,
    classification,
    importance,
    has_gdpr_token: !!gdpr_token,
    iso_control: 'A.8.2.1'
  });

  // Build the Microsoft Graph email payload
  const emailPayload: any = {
    message: {
      subject,
      body: {
        contentType: 'Text',
        content: body
      },
      toRecipients: [
        {
          emailAddress: {
            address: to
          }
        }
      ],
      // Solo agregar 'from' al message si se proporciona
      ...(from && {
        from: {
          emailAddress: {
            address: from
          }
        }
      }),
      importance,
      internetMessageHeaders: buildISO27001SecurityHeaders(trace_id, classification, gdpr_token, attachments)
    },
    saveToSentItems: true
  };

  // Process attachments if any exist
  if (attachments.length > 0) {
    emailPayload.message.attachments = await processAttachments(attachments, trace_id);
  }

  // Get OAuth token for Microsoft Graph
  const accessToken = await getAccessToken(config);
  const authenticatedSender = config.senderEmail;

  logger.info('Sending ISO 27001 compliant email via Microsoft Graph API', {
    trace_id,
    authenticated_sender: authenticatedSender,
    message_from: from || 'config_default',
    to,
    classification,
    importance,
    custom_headers_count: emailPayload.message.internetMessageHeaders.length,
    api_endpoint: `https://graph.microsoft.com/v1.0/users/${authenticatedSender}/sendMail`,
    iso_control: 'A.13.2.1'
  });

  try {
    const response = await axios.post(
      `https://graph.microsoft.com/v1.0/users/${authenticatedSender}/sendMail`,
      emailPayload,
      {
        headers: {
          Authorization: `Bearer ${accessToken}`,
          'Content-Type': 'application/json'
        }
      }
    );

    logger.info('ISO 27001 compliant email sent successfully via Microsoft Graph', {
      trace_id,
      to,
      authenticated_sender: authenticatedSender,
      message_from: from || 'config_default',
      status_code: response.status,
      attachment_count: attachments.length,
      classification,
      importance,
      iso_control: 'A.12.4.1'
    });

    return response.status;

  } catch (error: any) {
    // üéØ MANEJO ESPEC√çFICO DE ERRORES DE MICROSOFT GRAPH
    const statusCode = error.response?.status;
    const responseData = error.response?.data;
    const graphError = responseData?.error;
    
    logger.error('Failed to send enhanced email via Microsoft Graph', {
      trace_id,
      to,
      authenticated_sender: authenticatedSender,
      message_from: from || 'config_default',
      error: error.message,
      status: statusCode,
      graph_error_code: graphError?.code,
      graph_error_message: graphError?.message,
      response_data: responseData
    });

    // üî• CREAR ERRORES ESPEC√çFICOS Y √öTILES
    let errorMessage = 'Email sending failed';
    let errorDetails = '';
    let userAction = '';

    if (statusCode === 403) {
      if (graphError?.code === 'Forbidden' || graphError?.message?.includes('Send on behalf')) {
        errorMessage = 'Email sender not authorized';
        errorDetails = `The email address '${from || authenticatedSender}' is not authorized to send emails through this system`;
        userAction = from && from !== authenticatedSender 
          ? `Either use '${authenticatedSender}' as sender or contact your administrator to authorize '${from}'`
          : 'Contact your administrator to verify email sending permissions';
      } else if (graphError?.message?.includes('permission') || graphError?.message?.includes('scope')) {
        errorMessage = 'Insufficient email permissions';
        errorDetails = 'The application does not have the required permissions to send emails';
        userAction = 'Contact your administrator to grant the necessary Microsoft Graph email permissions';
      } else {
        errorMessage = 'Email access forbidden';
        errorDetails = graphError?.message || 'Access to email service was denied';
        userAction = 'Verify your email permissions with the system administrator';
      }
    } else if (statusCode === 401) {
      errorMessage = 'Email service authentication failed';
      errorDetails = 'The authentication token is invalid or expired';
      userAction = 'The system will attempt to refresh the token automatically. If the problem persists, contact support';
    } else if (statusCode === 400) {
      errorMessage = 'Invalid email request';
      errorDetails = graphError?.message || 'The email request contains invalid data';
      userAction = 'Check your email parameters (recipient, subject, body) and try again';
    } else if (statusCode === 429) {
      errorMessage = 'Email service rate limit exceeded';
      errorDetails = 'Too many emails have been sent in a short period';
      userAction = 'Wait a few minutes before sending more emails';
    } else if (statusCode >= 500) {
      errorMessage = 'Email service temporarily unavailable';
      errorDetails = 'Microsoft Graph service is experiencing issues';
      userAction = 'Try again in a few minutes. If the problem persists, contact support';
    } else {
      errorMessage = 'Email sending failed';
      errorDetails = graphError?.message || error.message || 'Unknown error occurred';
      userAction = 'Check your email parameters and try again';
    }

    // üöÄ THROW ERROR CON INFORMACI√ìN ESTRUCTURADA
    const enhancedError = new Error(errorMessage);
    (enhancedError as any).details = errorDetails;
    (enhancedError as any).userAction = userAction;
    (enhancedError as any).statusCode = statusCode;
    (enhancedError as any).graphErrorCode = graphError?.code;
    (enhancedError as any).traceId = trace_id;
    
    throw enhancedError;
  }
}

/**
 * Builds ISO 27001 compliant security headers for email
 * 
 * Headers comply with:
 * - A.8.2.1 (Information classification)
 * - A.12.4.1 (Event logging)
 * - A.13.2.1 (Information transfer)
 * 
 * Microsoft Graph API limits to 5 headers max, so we prioritize
 * the most critical ISO 27001 compliance headers.
 * 
 * @param trace_id - Unique trace ID for audit trail (A.12.4.1)
 * @param classification - ISO 27001 information classification (A.8.2.1)
 * @param gdpr_token - GDPR consent token (if available)
 * @param attachments - List of attachments for security marking
 * @returns Array of ISO 27001 compliant email headers
 */
function buildISO27001SecurityHeaders(
  trace_id: string,
  classification: ISO27001Classification,
  gdpr_token?: string, 
  attachments?: EmailAttachment[]
): Array<{ name: string; value: string }> {
  
  // ISO 27001 compliant headers (prioritized for 5-header limit)
  const headers = [
    // A.12.4.1 - Event logging: Unique trace ID for audit trail
    {
      name: 'X-ISO27001-Trace-ID',
      value: trace_id
    },
    // A.8.2.1 - Information classification: Security classification level
    {
      name: 'X-ISO27001-Classification',
      value: classification.toUpperCase()
    },
    // A.9.4.1 - Information access restriction: Security validation status
    {
      name: 'X-ISO27001-Access-Control',
      value: 'ABAC-Validated'
    },
    // A.13.2.1 - Information transfer: Compliance framework
    {
      name: 'X-ISO27001-Compliance',
      value: 'GDPR-ISO27001'
    }
  ];

  // Add GDPR status if token present (A.13.2.1)
  if (gdpr_token) {
    headers.push({
      name: 'X-ISO27001-GDPR-Status',
      value: 'Double-Validated'
    });
  } else {
    // If no GDPR token, add timestamp for audit trail (A.12.4.1)
    headers.push({
      name: 'X-ISO27001-Timestamp',
      value: new Date().toISOString()
    });
  }

  // Add digital signature info if we have signed PDFs and room for one more header (A.13.2.3)
  const signedPdfs = attachments?.filter(a => a.name.endsWith('_signed.pdf')) || [];
  if (signedPdfs.length > 0 && headers.length < 5) {
    // Remove last header to make room for signature info if needed
    if (headers.length === 5) headers.pop();
    headers.push({
      name: 'X-ISO27001-Digital-Signature',
      value: `Applied-${signedPdfs.length}-PDFs`
    });
  }

  logger.info('Built ISO 27001 compliant security headers', {
    trace_id,
    header_count: headers.length,
    classification,
    has_gdpr_token: !!gdpr_token,
    has_signed_pdfs: signedPdfs.length > 0,
    iso_controls: ['A.8.2.1', 'A.12.4.1', 'A.13.2.1']
  });

  return headers;
}

/**
 * Processes email attachments for Microsoft Graph API
 * 
 * Reads each attachment file, converts to base64, and formats
 * according to Microsoft Graph fileAttachment specification.
 * 
 * @param attachments - Array of attachment file information
 * @param trace_id - Trace ID for logging
 * @returns Array of Microsoft Graph attachment objects
 * @throws Error if any attachment file cannot be read
 */
async function processAttachments(
  attachments: EmailAttachment[],
  trace_id: string
): Promise<any[]> {
  
  logger.info('Processing email attachments with security validation', {
    trace_id,
    attachment_count: attachments.length
  });

  const processedAttachments = [];

  for (const attachment of attachments) {
    try {
      // Validate attachment file exists and is readable
      await validateAttachmentFile(attachment, trace_id);

      // Read file and convert to base64
      const contentBytes = await fs.readFile(attachment.path);
      const base64Content = contentBytes.toString('base64');

      logger.info('Attachment processed successfully', {
        trace_id,
        name: attachment.name,
        path: attachment.path,
        size_bytes: contentBytes.length,
        base64_length: base64Content.length,
        is_signed_pdf: attachment.name.endsWith('_signed.pdf')
      });

      // Format for Microsoft Graph API
      processedAttachments.push({
        '@odata.type': '#microsoft.graph.fileAttachment',
        name: attachment.name,
        contentBytes: base64Content,
        contentType: getContentType(attachment.name),
        // Add custom properties for signed PDFs
        ...(attachment.name.endsWith('_signed.pdf') && {
          isInline: false,
          size: contentBytes.length
        })
      });

    } catch (error) {
      logger.error('Failed to process attachment', {
        trace_id,
        name: attachment.name,
        path: attachment.path,
        error: (error as Error).message
      });

      throw new Error(`Failed to process attachment ${attachment.name}: ${(error as Error).message}`);
    }
  }

  return processedAttachments;
}

/**
 * Validates that an attachment file exists and is readable
 * 
 * @param attachment - Attachment to validate
 * @param trace_id - Trace ID for logging
 * @throws Error if file is not accessible
 */
async function validateAttachmentFile(
  attachment: EmailAttachment,
  trace_id: string
): Promise<void> {
  
  try {
    const stats = await fs.stat(attachment.path);
    
    if (!stats.isFile()) {
      throw new Error(`Attachment path is not a file: ${attachment.path}`);
    }

    // Additional security validation for signed PDFs
    if (attachment.name.endsWith('_signed.pdf')) {
      logger.info('Validating signed PDF attachment', {
        trace_id,
        name: attachment.name,
        path: attachment.path,
        size_bytes: stats.size
      });
    }

    logger.info('Attachment file validated', {
      trace_id,
      name: attachment.name,
      path: attachment.path,
      size_bytes: stats.size
    });

  } catch (error) {
    logger.error('Attachment file validation failed', {
      trace_id,
      name: attachment.name,
      path: attachment.path,
      error: (error as Error).message
    });

    throw new Error(`Attachment file not accessible: ${attachment.path}`);
  }
}

/**
 * Determines content type based on file extension
 * 
 * @param filename - Name of the file
 * @returns MIME type string
 */
function getContentType(filename: string): string {
  const extension = path.extname(filename).toLowerCase();
  
  switch (extension) {
    case '.pdf':
      return 'application/pdf';
    case '.txt':
      return 'text/plain';
    case '.json':
      return 'application/json';
    case '.xml':
      return 'application/xml';
    case '.csv':
      return 'text/csv';
    case '.jpg':
    case '.jpeg':
      return 'image/jpeg';
    case '.png':
      return 'image/png';
    case '.docx':
      return 'application/vnd.openxmlformats-officedocument.wordprocessingml.document';
    case '.xlsx':
      return 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet';
    default:
      return 'application/octet-stream';
  }
}
// src/services/email/emailServiceHelpers.ts
// src/services/email/emailServiceHelpers.ts

import { sendEmail, EmailParams } from './emailService';
import { getServiceContainer } from '../serviceContainer';

/**
 * Send email using the initialized service container
 * Clean helper that automatically uses the correct config
 */
export async function sendEmailWithConfig(
  emailParams: EmailParams,
  trace_id: string
): Promise<number> {
  const container = getServiceContainer();
  const emailConfig = container.getEmailConfig();
  
  return sendEmail(emailParams, trace_id, emailConfig);
}

/**
 * Export for easy importing
 */
export { EmailParams, EmailAttachment } from './emailService';
// src/services/email/internalRoutes.ts
import express from 'express';
import { sendEmailWithConfig } from './emailServiceHelpers';
import { createAuthenticateJWT, authorizeAdmin } from '../../middlewares';
import { getServiceContainer } from '../serviceContainer';
import { v4 as uuidv4 } from 'uuid';

const router = express.Router();

router.post(
  '/send-internal',
  // Dynamic middleware - creates JWT authenticator with loaded config
  (req, res, next) => {
    try {
      const container = getServiceContainer();
      const jwtSecret = container.getJwtSecret();
      const authenticateJWT = createAuthenticateJWT(jwtSecret);
      authenticateJWT(req, res, next);
    } catch (error) {
      res.status(500).json({ 
        error: 'Service not properly initialized',
        details: (error as Error).message 
      });
    }
  },
  authorizeAdmin,
  async (req, res) => {
    const trace_id = uuidv4(); // Generate a unique trace ID for this request
    
    try {
      const result = await sendEmailWithConfig(req.body, trace_id);
      res.json({ success: true, trace_id, result });
    } catch (err) {
      res.status(500).json({ error: 'Failed to send email', trace_id });
    }
  }
);

export default router;
// src/services/email/pdfSigningService.ts
// src/services/pdfSigningService.ts

import { signPDF } from '../../utils/pdfSigner';
import logger from '../../utils/logging';
import { getServiceContainer } from '../serviceContainer';
import { EmailAttachment } from '../email/emailService';
import { PdfSigningConfig } from '../../types/pdfTypes';

/**
 * PDF Signing Service
 * 
 * Single responsibility: Sign PDF files and return updated attachment paths.
 * This service doesn't know about emails, GDPR, or anything else.
 * It just signs PDFs and tells you where the signed versions are.
 */

export interface SigningResult {
  originalAttachment: EmailAttachment;
  signedAttachment: EmailAttachment;
  wasSigned: boolean;
  reason: string;
}

/**
 * Signs all PDF attachments in the provided list
 * 
 * For each attachment:
 * - If it's not a PDF: returns original unchanged
 * - If it's already signed: returns original unchanged  
 * - If it's a PDF that needs signing: signs it and returns new path
 * 
 * @param attachments - Array of original attachments
 * @param trace_id - Trace ID for logging context
 * @returns Array of signing results with original and signed attachment info
 */
export async function signPDFAttachments(
  attachments: EmailAttachment[],
  trace_id: string
): Promise<SigningResult[]> {
  
  if (!attachments || attachments.length === 0) {
    logger.info('No attachments provided for signing', { trace_id });
    return [];
  }

  const results: SigningResult[] = [];

  for (const attachment of attachments) {
    const result = await signSingleAttachment(attachment, trace_id);
    results.push(result);
  }

  logger.info('PDF signing batch completed', {
    trace_id,
    total_attachments: attachments.length,
    signed_count: results.filter(r => r.wasSigned).length,
    skipped_count: results.filter(r => !r.wasSigned).length
  });

  return results;
}

/**
 * Signs a single attachment if it's a PDF that needs signing
 * 
 * Business rules:
 * - Non-PDF files: skip (return original)
 * - Already signed PDFs: skip (return original)
 * - Unsigned PDFs: sign and return new path
 * 
 * @param attachment - Single attachment to process
 * @param trace_id - Trace ID for logging
 * @returns Signing result with original and signed attachment info
 */
async function signSingleAttachment(
  attachment: EmailAttachment,
  trace_id: string
): Promise<SigningResult> {
  
  // Skip non-PDF files
  if (!attachment.name.toLowerCase().endsWith('.pdf')) {
    logger.info('Skipping non-PDF attachment', {
      trace_id,
      name: attachment.name,
      reason: 'Not a PDF file'
    });
    
    return {
      originalAttachment: attachment,
      signedAttachment: attachment, // Same as original
      wasSigned: false,
      reason: 'Not a PDF file'
    };
  }

  // Skip already signed PDFs
  if (attachment.name.endsWith('_signed.pdf')) {
    logger.info('Skipping already signed PDF', {
      trace_id,
      name: attachment.name,
      reason: 'Already signed'
    });
    
    return {
      originalAttachment: attachment,
      signedAttachment: attachment, // Same as original
      wasSigned: false,
      reason: 'Already signed'
    };
  }

  // Sign the PDF
  const signedName = attachment.name.replace(/\.pdf$/i, '_signed.pdf');
  const signedPath = attachment.path.replace(/\.pdf$/i, '_signed.pdf');

  logger.info('Signing PDF attachment', {
    trace_id,
    original_name: attachment.name,
    original_path: attachment.path,
    signed_name: signedName,
    signed_path: signedPath
  });

  try {
    // Get PDF signing config from service container
    const container = getServiceContainer();
    const pdfConfig = container.getPdfSigningConfig();

    await signPDF({
      pdfPath: attachment.path,
      outputPath: signedPath,
      certPath: pdfConfig.certPdfSignPath,
      certPassword: pdfConfig.certPdfSignPassword || '',
      type: pdfConfig.certPdfSignType
    });

    logger.info('PDF signing successful', {
      trace_id,
      original_name: attachment.name,
      signed_name: signedName
    });

    return {
      originalAttachment: attachment,
      signedAttachment: {
        name: signedName,
        path: signedPath
      },
      wasSigned: true,
      reason: 'Successfully signed'
    };

  } catch (error) {
    logger.error('PDF signing failed', {
      trace_id,
      original_name: attachment.name,
      error: (error as Error).message
    });

    // In case of signing failure, we could either:
    // 1. Throw error (fail fast)
    // 2. Return original (fallback)
    // For security, we choose fail fast
    throw new Error(`Failed to sign PDF ${attachment.name}: ${(error as Error).message}`);
  }
}

/**
 * Extracts only the signed attachments from signing results
 * 
 * This is a convenience function for callers who just want
 * the final list of attachments to use for email sending.
 * 
 * @param signingResults - Results from signPDFAttachments()
 * @returns Array of attachments ready for email sending
 */
export function getSignedAttachments(signingResults: SigningResult[]): EmailAttachment[] {
  return signingResults.map(result => result.signedAttachment);
}

/**
 * Validates that all expected PDFs were successfully signed
 * 
 * Use this to ensure no unsigned PDFs slip through the process.
 * Throws error if any PDF that should have been signed wasn't.
 * 
 * @param signingResults - Results from signPDFAttachments()
 * @param trace_id - Trace ID for logging
 * @throws Error if validation fails
 */
export function validateSigningResults(
  signingResults: SigningResult[],
  trace_id: string
): void {
  
  const pdfFiles = signingResults.filter(r => 
    r.originalAttachment.name.toLowerCase().endsWith('.pdf')
  );
  
  const unsignedPdfs = pdfFiles.filter(r => 
    !r.wasSigned && !r.originalAttachment.name.endsWith('_signed.pdf')
  );

  if (unsignedPdfs.length > 0) {
    const unsignedNames = unsignedPdfs.map(r => r.originalAttachment.name);
    
    logger.error('Validation failed: unsigned PDFs detected', {
      trace_id,
      unsigned_pdfs: unsignedNames
    });
    
    throw new Error(`Unsigned PDFs detected: ${unsignedNames.join(', ')}`);
  }

  logger.info('PDF signing validation passed', {
    trace_id,
    total_pdfs: pdfFiles.length,
    all_signed: true
  });
}
// src/services/email/publicRoutes.ts
// src/services/email/publicRoutes.ts
import express from 'express';
import { abacSend } from '../../controllers/email/abacSend';
import { getServiceContainer } from '../serviceContainer';
import { v4 as uuidv4 } from 'uuid';
import logger from '../../utils/logging';
import { sendEmail } from './emailService';

const router = express.Router();

// No requiere JWT, solo gdpr_token
router.post('/send-with-consent', abacSend);

router.post('/send-internal', async (req, res) => {
    const trace_id = uuidv4();
   
    try {
      logger.info('Internal email request received - BYPASS MODE', {
        trace_id,
        to: req.body.to,
        from: req.body.from || 'config_default',
        subject: req.body.subject?.substring(0, 50),
        from_oracle: true,
        endpoint: 'send-internal',
        bypass_pep: true
      });

      // Llamar directamente a sendEmail SIN PEP
      const container = getServiceContainer();
      const emailConfig = container.getEmailConfig();
      
      const result = await sendEmail(req.body, trace_id, emailConfig);
     
      logger.info('Internal email sent successfully - BYPASS MODE', {
        trace_id,
        result,
        to: req.body.to,
        from: req.body.from || 'config_default',
        bypass_pep: true
      });
 
      res.json({
        success: true,
        trace_id,
        result,
        message: 'Email sent successfully from internal endpoint (bypass mode)'
      });
     
    } catch (err: any) {
      // üî• MEJORAR ERRORES COMO EN EL ENDPOINT GDPR
      const error = err as any;
      
      logger.error('Internal email failed', {
        trace_id,
        error: error.message,
        to: req.body.to,
        from: req.body.from || 'config_default',
        bypass_mode: true,
        has_error_details: !!error.details,
        has_user_action: !!error.userAction
      });

      // Si el error viene del emailService mejorado, usar esos detalles
      if (error.details && error.userAction) {
        return res.status(error.statusCode === 403 ? 403 : 500).json({
          success: false,
          trace_id,
          error: error.message,
          details: error.details,
          user_action: error.userAction,
          ...(error.graphErrorCode && { graph_error_code: error.graphErrorCode }),
          endpoint: 'send-internal',
          timestamp: new Date().toISOString()
        });
      } else {
        // Error gen√©rico
        return res.status(500).json({
          success: false,
          trace_id,
          error: error.message || 'Unknown error',
          details: 'An unexpected error occurred in internal email endpoint',
          user_action: 'Check the email parameters and try again',
          endpoint: 'send-internal',
          timestamp: new Date().toISOString()
        });
      }
    }
}); 

export default router;
// src/services/email/tokenService.ts
import axios from 'axios';
import qs from 'querystring';

export interface TokenServiceConfig {
  tokenEndpoint: string;
  tenantId: string;
  clientId: string;
  clientSecret: string;
}

let cachedToken: {
  value: string;
  expiresAt: number;
} | null = null;

export async function getAccessToken(config: TokenServiceConfig): Promise<string> {
  const now = Date.now();
  
  // Return cached token if still valid
  if (cachedToken && cachedToken.expiresAt > now + 60_000) {
    return cachedToken.value;
  }

  const url = `${config.tokenEndpoint}/${config.tenantId}/oauth2/v2.0/token`;
  const data = qs.stringify({
    client_id: config.clientId,
    client_secret: config.clientSecret,
    scope: 'https://graph.microsoft.com/.default',
    grant_type: 'client_credentials'
  });

  const headers = {
    'Content-Type': 'application/x-www-form-urlencoded'
  };

  const response = await axios.post(url, data, { headers });
  const { access_token, expires_in } = response.data;

  // Cache token with expiration time
  cachedToken = {
    value: access_token,
    expiresAt: now + expires_in * 1000
  };

  console.log('[core-services] OAuth token acquired from Microsoft Graph');
  return access_token;
}
// src/services/gdpr/gdprTokenService.ts
// services/gdpr/gdprTokenService.ts

import crypto from 'crypto';
import logger from '../../utils/logging';
import { v4 as uuidv4 } from 'uuid';

/**
 * GDPR Token Service - Production Implementation
 * 
 * Generates and validates GDPR consent tokens with payload hash validation
 * Implements Zero Trust principles for email consent verification
 */

export interface GDPRConsentRecord {
  token: string;
  original_hash: string;
  signed_hash?: string;  // Generated after PDF signing
  subject: string;
  purpose: string;
  created_at: string;
  expires_at: string;
  user_id?: string;
  client_id?: string;
}

export interface GDPRTokenRequest {
  recipient_email: string;
  purpose: string;
  payload_hash: string;
  expires_in_hours?: number;
  user_id?: string;
  client_id?: string;
}

export interface GDPRValidationRequest {
  token: string;
  payload_hash: string;
  recipient_email: string;
  purpose?: string;
}

export interface GDPRValidationResult {
  valid: boolean;
  reason: string;
  consent_record?: GDPRConsentRecord;
  hash_type?: 'original' | 'signed';
}

/**
 * In-memory consent store (production should use database)
 * This is a simple implementation for immediate use
 */
class ConsentStore {
  private consents = new Map<string, GDPRConsentRecord>();
  
  save(record: GDPRConsentRecord): void {
    this.consents.set(record.token, record);
    
    logger.system('GDPR consent record saved', {
      token_preview: record.token.substring(0, 8) + '...',
      subject: record.subject,
      purpose: record.purpose,
      expires_at: record.expires_at,
      has_signed_hash: !!record.signed_hash
    });
  }
  
  get(token: string): GDPRConsentRecord | undefined {
    return this.consents.get(token);
  }
  
  updateSignedHash(token: string, signed_hash: string): boolean {
    const record = this.consents.get(token);
    if (record) {
      record.signed_hash = signed_hash;
      this.consents.set(token, record);
      
      logger.system('GDPR consent signed hash updated', {
        token_preview: token.substring(0, 8) + '...',
        signed_hash: signed_hash.substring(0, 16) + '...'
      });
      
      return true;
    }
    return false;
  }
  
  cleanup(): void {
    const now = new Date();
    const expired: string[] = [];
    
    this.consents.forEach((record, token) => {
      if (new Date(record.expires_at) < now) {
        expired.push(token);
      }
    });
    
    expired.forEach(token => this.consents.delete(token));
    
    if (expired.length > 0) {
      logger.system('GDPR consent cleanup completed', {
        expired_tokens: expired.length,
        remaining_tokens: this.consents.size
      });
    }
  }
  
  getStats(): { total: number; active: number; expired: number } {
    const now = new Date();
    let active = 0;
    let expired = 0;
    
    this.consents.forEach(record => {
      if (new Date(record.expires_at) < now) {
        expired++;
      } else {
        active++;
      }
    });
    
    return { total: this.consents.size, active, expired };
  }
}

/**
 * GDPR Token Service Implementation
 */
export class GDPRTokenService {
  private consentStore: ConsentStore;
  private cleanupInterval: NodeJS.Timeout | null = null;
  
  constructor() {
    this.consentStore = new ConsentStore();
    this.startCleanupTimer();
  }
  
  /**
   * Generate a new GDPR consent token
   * 
   * @param request - Token generation request
   * @returns Generated consent record with token
   */
  generateToken(request: GDPRTokenRequest): GDPRConsentRecord {
    const trace_id = uuidv4();
    
    logger.system('Generating GDPR consent token', {
      trace_id,
      recipient: request.recipient_email,
      purpose: request.purpose,
      payload_hash: request.payload_hash.substring(0, 16) + '...',
      expires_in_hours: request.expires_in_hours || 24
    });
    
    // Generate unique token
    const token = this.createUniqueToken();
    
    // Calculate expiration
    const expiresAt = new Date();
    expiresAt.setHours(expiresAt.getHours() + (request.expires_in_hours || 24));
    
    // Create consent record
    const record: GDPRConsentRecord = {
      token,
      original_hash: request.payload_hash,
      subject: request.recipient_email,
      purpose: request.purpose,
      created_at: new Date().toISOString(),
      expires_at: expiresAt.toISOString(),
      user_id: request.user_id,
      client_id: request.client_id
    };
    
    // Store the consent
    this.consentStore.save(record);
    
    logger.system('GDPR consent token generated successfully', {
      trace_id,
      token_preview: token.substring(0, 8) + '...',
      expires_at: record.expires_at,
      recipient: request.recipient_email
    });
    
    return record;
  }
  
  /**
   * Validate GDPR consent token and payload
   * 
   * @param request - Validation request
   * @returns Validation result
   */
  validateToken(request: GDPRValidationRequest): GDPRValidationResult {
    const trace_id = uuidv4();
    
    logger.system('Validating GDPR consent token', {
      trace_id,
      token_preview: request.token.substring(0, 8) + '...',
      payload_hash: request.payload_hash.substring(0, 16) + '...',
      recipient: request.recipient_email
    });
    
    // Clean up expired tokens first
    this.consentStore.cleanup();
    
    // Get consent record
    const record = this.consentStore.get(request.token);
    if (!record) {
      logger.warn('GDPR token validation failed - token not found', {
        trace_id,
        token_preview: request.token.substring(0, 8) + '...'
      });
      
      return {
        valid: false,
        reason: 'Invalid or expired GDPR token - no consent record found'
      };
    }
    
    // Check expiration
    if (new Date(record.expires_at) < new Date()) {
      logger.warn('GDPR token validation failed - token expired', {
        trace_id,
        token_preview: request.token.substring(0, 8) + '...',
        expires_at: record.expires_at
      });
      
      return {
        valid: false,
        reason: 'GDPR consent has expired'
      };
    }
    
    // Validate payload hash (original or signed)
    const originalMatch = record.original_hash === request.payload_hash;
    const signedMatch = record.signed_hash === request.payload_hash;
    
    if (!originalMatch && !signedMatch) {
      logger.warn('GDPR token validation failed - hash mismatch', {
        trace_id,
        token_preview: request.token.substring(0, 8) + '...',
        expected_original: record.original_hash.substring(0, 16) + '...',
        expected_signed: record.signed_hash?.substring(0, 16) + '...' || 'not_set',
        received: request.payload_hash.substring(0, 16) + '...'
      });
      
      return {
        valid: false,
        reason: `Payload hash does not match registered consent. Expected: ${record.original_hash} (original)${record.signed_hash ? ` or ${record.signed_hash} (signed)` : ''}, got: ${request.payload_hash}`
      };
    }
    
    // Validate subject
    if (record.subject !== request.recipient_email) {
      logger.warn('GDPR token validation failed - subject mismatch', {
        trace_id,
        token_preview: request.token.substring(0, 8) + '...',
        expected_subject: record.subject,
        received_subject: request.recipient_email
      });
      
      return {
        valid: false,
        reason: `Subject mismatch. Expected: ${record.subject}, got: ${request.recipient_email}`
      };
    }
    
    // Validate purpose if provided
    if (request.purpose && record.purpose !== request.purpose) {
      logger.warn('GDPR token validation failed - purpose mismatch', {
        trace_id,
        token_preview: request.token.substring(0, 8) + '...',
        expected_purpose: record.purpose,
        received_purpose: request.purpose
      });
      
      return {
        valid: false,
        reason: `Purpose mismatch. Expected: ${record.purpose}, got: ${request.purpose}`
      };
    }
    
    const hashType = originalMatch ? 'original' : 'signed';
    
    logger.system('GDPR token validation successful', {
      trace_id,
      token_preview: request.token.substring(0, 8) + '...',
      hashType,
      subject: record.subject,
      purpose: record.purpose
    });
    
    return {
      valid: true,
      reason: `Consent valid and policy conditions met (${hashType} payload hash)`,
      consent_record: record,
      hash_type: hashType
    };
  }
  
  /**
   * Update signed hash for existing consent (after PDF signing)
   * 
   * @param token - Consent token
   * @param signed_hash - Hash of payload with signed PDFs
   * @returns Success status
   */
  updateSignedHash(token: string, signed_hash: string): boolean {
    return this.consentStore.updateSignedHash(token, signed_hash);
  }
  
  /**
   * Create unique token with proper entropy
   */
  private createUniqueToken(): string {
    const timestamp = Date.now().toString(36);
    const randomBytes = crypto.randomBytes(16).toString('hex');
    return `gdpr_${timestamp}_${randomBytes}`;
  }
  
  /**
   * Start automatic cleanup of expired tokens
   */
  private startCleanupTimer(): void {
    // Clean up every hour
    this.cleanupInterval = setInterval(() => {
      this.consentStore.cleanup();
    }, 60 * 60 * 1000);
  }
  
  /**
   * Stop cleanup timer
   */
  stopCleanupTimer(): void {
    if (this.cleanupInterval) {
      clearInterval(this.cleanupInterval);
      this.cleanupInterval = null;
    }
  }
  
  /**
   * Get service statistics
   */
  getStats(): { total: number; active: number; expired: number } {
    return this.consentStore.getStats();
  }
}

// Singleton instance
let gdprService: GDPRTokenService | null = null;

/**
 * Get GDPR service instance
 */
export function getGDPRService(): GDPRTokenService {
  if (!gdprService) {
    gdprService = new GDPRTokenService();
  }
  return gdprService;
}

/**
 * Initialize GDPR service (called from app startup)
 */
export function initGDPRService(): GDPRTokenService {
  if (gdprService) {
    gdprService.stopCleanupTimer();
  }
  gdprService = new GDPRTokenService();
  
  logger.system('GDPR token service initialized', {
    cleanup_interval: '1 hour',
    default_token_expiry: '24 hours'
  });
  return gdprService;
} 
// src/services/pdf/generate.ts
// src/services/pdf/generate.ts - VERSI√ìN MEJORADA
import path from "path";
import { promises as fs } from "fs";
import { compileFile } from "pug";
import { paths } from "../../config/paths";
import { acquirePage, releasePage } from "../../config/browserPool";
import { v4 as uuidv4 } from "uuid";
import logger from "../../utils/logging";

interface CoreReportInfo {
  report_name: string;
  report_description: string;
  report_template: string;
  report_version: string;
  report_file_name: string;
  report_out_mode: string;
}

/**
 * Estructura de error mejorada para PDF generation
 */
class PDFGenerationError extends Error {
  public readonly code: string;
  public readonly details: string;
  public readonly userAction: string;
  public readonly statusCode: number;
  public readonly traceId: string;

  constructor(
    message: string,
    code: string,
    details: string,
    userAction: string,
    statusCode: number = 500,
    traceId: string
  ) {
    super(message);
    this.name = 'PDFGenerationError';
    this.code = code;
    this.details = details;
    this.userAction = userAction;
    this.statusCode = statusCode;
    this.traceId = traceId;
  }
}

export async function generatePDF(data: any): Promise<Buffer> {
  const startTime = Date.now();
  const info: CoreReportInfo = data.core_report_info;
  const trace_id = uuidv4();
  
  logger.pdf('Starting PDF generation', {
    trace_id,
    correlation_id: trace_id,
    client_name: data.client_name,
    report_name: info?.report_name,
    report_template: info?.report_template,
    report_file_name: info?.report_file_name
  });

  // VALIDACI√ìN MEJORADA
  try {
    validatePDFRequest(info, trace_id);
  } catch (error) {
    if (error instanceof PDFGenerationError) {
      throw error;
    }
    throw new PDFGenerationError(
      'PDF request validation failed',
      'VALIDATION_ERROR',
      (error as Error).message,
      'Check your request format and ensure all required fields are provided',
      400,
      trace_id
    );
  }

  const templatePath = path.join(paths.pdf.templatePath, `${info.report_template}.pug`);
  const cssPath = path.join(paths.pdf.cssPath, `${info.report_template}.css`);

  let page: any = null;
  
  try {
    // PASO 1: Verificar archivos de template
    logger.pdf('Verifying template files', {
      trace_id,
      template_path: templatePath,
      css_path: cssPath
    });

    await validateTemplateFiles(templatePath, cssPath, trace_id);

    // PASO 2: Leer y compilar template
    logger.pdf('Loading and compiling template', {
      trace_id,
      template_path: templatePath
    });

    const [cssContent, htmlContent] = await compileTemplate(templatePath, cssPath, data, trace_id);

    // PASO 3: Adquirir p√°gina del browser pool
    logger.pdf('Acquiring browser page from pool', {
      trace_id,
      html_length: htmlContent.length,
      duration_ms: Date.now() - startTime
    });

    page = await acquirePageSafely(trace_id);
    
    // PASO 4: Generar PDF
    logger.pdf('Rendering PDF content', {
      trace_id,
      duration_ms: Date.now() - startTime
    });

    const pdfBuffer = await renderPDF(page, htmlContent, trace_id);
    
    // PASO 5: M√©tricas finales y retorno
    const totalDuration = Date.now() - startTime;
    const pdfSizeKB = Math.round(pdfBuffer.length / 1024);
    
    logger.pdf('PDF generated successfully', {
      trace_id,
      duration_ms: totalDuration,
      file_size_bytes: pdfBuffer.length,
      file_size_kb: pdfSizeKB,
      report_name: info.report_name,
      report_file_name: info.report_file_name,
      template: info.report_template
    });

    return pdfBuffer;
    
  } catch (error) {
    const errorDuration = Date.now() - startTime;
    
    // Si ya es nuestro error estructurado, re-lanzar
    if (error instanceof PDFGenerationError) {
      logger.pdf('PDF generation failed with structured error', {
        trace_id,
        duration_ms: errorDuration,
        error_code: error.code,
        error_message: error.message,
        template: info?.report_template
      });
      throw error;
    }
    
    // Convertir errores no estructurados a errores estructurados
    const structuredError = createStructuredPDFError(error as Error, trace_id, info);
    
    logger.pdf('PDF generation failed', {
      trace_id,
      duration_ms: errorDuration,
      error_code: structuredError.code,
      error_message: structuredError.message,
      original_error: (error as Error).message,
      template: info?.report_template
    });
    
    throw structuredError;
    
  } finally {
    // SIEMPRE liberar la p√°gina
    if (page) {
      try {
        await releasePage(page);
        logger.pdf('Browser page released', {
          trace_id,
          duration_ms: Date.now() - startTime
        });
      } catch (releaseError) {
        logger.pdf('Failed to release browser page', {
          trace_id,
          error_message: (releaseError as Error).message,
          duration_ms: Date.now() - startTime
        });
      }
    }
  }
}

/**
 * Validaci√≥n mejorada del request
 */
function validatePDFRequest(info: CoreReportInfo, trace_id: string): void {
  if (!info) {
    throw new PDFGenerationError(
      'Missing core_report_info',
      'MISSING_REPORT_INFO',
      'The core_report_info object is required but was not provided',
      'Include core_report_info in your request with all required fields',
      400,
      trace_id
    );
  }

  if (!info.report_template) {
    throw new PDFGenerationError(
      'Missing report template',
      'MISSING_TEMPLATE',
      'report_template field is required in core_report_info',
      'Specify a valid report_template name in core_report_info',
      400,
      trace_id
    );
  }

  // Validar caracteres peligrosos
  if (!/^[a-zA-Z0-9_-]+$/.test(info.report_template)) {
    throw new PDFGenerationError(
      'Invalid template name',
      'INVALID_TEMPLATE_NAME',
      'Template name contains invalid characters',
      'Use only alphanumeric characters, underscores, and hyphens in template names',
      400,
      trace_id
    );
  }
}

/**
 * Validar que existan los archivos de template
 */
async function validateTemplateFiles(templatePath: string, cssPath: string, trace_id: string): Promise<void> {
  try {
    await fs.access(templatePath, fs.constants.R_OK);
  } catch (error) {
    throw new PDFGenerationError(
      'Template file not found',
      'TEMPLATE_NOT_FOUND',
      `Template file does not exist: ${templatePath}`,
      'Ensure the template file exists and is readable, or contact the system administrator',
      404,
      trace_id
    );
  }

  try {
    await fs.access(cssPath, fs.constants.R_OK);
  } catch (error) {
    logger.pdf('CSS file not found, continuing without styles', {
      trace_id,
      css_path: cssPath,
      warning: 'PDF will be generated without custom styles'
    });
  }
}

/**
 * Compilar template de manera segura
 */
async function compileTemplate(
  templatePath: string, 
  cssPath: string, 
  data: any, 
  trace_id: string
): Promise<[string, string]> {
  try {
    // Leer CSS (opcional)
    let cssContent = '';
    try {
      cssContent = await fs.readFile(cssPath, "utf-8");
    } catch {
      // CSS es opcional
      logger.pdf('No CSS file found, using default styles', { trace_id });
    }

    // Compilar template Pug
    const compile = compileFile(templatePath);
    const htmlContent = compile({
      ...data,
      embeddedCSS: cssContent ? `<style>${cssContent}</style>` : '',
    });

    return [cssContent, htmlContent];

  } catch (error) {
    if ((error as Error).message.includes('ENOENT')) {
      throw new PDFGenerationError(
        'Template compilation failed',
        'TEMPLATE_NOT_FOUND',
        `Template file not found: ${templatePath}`,
        'Verify the template name and ensure the file exists',
        404,
        trace_id
      );
    }

    throw new PDFGenerationError(
      'Template compilation failed',
      'TEMPLATE_COMPILATION_ERROR',
      `Failed to compile Pug template: ${(error as Error).message}`,
      'Check template syntax and data format, or contact support if the error persists',
      500,
      trace_id
    );
  }
}

/**
 * Adquirir p√°gina de manera segura
 */
async function acquirePageSafely(trace_id: string): Promise<any> {
  try {
    return await acquirePage();
  } catch (error) {
    throw new PDFGenerationError(
      'Browser page unavailable',
      'BROWSER_POOL_ERROR',
      `Failed to acquire browser page: ${(error as Error).message}`,
      'The system is experiencing high load. Try again in a few seconds',
      503,
      trace_id
    );
  }
}

/**
 * Renderizar PDF de manera segura
 */
async function renderPDF(page: any, htmlContent: string, trace_id: string): Promise<Buffer> {
  try {
    await page.setContent(htmlContent, { 
      waitUntil: "domcontentloaded",
      timeout: 30000 
    });
    
    const pdfGenerationStart = Date.now();
    const pdfBuffer = await page.pdf({
      format: "A4",
      printBackground: true,
      timeout: 30000
    });
    const pdfGenerationTime = Date.now() - pdfGenerationStart;

    logger.pdf('PDF rendering completed', {
      trace_id,
      pdf_generation_ms: pdfGenerationTime,
      file_size_bytes: pdfBuffer.length
    });

    return pdfBuffer;

  } catch (error) {
    const errorMessage = (error as Error).message;
    
    if (errorMessage.includes('timeout') || errorMessage.includes('Navigation timeout')) {
      throw new PDFGenerationError(
        'PDF generation timeout',
        'PDF_TIMEOUT',
        'PDF generation took too long to complete',
        'The document may be too complex. Try simplifying the content or contact support',
        408,
        trace_id
      );
    }

    if (errorMessage.includes('Protocol error') || errorMessage.includes('Target closed')) {
      throw new PDFGenerationError(
        'Browser connection lost',
        'BROWSER_CONNECTION_ERROR',
        'Connection to browser was lost during PDF generation',
        'This is usually temporary. Try again in a few seconds',
        503,
        trace_id
      );
    }

    throw new PDFGenerationError(
      'PDF rendering failed',
      'PDF_RENDER_ERROR',
      `Browser failed to render PDF: ${errorMessage}`,
      'Check your document content and try again, or contact support if the error persists',
      500,
      trace_id
    );
  }
}

/**
 * Crear error estructurado desde error gen√©rico
 */
function createStructuredPDFError(
  originalError: Error, 
  trace_id: string, 
  info?: CoreReportInfo
): PDFGenerationError {
  const message = originalError.message;
  
  // Categorizar errores comunes
  if (message.includes('EACCES')) {
    return new PDFGenerationError(
      'File access denied',
      'FILE_ACCESS_ERROR',
      'Insufficient permissions to access template files',
      'Contact your system administrator to check file permissions',
      403,
      trace_id
    );
  }

  if (message.includes('EMFILE') || message.includes('ENFILE')) {
    return new PDFGenerationError(
      'System resource exhausted',
      'RESOURCE_EXHAUSTION',
      'Too many open files or browser instances',
      'The system is overloaded. Try again in a few minutes',
      503,
      trace_id
    );
  }

  if (message.includes('heap') || message.includes('memory')) {
    return new PDFGenerationError(
      'Memory exhausted',
      'MEMORY_ERROR',
      'Insufficient memory to generate PDF',
      'The document may be too large. Try reducing content or contact support',
      507,
      trace_id
    );
  }

  // Error gen√©rico
  return new PDFGenerationError(
    'PDF generation failed',
    'UNKNOWN_ERROR',
    `Unexpected error: ${message}`,
    'An unexpected error occurred. Contact support with this trace ID if the problem persists',
    500,
    trace_id
  );
}

// Export del error personalizado para uso en routes
export { PDFGenerationError };
// src/services/pdf/routes.ts
import express from 'express';
import { generatePDF } from './generate';
import { pdfRequestSchema } from '../../validators/pdfRequestSchema';
import { validateBody } from '../../middlewares/validateBody';

const router = express.Router();

/**
 * POST /pdf
 * Accepts JSON with core_report_info and other dynamic data.
 * Returns a generated PDF document as binary.
 */
router.post('/', validateBody(pdfRequestSchema), async (req, res, next) => {
  try {
    const data = req.body;
    const info = data.core_report_info;

    const buffer = await generatePDF(data);
    const filename = info.report_file_name || 'report.pdf';

    res.set({
      'Content-Type': 'application/pdf',
      'Content-Disposition': `attachment; filename=${filename}`,
      'Content-Length': buffer.length
    });

    return res.end(buffer);
  } catch (error) {
    return next(error);
  }
});

export default router;

// src/services/pdp-production.ts
// services/pdp-production.ts

import { getGDPRService, GDPRValidationRequest } from './gdpr/gdprTokenService';
import logger from '../utils/logging';

/**
 * Production PDP (Policy Decision Point) service
 * Replaces the mock implementation with real GDPR token validation
 */

export type PDPAttributes = {
  gdpr_token: string;
  payload_hash: string;
  purpose?: string;
  subject?: string;
  user_id?: string;
};

export type PDPDecision = {
  allow: boolean;
  reason: string;
  hash_type?: 'original' | 'signed' | 'bypassed';
};

/**
 * Evaluates GDPR consent policy using the production GDPR service
 * 
 * ‚ö†Ô∏è TEMPORARILY DISABLED FOR PHASE 1 - ALWAYS RETURNS TRUE
 * TODO: Re-enable for Phase 2 after hash algorithm is fixed
 * 
 * @param attributes - Policy attributes to validate
 * @returns Policy decision with detailed reasoning
 */
export const evaluatePolicy = (attributes: PDPAttributes): PDPDecision => {
  logger.system('GDPR policy evaluation (BYPASS MODE)', {
    token_preview: attributes.gdpr_token?.substring(0, 8) + '...',
    payload_hash: attributes.payload_hash?.substring(0, 16) + '...',
    subject: attributes.subject,
    purpose: attributes.purpose,
    bypass_mode: true,
    phase: 'PHASE_1_DEVELOPMENT'
  });
  
  // ‚ö†Ô∏è PHASE 1: BYPASS ALL VALIDATION - ALWAYS ALLOW
  logger.warn('GDPR validation bypassed for Phase 1 development', {
    token_preview: attributes.gdpr_token?.substring(0, 8) + '...',
    bypass_reason: 'Hash algorithm inconsistency - to be fixed in Phase 2',
    security_impact: 'REDUCED - This is temporary for development'
  });
  
  return {
    allow: true,
    reason: 'PHASE 1: GDPR validation bypassed - hash algorithm fix pending for Phase 2',
    hash_type: 'bypassed'
  };
  
  /* 
  // üîí PHASE 2: RESTORE THIS CODE WHEN HASH IS FIXED
  const gdprService = getGDPRService();
  
  // Validate required fields
  if (!attributes.gdpr_token) {
    return {
      allow: false,
      reason: 'Missing GDPR token'
    };
  }
  
  if (!attributes.payload_hash) {
    return {
      allow: false,
      reason: 'Missing payload hash'
    };
  }
  
  if (!attributes.subject) {
    return {
      allow: false,
      reason: 'Missing subject email'
    };
  }
  
  // Validate with GDPR service
  const validationRequest: GDPRValidationRequest = {
    token: attributes.gdpr_token,
    payload_hash: attributes.payload_hash,
    recipient_email: attributes.subject,
    purpose: attributes.purpose
  };
  
  const result = gdprService.validateToken(validationRequest);
  
  logger.system('GDPR policy evaluation completed', {
    token_preview: attributes.gdpr_token.substring(0, 8) + '...',
    validation_result: result.valid,
    reason: result.reason,
    hash_type: result.hash_type
  });
  
  return {
    allow: result.valid,
    reason: result.reason,
    hash_type: result.hash_type
  };
  */
};
// src/services/pdp.ts
import crypto from 'crypto';

/**
 * PDP (Policy Decision Point) service for evaluating GDPR consent policies.
 * This service checks if a given set of attributes meets the policy requirements.
 */
export type PDPAttributes = {
  gdpr_token: string;
  payload_hash: string;
  purpose?: string;
  expiration?: string; // ISO string
  subject?: string;
  user_id?: string;
};

// Represents the decision made by the PDP service
// It indicates whether the request is allowed or denied, along with a reason.
// The decision is based on the evaluation of the provided attributes against the policy.
// The decision can be used to enforce access control or compliance with GDPR regulations.
export type PDPDecision = {
  allow: boolean;
  reason: string;
};

/**
 * Mock consent database for testing double validation flow
 * 
 * In production, this would be a real database with consent records.
 * Each entry represents a GDPR consent with both original and signed payload hashes.
 * 
 * The system validates twice:
 * 1. First validation: original payload hash (before PDF signing)
 * 2. Second validation: signed payload hash (after PDF signing)
 * 
 * Both hashes must be pre-registered for the same consent token.
 */
const mockConsentDatabase = new Map<string, { 
  original_hash: string; 
  signed_hash: string; 
  expiresAt: string; 
  subject: string;
  purpose: string;
}>([
  ['token-gdpr-hash08a0', {
    // Hash of payload with original PDF path
    original_hash: '2033748d2d308f33a1350741264822a5a2e62f2747681193f8674abd0c861720',
    // Hash of payload with _signed.pdf path  
    signed_hash: '7515da9f7b87ae50786c68288e1c70aebc54ac0b3a56bfeb11673ec62925ea54',
    expiresAt: '2099-12-31T23:59:59.000Z',
    subject: 'alejandro.prado@coretechnology.ie',
    purpose: 'email_notification'
  }]
]);

/**
 * Evaluates the provided attributes against the GDPR consent policy.
 * 
 * This function implements Zero Trust validation by checking:
 * 1. Valid GDPR token exists in consent database
 * 2. Payload hash matches either original_hash OR signed_hash
 * 3. Consent has not expired
 * 4. Subject (email recipient) matches registered consent
 * 5. Purpose matches registered consent purpose
 * 
 * @param attributes - The attributes to evaluate, including gdpr_token, payload_hash, and optional fields.
 * @returns A PDPDecision indicating whether the request is allowed or denied, along with a reason.
 */
export const evaluatePolicy = (attributes: PDPAttributes): PDPDecision => {
  
  // Check if consent record exists for this token
  const consent = mockConsentDatabase.get(attributes.gdpr_token);
  if (!consent) {
    return { 
      allow: false, 
      reason: 'Invalid or expired gdpr_token - no consent record found' 
    };
  }

  // Validate payload hash matches either original or signed version
  // This allows both first validation (original) and second validation (signed) to pass
  const hashMatches = (
    consent.original_hash === attributes.payload_hash || 
    consent.signed_hash === attributes.payload_hash
  );
  
  if (!hashMatches) {
    return { 
      allow: false, 
      reason: `Payload hash does not match registered consent. Expected: ${consent.original_hash} (original) or ${consent.signed_hash} (signed), got: ${attributes.payload_hash}` 
    };
  }

  // Check if consent has expired
  if (new Date(consent.expiresAt) < new Date()) {
    return { 
      allow: false, 
      reason: 'Consent has expired' 
    };
  }

  // Validate subject (email recipient) matches
  if (attributes.subject && consent.subject !== attributes.subject) {
    return { 
      allow: false, 
      reason: `Subject mismatch. Expected: ${consent.subject}, got: ${attributes.subject}` 
    };
  }

  // Validate purpose matches (if specified)
  if (attributes.purpose && consent.purpose !== attributes.purpose) {
    return { 
      allow: false, 
      reason: `Purpose mismatch. Expected: ${consent.purpose}, got: ${attributes.purpose}` 
    };
  }

  // All validations passed
  const hashType = consent.original_hash === attributes.payload_hash ? 'original' : 'signed';
  
  return { 
    allow: true, 
    reason: `Consent valid and policy conditions met (${hashType} payload hash)` 
  };
};
// src/services/pep.ts
// src/services/pep.ts
//import { evaluatePolicy, PDPAttributes } from './pdp';
import { evaluatePolicy, PDPAttributes } from './pdp-production';
import { z } from 'zod';
//import { createHash } from 'crypto';
import logger from '../utils/logging';
import { generatePayloadHash } from '../utils/hashUtils';

const EmailPayloadSchema = z.object({
  to: z.string().email(),
  subject: z.string().min(1).max(500),
  body: z.string().min(1),
  attachments: z
    .array(
      z.object({
        name: z.string().min(1),
        path: z.string().min(1)
      })
    )
    .optional()
});

export const enforceEmailPolicy = (
  payload: unknown,
  gdpr_token: string
): { allowed: boolean; reason: string; hash?: string } => {
  
  // 1. Validaci√≥n estricta del contenido
  const validation = EmailPayloadSchema.safeParse(payload);
  if (!validation.success) {
    logger.warn('Email payload failed schema validation', {
      operation: 'EMAIL_OPERATION',
      reason: validation.error.message,
      validation_errors: validation.error.errors?.length || 0
    });
    return {
      allowed: false,
      reason: 'Schema validation failed'
    };
  }

  const validated = validation.data;
  
  // SECURE: Only log payload details in verbose mode
  logger.debug('Email payload validated', {
    operation: 'EMAIL_OPERATION',
    recipient_domain: validated.to.split('@')[1],
    subject_length: validated.subject.length,
    body_length: validated.body.length,
    attachments_count: validated.attachments?.length || 0,
    attachment_names: validated.attachments?.map(a => a.name) || [],
    // VERBOSE ONLY: Full payload for debugging
    ...(logger.isVerbose() && {
      verbose_validated_payload: validated
    })
  });

  // 2. Ordenar y hashear el payload de forma determinista
  const hash = generatePayloadHash(validated);
  
  logger.debug('Payload hash generated', {
    operation: 'EMAIL_OPERATION',
    hash,
    hash_algorithm: 'SHA-256',
    payload_keys: Object.keys(validated)
  });

  // 3. Atributos para PDP (modo estricto)
  const attributes: PDPAttributes = {
    gdpr_token,
    payload_hash: hash,
    subject: validated.to,
    purpose: 'email_notification',
    //expiration: undefined, // future: from consent registry
    user_id: process.env.TENANT_CLIENT_ID
  };

  // 4. Consulta a PDP
  const decision = evaluatePolicy(attributes);

  // 5. Logging estructurado con informaci√≥n de decisi√≥n
  logger.info('ABAC decision evaluated', {
    operation: 'EMAIL_OPERATION',
    user_id: process.env.TENANT_CLIENT_ID,
    hash,
    decision_allowed: decision.allow,
    decision_reason: decision.reason,
    gdpr_token_length: gdpr_token?.length || 0,
    purpose: attributes.purpose,
    // VERBOSE ONLY: Full decision details
    ...(logger.isVerbose() && {
      verbose_full_attributes: attributes,
      verbose_full_decision: decision,
      verbose_gdpr_token: gdpr_token
    })
  });

  return {
    allowed: decision.allow,
    reason: decision.reason,
    hash
  };
};


// Ordena las claves de objetos recursivamente
function sortObjectRecursively(obj: any): any {
  if (Array.isArray(obj)) {
    return obj.map(sortObjectRecursively);
  } else if (obj !== null && typeof obj === 'object') {
    return Object.keys(obj)
      .sort()
      .reduce((result: any, key) => {
        result[key] = sortObjectRecursively(obj[key]);
        return result;
      }, {});
  }
  return obj;
}
// src/services/serviceContainer.ts
// src/services/serviceContainer.ts
import { EmailServiceConfig } from './email/emailService';
import { TokenServiceConfig } from './email/tokenService';
import { PdfSigningConfig } from '../types/pdfTypes';
import { AuthConfig } from '../utils/getToken';
import { initializeBrowserPool } from '../config/browserPool';

/**
 * Browser Pool Configuration
 */
export interface BrowserPoolConfig {
  maxBrowsers: number;
  maxPagesPerBrowser: number;
  pageIdleTimeout: number;
}

/**
 * Service Container for Dependency Injection
 * Centralizes all service configurations and provides clean access
 */
export class ServiceContainer {
  private emailConfig: EmailServiceConfig;
  private jwtSecret: string;
  private pdfSigningConfig: PdfSigningConfig;
  private authConfig: AuthConfig;
  private browserPoolConfig: BrowserPoolConfig;

  constructor(appConfig: any) {
    // Email service configuration
    this.emailConfig = {
      senderEmail: appConfig.senderEmail,
      tokenEndpoint: appConfig.tokenEndpoint,
      tenantId: appConfig.tenantId,
      clientId: appConfig.clientId,
      clientSecret: appConfig.clientSecret
    };

    // JWT configuration
    this.jwtSecret = appConfig.jwtSecret;

    // PDF signing configuration
    this.pdfSigningConfig = {
      certPdfSignPath: appConfig.certPdfSignPath,
      certPdfSignPassword: appConfig.certPdfSignPassword,
      certPdfSignType: appConfig.certPdfSignType || 'p12'
    };

    // Auth configuration
    this.authConfig = {
      authUrl: appConfig.authFullUrl,
      authUsername: appConfig.authUsername,
      authPassword: appConfig.authPassword
    };

    // Browser Pool configuration
    this.browserPoolConfig = {
      maxBrowsers: appConfig.maxBrowsers || 2,
      maxPagesPerBrowser: appConfig.maxPagesPerBrowser || 3,
      pageIdleTimeout: appConfig.pageIdleTimeout || 300000
    };
  }

  /**
   * Get Email Service Configuration
   */
  getEmailConfig(): EmailServiceConfig {
    return this.emailConfig;
  }

  /**
   * Get Token Service Configuration
   */
  getTokenConfig(): TokenServiceConfig {
    return {
      tokenEndpoint: this.emailConfig.tokenEndpoint,
      tenantId: this.emailConfig.tenantId,
      clientId: this.emailConfig.clientId,
      clientSecret: this.emailConfig.clientSecret
    };
  }

  /**
   * Get JWT Secret
   */
  getJwtSecret(): string {
    return this.jwtSecret;
  }

  /**
   * Get PDF Signing Configuration
   */
  getPdfSigningConfig(): PdfSigningConfig {
    return this.pdfSigningConfig;
  }

  /**
   * Get Auth Configuration
   */
  getAuthConfig(): AuthConfig {
    return this.authConfig;
  }

  /**
   * Get Browser Pool Configuration
   */
  getBrowserPoolConfig(): BrowserPoolConfig {
    return this.browserPoolConfig;
  }
}

// Global service container instance
let serviceContainer: ServiceContainer | null = null;

/**
 * Initialize the service container with app configuration
 * Should be called once during app startup
 */
export async function initServiceContainer(appConfig: any): Promise<ServiceContainer> {
  serviceContainer = new ServiceContainer(appConfig);
  
  // Inicializar browser pool con la configuraci√≥n del container
  await initializeBrowserPool(serviceContainer.getBrowserPoolConfig());
  
  console.log('üåê Browser pool initialized with config:', {
    maxBrowsers: serviceContainer.getBrowserPoolConfig().maxBrowsers,
    maxPagesPerBrowser: serviceContainer.getBrowserPoolConfig().maxPagesPerBrowser,
    pageIdleTimeout: serviceContainer.getBrowserPoolConfig().pageIdleTimeout
  });
  
  return serviceContainer;
}

/**
 * Get the initialized service container
 * Throws error if not initialized
 */
export function getServiceContainer(): ServiceContainer {
  if (!serviceContainer) {
    throw new Error('Service container not initialized. Call initServiceContainer() first.');
  }
  return serviceContainer;
}
// src/services/zpl/generate.ts
// src/services/zpl/generate.ts - VERSI√ìN MEJORADA
import fs from 'fs';
import path from 'path';
import mustache from 'mustache';
import { paths } from "../../config/paths";
import { v4 as uuidv4 } from 'uuid';
import logger from '../../utils/logging';

/**
 * Estructura de error mejorada para ZPL generation
 */
class ZPLGenerationError extends Error {
  public readonly code: string;
  public readonly details: string;
  public readonly userAction: string;
  public readonly statusCode: number;
  public readonly traceId: string;

  constructor(
    message: string,
    code: string,
    details: string,
    userAction: string,
    statusCode: number = 500,
    traceId: string
  ) {
    super(message);
    this.name = 'ZPLGenerationError';
    this.code = code;
    this.details = details;
    this.userAction = userAction;
    this.statusCode = statusCode;
    this.traceId = traceId;
  }
}

/**
 * Generates a ZPL string using a mustache template and dynamic data.
 * @param data Input JSON containing `core_report_info` and custom data
 * @returns Rendered ZPL string
 * @throws ZPLGenerationError if template is missing or cannot be rendered
 */
export const generateZPL = async (data: any): Promise<string> => {
  const startTime = Date.now();
  const trace_id = uuidv4();
  const templateName = data.core_report_info?.report_template;
  
  logger.zpl('Starting ZPL generation', {
    trace_id,
    correlation_id: trace_id,
    client_name: data.client_name,
    template_name: templateName,
    report_name: data.core_report_info?.report_name,
    report_file_name: data.core_report_info?.report_file_name
  });

  // VALIDACI√ìN MEJORADA
  try {
    validateZPLRequest(data, trace_id);
  } catch (error) {
    if (error instanceof ZPLGenerationError) {
      throw error;
    }
    throw new ZPLGenerationError(
      'ZPL request validation failed',
      'VALIDATION_ERROR',
      (error as Error).message,
      'Check your request format and ensure all required fields are provided',
      400,
      trace_id
    );
  }

  const templatePath = path.join(paths.zpl.templatePath, `${templateName}.zpl`);
  
  try {
    // PASO 1: Verificar template existe
    logger.zpl('Verifying template file', {
      trace_id,
      template_path: templatePath,
      template_name: templateName,
      duration_ms: Date.now() - startTime
    });

    await validateTemplateExists(templatePath, templateName, trace_id);

    // PASO 2: Leer template
    logger.zpl('Reading template file', {
      trace_id,
      template_path: templatePath,
      duration_ms: Date.now() - startTime
    });

    const zplTemplate = await readTemplateSafely(templatePath, trace_id);
    const templateSize = zplTemplate.length;
    
    // PASO 3: Validar template content
    validateTemplateContent(zplTemplate, templateName, trace_id);
    
    // PASO 4: Renderizar con Mustache
    logger.zpl('Rendering template with Mustache', {
      trace_id,
      template_size_bytes: templateSize,
      template_size_chars: templateSize,
      data_keys: Object.keys(data).length,
      duration_ms: Date.now() - startTime
    });

    const zplRendered = await renderTemplateSafely(zplTemplate, data, templateName, trace_id);
    
    // PASO 5: Validar output
    validateZPLOutput(zplRendered, trace_id);
    
    // PASO 6: M√©tricas finales
    const totalDuration = Date.now() - startTime;
    const outputSize = zplRendered.length;
    const compressionRatio = templateSize > 0 ? (outputSize / templateSize).toFixed(2) : '0';
    const labelCount = countZPLLabels(zplRendered);
    
    logger.zpl('ZPL generated successfully', {
      trace_id,
      duration_ms: totalDuration,
      template_name: templateName,
      template_size_bytes: templateSize,
      output_size_bytes: outputSize,
      output_size_chars: outputSize,
      compression_ratio: compressionRatio,
      zpl_labels: labelCount,
      estimated_labels: labelCount,
      report_name: data.core_report_info?.report_name,
      report_file_name: data.core_report_info?.report_file_name
    });

    return zplRendered;
    
  } catch (error) {
    const errorDuration = Date.now() - startTime;
    
    // Si ya es nuestro error estructurado, re-lanzar
    if (error instanceof ZPLGenerationError) {
      logger.zpl('ZPL generation failed with structured error', {
        trace_id,
        duration_ms: errorDuration,
        error_code: error.code,
        error_message: error.message,
        template_name: templateName
      });
      throw error;
    }
    
    // Convertir errores no estructurados
    const structuredError = createStructuredZPLError(error as Error, trace_id, templateName);
    
    logger.zpl('ZPL generation failed', {
      trace_id,
      duration_ms: errorDuration,
      error_code: structuredError.code,
      error_message: structuredError.message,
      original_error: (error as Error).message,
      template_name: templateName
    });
    
    throw structuredError;
  }
};

/**
 * Validaci√≥n mejorada del request
 */
function validateZPLRequest(data: any, trace_id: string): void {
  if (!data.core_report_info) {
    throw new ZPLGenerationError(
      'Missing core_report_info',
      'MISSING_REPORT_INFO',
      'The core_report_info object is required but was not provided',
      'Include core_report_info in your request with the report_template field',
      400,
      trace_id
    );
  }

  const templateName = data.core_report_info.report_template;
  if (!templateName) {
    throw new ZPLGenerationError(
      'Missing template name',
      'MISSING_TEMPLATE_NAME',
      'report_template field is required in core_report_info',
      'Specify a valid report_template name in core_report_info',
      400,
      trace_id
    );
  }

  // Validar caracteres peligrosos
  if (!/^[a-zA-Z0-9_-]+$/.test(templateName)) {
    throw new ZPLGenerationError(
      'Invalid template name',
      'INVALID_TEMPLATE_NAME',
      'Template name contains invalid characters',
      'Use only alphanumeric characters, underscores, and hyphens in template names',
      400,
      trace_id
    );
  }

  // Validar longitud del nombre
  if (templateName.length > 50) {
    throw new ZPLGenerationError(
      'Template name too long',
      'TEMPLATE_NAME_TOO_LONG',
      'Template name exceeds maximum length of 50 characters',
      'Use a shorter template name',
      400,
      trace_id
    );
  }
}

/**
 * Validar que el template existe
 */
async function validateTemplateExists(templatePath: string, templateName: string, trace_id: string): Promise<void> {
  try {
    const stats = await fs.promises.stat(templatePath);
    
    if (!stats.isFile()) {
      throw new ZPLGenerationError(
        'Template not found',
        'TEMPLATE_NOT_FILE',
        `Template path exists but is not a file: ${templatePath}`,
        'Ensure the template is a valid file, not a directory',
        404,
        trace_id
      );
    }

    // Verificar que sea legible
    await fs.promises.access(templatePath, fs.constants.R_OK);
    
  } catch (error) {
    if ((error as any).code === 'ENOENT') {
      throw new ZPLGenerationError(
        'Template not found',
        'TEMPLATE_NOT_FOUND',
        `ZPL template file does not exist: ${templatePath}`,
        `Ensure the template '${templateName}.zpl' exists in the templates directory, or contact your administrator`,
        404,
        trace_id
      );
    }

    if ((error as any).code === 'EACCES') {
      throw new ZPLGenerationError(
        'Template access denied',
        'TEMPLATE_ACCESS_DENIED',
        `Insufficient permissions to read template: ${templatePath}`,
        'Contact your system administrator to check file permissions',
        403,
        trace_id
      );
    }

    // Si ya es nuestro error, re-lanzar
    if (error instanceof ZPLGenerationError) {
      throw error;
    }

    throw new ZPLGenerationError(
      'Template validation failed',
      'TEMPLATE_VALIDATION_ERROR',
      `Failed to validate template: ${(error as Error).message}`,
      'Contact support if this problem persists',
      500,
      trace_id
    );
  }
}

/**
 * Leer template de manera segura
 */
async function readTemplateSafely(templatePath: string, trace_id: string): Promise<string> {
  try {
    const templateContent = await fs.promises.readFile(templatePath, 'utf-8');
    
    if (templateContent.length === 0) {
      throw new ZPLGenerationError(
        'Empty template file',
        'EMPTY_TEMPLATE',
        'Template file exists but contains no content',
        'Check the template file and ensure it contains valid ZPL content',
        400,
        trace_id
      );
    }

    return templateContent;
    
  } catch (error) {
    if (error instanceof ZPLGenerationError) {
      throw error;
    }

    throw new ZPLGenerationError(
      'Failed to read template',
      'TEMPLATE_READ_ERROR',
      `Error reading template file: ${(error as Error).message}`,
      'Contact your system administrator to check file permissions and disk space',
      500,
      trace_id
    );
  }
}

/**
 * Validar contenido del template
 */
function validateTemplateContent(templateContent: string, templateName: string, trace_id: string): void {
  // Validar que contiene comandos ZPL b√°sicos
  if (!templateContent.includes('^XA') && !templateContent.includes('^xa')) {
    throw new ZPLGenerationError(
      'Invalid ZPL template',
      'INVALID_ZPL_CONTENT',
      'Template does not contain ZPL start command (^XA)',
      'Ensure the template contains valid ZPL commands starting with ^XA',
      400,
      trace_id
    );
  }

  // Validar tama√±o razonable
  if (templateContent.length > 1024 * 1024) { // 1MB
    throw new ZPLGenerationError(
      'Template too large',
      'TEMPLATE_TOO_LARGE',
      'Template file exceeds maximum size of 1MB',
      'Use a smaller template or split into multiple templates',
      413,
      trace_id
    );
  }

  logger.zpl('Template content validated', {
    trace_id,
    template_name: templateName,
    content_length: templateContent.length,
    has_start_command: templateContent.includes('^XA') || templateContent.includes('^xa')
  });
}

/**
 * Renderizar template con Mustache de manera segura
 */
async function renderTemplateSafely(
  templateContent: string, 
  data: any, 
  templateName: string, 
  trace_id: string
): Promise<string> {
  try {
    const renderStartTime = Date.now();
    
    // Validar que data es serializable
    try {
      JSON.stringify(data);
    } catch (error) {
      throw new ZPLGenerationError(
        'Invalid data format',
        'DATA_NOT_SERIALIZABLE',
        'Template data contains non-serializable content',
        'Ensure all data values are JSON-serializable (no functions, circular references, etc.)',
        400,
        trace_id
      );
    }

    const zplRendered = mustache.render(templateContent, data);
    const renderDuration = Date.now() - renderStartTime;
    
    logger.zpl('Mustache rendering completed', {
      trace_id,
      template_name: templateName,
      render_duration_ms: renderDuration,
      input_size: templateContent.length,
      output_size: zplRendered.length
    });

    return zplRendered;
    
  } catch (error) {
    const errorMessage = (error as Error).message;
    
    if (error instanceof ZPLGenerationError) {
      throw error;
    }

    // Errores espec√≠ficos de Mustache
    if (errorMessage.includes('Unclosed tag') || errorMessage.includes('Unopened tag')) {
      throw new ZPLGenerationError(
        'Template syntax error',
        'MUSTACHE_SYNTAX_ERROR',
        `Mustache template syntax error: ${errorMessage}`,
        'Check template syntax for unmatched {{}} tags and correct them',
        400,
        trace_id
      );
    }

    if (errorMessage.includes('Maximum call stack') || errorMessage.includes('RangeError')) {
      throw new ZPLGenerationError(
        'Template too complex',
        'TEMPLATE_COMPLEXITY_ERROR',
        'Template is too complex or contains circular references',
        'Simplify the template or check for recursive references in your data',
        400,
        trace_id
      );
    }

    throw new ZPLGenerationError(
      'Template rendering failed',
      'MUSTACHE_RENDER_ERROR',
      `Failed to render template: ${errorMessage}`,
      'Check template syntax and data format, or contact support if the error persists',
      500,
      trace_id
    );
  }
}

/**
 * Validar output ZPL
 */
function validateZPLOutput(zplOutput: string, trace_id: string): void {
  if (zplOutput.length === 0) {
    throw new ZPLGenerationError(
      'Empty ZPL output',
      'EMPTY_OUTPUT',
      'Template rendering produced no output',
      'Check that your template data contains the required values',
      500,
      trace_id
    );
  }

  // Validar tama√±o razonable del output
  if (zplOutput.length > 10 * 1024 * 1024) { // 10MB
    throw new ZPLGenerationError(
      'ZPL output too large',
      'OUTPUT_TOO_LARGE',
      'Generated ZPL exceeds maximum size of 10MB',
      'Reduce the amount of data or use multiple smaller labels',
      413,
      trace_id
    );
  }

  logger.zpl('ZPL output validated', {
    trace_id,
    output_size: zplOutput.length,
    validation_passed: true
  });
}

/**
 * Contar labels en el ZPL
 */
function countZPLLabels(zplContent: string): number {
  // Contar comandos ^XA que inician una label
  const matches = zplContent.match(/\^XA/gi) || [];
  return matches.length;
}

/**
 * Crear error estructurado desde error gen√©rico
 */
function createStructuredZPLError(
  originalError: Error, 
  trace_id: string, 
  templateName?: string
): ZPLGenerationError {
  const message = originalError.message;
  
  // Categorizar errores comunes
  if (message.includes('EACCES')) {
    return new ZPLGenerationError(
      'File access denied',
      'FILE_ACCESS_ERROR',
      'Insufficient permissions to access template files',
      'Contact your system administrator to check file permissions',
      403,
      trace_id
    );
  }

  if (message.includes('ENOENT')) {
    return new ZPLGenerationError(
      'Template not found',
      'TEMPLATE_NOT_FOUND',
      `Template file not found: ${templateName || 'unknown'}`,
      'Verify the template name and ensure the file exists',
      404,
      trace_id
    );
  }

  if (message.includes('EMFILE') || message.includes('ENFILE')) {
    return new ZPLGenerationError(
      'System resource exhausted',
      'RESOURCE_EXHAUSTION',
      'Too many open files',
      'The system is overloaded. Try again in a few minutes',
      503,
      trace_id
    );
  }

  if (message.includes('heap') || message.includes('memory')) {
    return new ZPLGenerationError(
      'Memory exhausted',
      'MEMORY_ERROR',
      'Insufficient memory to generate ZPL',
      'The template or data may be too large. Try reducing content size',
      507,
      trace_id
    );
  }

  // Error gen√©rico
  return new ZPLGenerationError(
    'ZPL generation failed',
    'UNKNOWN_ERROR',
    `Unexpected error: ${message}`,
    'An unexpected error occurred. Contact support with this trace ID if the problem persists',
    500,
    trace_id
  );
}

// Export del error personalizado para uso en routes
export { ZPLGenerationError };
// src/services/zpl/routes.ts
import express from 'express';
import { generateZPL } from './generate';
import { zplRequestSchema } from '../../validators/zplRequestSchema';
import { validateBody } from '../../middlewares/validateBody';

const router = express.Router();

/**
 * POST /zpl
 * Accepts JSON with core_report_info and other dynamic data.
 * Returns a generated ZPL string as a plain text file.
 */
router.post('/', validateBody(zplRequestSchema), async (req, res, next) => {
  try {
    const data = req.body;
    const zpl = await generateZPL(data);

    res.set({
      'Content-Type': 'text/plain',
      'Content-Disposition': 'attachment; filename=etiquetas.zpl.txt',
      'Content-Length': Buffer.byteLength(zpl),
    });

    return res.end(zpl);
  } catch (error) {
    return next(error);
  }
});

export default router;

// src/types/iso27001.ts
// src/types/iso27001.ts

/**
 * ISO 27001 Annex A.8.2 - Information Classification Levels
 * 
 * These classification levels map directly to ISO 27001 security controls
 * and determine the appropriate security measures for each email.
 */
export type ISO27001Classification = 'internal' | 'confidential' | 'restricted';

/**
 * ISO 27001 Security Control Mapping
 * 
 * - internal: A.9.4.1 (Information access restriction)
 * - confidential: A.9.4.1 + A.13.2.1 (Information transfer)  
 * - restricted: A.9.4.1 + A.13.2.1 + A.13.2.3 (Electronic messaging with digital signatures)
 */
export interface ISO27001SecurityControls {
  accessRestriction: boolean;      // A.9.4.1
  informationTransfer: boolean;    // A.13.2.1
  electronicMessaging: boolean;    // A.13.2.3
  auditLogging: boolean;          // A.12.4.1
}

/**
 * Maps ISO classification to required security controls
 */
export const getSecurityControls = (classification: ISO27001Classification): ISO27001SecurityControls => {
  switch (classification) {
    case 'internal':
      return {
        accessRestriction: true,
        informationTransfer: false,
        electronicMessaging: false,
        auditLogging: true
      };
    case 'confidential':
      return {
        accessRestriction: true,
        informationTransfer: true,
        electronicMessaging: false,
        auditLogging: true
      };
    case 'restricted':
      return {
        accessRestriction: true,
        informationTransfer: true,
        electronicMessaging: true,
        auditLogging: true
      };
    default:
      throw new Error(`Invalid ISO 27001 classification: ${classification}`);
  }
};
// src/types/pdfTypes.ts
// src/types/pdfTypes.ts

export interface PdfSigningConfig {
    certPdfSignPath: string;
    certPdfSignPassword: string;
    certPdfSignType: 'p12' | 'pem';
  }
// src/utils/emailHashTest.ts
// utils/emailHashTest.ts

import { createHash } from 'crypto';

function sortObjectRecursively(obj: any): any {
    if (Array.isArray(obj)) {
      return obj.map(sortObjectRecursively);
    } else if (obj !== null && typeof obj === 'object') {
      return Object.keys(obj)
        .sort()
        .reduce((result: any, key) => {
          result[key] = sortObjectRecursively(obj[key]);
          return result;
        }, {});
    }
    return obj;
  }

const payload = {
    to: 'alejandro.prado@coretechnology.ie',
    subject: 'Email Test from nodejs',
    body: 'Hello there using OAuth2 and SMTP. With attachments',
    attachments: [
      {
        name: 'PrescriptionAuth_473_20250512_085309.pdf',
        path: '//cul-cor-app01/CoreSoftware/DEV/Dockets/PrescriptionAuth_473_20250512_085309.pdf'
      }
    ]
  };

  const canonical = JSON.stringify(sortObjectRecursively(payload));
const hash = createHash('sha256').update(canonical).digest('hex');
console.log(hash);

// src/utils/getToken.ts
import axios from "axios";
import { getServiceContainer } from "../services/serviceContainer";
import logger from "./logging";

let cachedToken = "";
let tokenExpiresAt = 0;

export interface AuthConfig {
  authUrl: string;
  authUsername: string;
  authPassword: string;
}

export async function getAuthToken(): Promise<string> {
  const startTime = Date.now();
  const now = Date.now();

  // Check if we have a valid cached token
  if (cachedToken && now < tokenExpiresAt) {
    logger.auth('Using cached auth token', {
      token_cached: true,
      expires_in_ms: tokenExpiresAt - now,
      expires_in_minutes: Math.round((tokenExpiresAt - now) / 60000)
    });
    return cachedToken;
  }

  try {
    // Get auth config from service container
    const container = getServiceContainer();
    const authConfig = container.getAuthConfig();
    
    logger.auth('Requesting new auth token', {
      auth_url: authConfig.authUrl,
      auth_username: authConfig.authUsername,
      has_auth_password: !!authConfig.authPassword,
      password_length: authConfig.authPassword?.length || 0,
      token_expired: cachedToken && now >= tokenExpiresAt,
      had_cached_token: !!cachedToken,
      // VERBOSE ONLY: Show more details for debugging
      ...(logger.isVerbose() && {
        verbose_auth_url: authConfig.authUrl,
        verbose_username: authConfig.authUsername,
        verbose_password_masked: authConfig.authPassword ? 
          authConfig.authPassword.substring(0, 3) + '*'.repeat(authConfig.authPassword.length - 3) : 
          'NOT_SET'
      })
    });

    if (!authConfig.authUrl || !authConfig.authUsername || !authConfig.authPassword) {
      logger.auth('Authentication configuration incomplete', {
        error_code: 'MISSING_AUTH_CONFIG',
        has_url: !!authConfig.authUrl,
        has_username: !!authConfig.authUsername,
        has_password: !!authConfig.authPassword,
        duration_ms: Date.now() - startTime
      });
      throw new Error("Missing authentication configuration");
    }

    const authRequestStart = Date.now();
    
    logger.auth('Sending authentication request', {
      auth_url: authConfig.authUrl,
      request_payload_keys: ['username', 'password'],
      duration_ms: Date.now() - startTime
    });

    const response = await axios.post(`${authConfig.authUrl}`, {
      username: authConfig.authUsername,
      password: authConfig.authPassword,
    });

    const authRequestDuration = Date.now() - authRequestStart;

    if (response.status !== 200 || !response.data?.token) {
      logger.auth('Invalid response from auth server', {
        error_code: 'INVALID_AUTH_RESPONSE',
        status_code: response.status,
        has_token: !!response.data?.token,
        has_response_data: !!response.data,
        auth_request_duration_ms: authRequestDuration,
        total_duration_ms: Date.now() - startTime
      });
      throw new Error("Invalid response from auth server");
    }

    cachedToken = response.data.token;
    
    const expiresIn = response.data.expiresIn || 3600; // en segundos
    tokenExpiresAt = now + expiresIn * 1000 - 10000; // 10s de margen
    
    // Calculate token info for logging (but never log the actual token)
    const tokenLength = cachedToken.length;
    const tokenPrefix = cachedToken.substring(0, 8);
    const tokenSuffix = cachedToken.substring(cachedToken.length - 4);
    
    logger.auth('Auth token received successfully', {
      token_length: tokenLength,
      token_preview: `${tokenPrefix}...${tokenSuffix}`,
      expires_in_seconds: expiresIn,
      expires_in_minutes: Math.round(expiresIn / 60),
      expires_at: new Date(tokenExpiresAt).toISOString(),
      auth_request_duration_ms: authRequestDuration,
      total_duration_ms: Date.now() - startTime,
      // VERBOSE ONLY: More token details for debugging
      ...(logger.isVerbose() && {
        verbose_token_full: cachedToken, // ONLY in verbose mode
        verbose_response_keys: Object.keys(response.data || {}),
        verbose_response_status: response.status,
        verbose_response_headers_content_type: response.headers['content-type']
      })
    });
    
    return cachedToken;
    
  } catch (err: any) {
    const errorDuration = Date.now() - startTime;
    const isAxiosError = err.response;
    
    logger.auth('Authentication failed', {
      error_code: 'AUTHENTICATION_FAILED',
      error_message: err.message,
      duration_ms: errorDuration,
      is_network_error: !isAxiosError,
      ...(isAxiosError && {
        response_status: err.response?.status,
        response_status_text: err.response?.statusText,
        response_data_available: !!err.response?.data
      }),
      // VERBOSE ONLY: Detailed error info
      ...(logger.isVerbose() && {
        verbose_error_stack: err.stack,
        verbose_response_data: err.response?.data,
        verbose_request_config: {
          url: err.config?.url,
          method: err.config?.method,
          timeout: err.config?.timeout
        }
      })
    });

    throw new Error("Authentication failed");
  }
}
// src/utils/hashUtils.ts
// utils/hashUtils.ts

import { createHash } from 'crypto';

/**
 * Shared hash utility for consistent hashing across all services
 * 
 * This ensures that the GDPR token generation and email validation
 * use exactly the same hashing algorithm.
 */

/**
 * Sort object keys recursively for consistent hashing
 * This is the SINGLE SOURCE OF TRUTH for object sorting
 */
export function sortObjectRecursively(obj: any): any {
  if (Array.isArray(obj)) {
    return obj.map(sortObjectRecursively);
  } else if (obj !== null && typeof obj === 'object') {
    return Object.keys(obj)
      .sort()
      .reduce((result: any, key) => {
        result[key] = sortObjectRecursively(obj[key]);
        return result;
      }, {});
  }
  return obj;
}

/**
 * Generate consistent SHA-256 hash of any payload
 * This is the SINGLE SOURCE OF TRUTH for payload hashing
 */
export function generatePayloadHash(payload: any): string {
  const canonicalJson = JSON.stringify(sortObjectRecursively(payload));
  return createHash('sha256').update(canonicalJson).digest('hex');
}
// src/utils/logging/core/config.ts
/**
 * üîß CORE-SERVICES: Logger Configuration
 * 
 * Configuration utilities and helpers for the logging system
 * Centralized configuration logic following SOLID principles
 * 
 * Classification: INTERNAL (service infrastructure)
 */

import path from 'path';
import fs from 'fs';
import { LogMode, LogLevel, LoggerConfig } from './types';

/**
 * Sensitive field patterns for data sanitization
 * More specific to core-services operations
 */
export const SENSITIVE_PATTERNS = [
  /password/i,
  /secret/i,
  /token/i,
  /key/i,
  /auth/i,
  /credential/i,
  /private/i,
  /email/i,
  /mail/i,
  /recipient/i,
  /sender/i,
  /subject/i,
  /body/i,
  /content/i,
  /payload/i,
  /data/i
];

/**
 * Determine log mode from environment
 */
export const getLogMode = (): LogMode => {
  const mode = process.env.LOG_LEVEL?.toLowerCase();
  return (mode === 'verbose' || mode === 'debug') ? 'verbose' : 'normal';
};

/**
 * Get effective log level based on mode and environment
 */
export const getLogLevel = (): LogLevel => {
  const mode = getLogMode();
  const envLevel = process.env.LOG_LEVEL?.toUpperCase();
  
  if (mode === 'verbose') {
    return 'DEBUG';
  }
  
  // Normal mode levels
  const validLevels: LogLevel[] = ['INFO', 'WARN', 'ERROR'];
  if (envLevel && validLevels.includes(envLevel as LogLevel)) {
    return envLevel as LogLevel;
  }
  
  return process.env.NODE_ENV === 'production' ? 'INFO' : 'DEBUG';
};

/**
 * Get service name from environment or default
 */
export const getServiceName = (): string => {
  return process.env.SERVICE_NAME || 'core-services';
};

/**
 * Get environment name
 */
export const getEnvironment = (): string => {
  return process.env.NODE_ENV || 'development';
};

/**
 * Create logs directory lazily and safely
 */
export const ensureLogsDirectory = (): string => {
  const logsDir = path.join(process.cwd(), 'logs');
  if (!fs.existsSync(logsDir)) {
    fs.mkdirSync(logsDir, { recursive: true });
  }
  return logsDir;
};

/**
 * Create complete logger configuration
 */
export const createLoggerConfig = (): LoggerConfig => {
  return {
    mode: getLogMode(),
    level: getLogLevel(),
    environment: getEnvironment(),
    service: getServiceName(),
    logsDirectory: ensureLogsDirectory()
  };
};

/**
 * Sanitize sensitive data based on log mode
 * Single Responsibility: Only handles data sanitization
 */
export const sanitizeLogData = (data: any, mode: LogMode): any => {
  // In verbose mode, don't sanitize anything
  if (mode === 'verbose') {
    return data;
  }
  
  if (typeof data !== 'object' || data === null) {
    return data;
  }
  
  if (Array.isArray(data)) {
    return data.map(item => sanitizeLogData(item, mode));
  }
  
  const sanitized: any = {};
  
  for (const [key, value] of Object.entries(data)) {
    const isSensitive = SENSITIVE_PATTERNS.some(pattern => pattern.test(key));
    
    if (isSensitive) {
      sanitized[key] = '[REDACTED]';
    } else if (typeof value === 'object' && value !== null) {
      sanitized[key] = sanitizeLogData(value, mode);
    } else {
      sanitized[key] = value;
    }
  }
  
  return sanitized;
};

/**
 * Format uptime to human readable string
 */
export const formatUptime = (seconds: number): string => {
  const hours = Math.floor(seconds / 3600);
  const minutes = Math.floor((seconds % 3600) / 60);
  const secs = Math.floor(seconds % 60);
  
  if (hours > 0) {
    return `${hours}h ${minutes}m ${secs}s`;
  } else if (minutes > 0) {
    return `${minutes}m ${secs}s`;
  } else {
    return `${secs}s`;
  }
};

/**
 * Validate log level
 */
export const isValidLogLevel = (level: string): level is LogLevel => {
  const validLevels: LogLevel[] = ['DEBUG', 'INFO', 'WARN', 'ERROR'];
  return validLevels.includes(level as LogLevel);
};

/**
 * Validate service operation
 */
export const isValidServiceOperation = (operation: string): boolean => {
  const validOperations = [
    'PDF_GENERATION',
    'ZPL_GENERATION', 
    'EMAIL_OPERATION',
    'SERVICE_CONTAINER',
    'AUTHENTICATION',
    'SYSTEM',
    'API_REQUEST'
  ];
  return validOperations.includes(operation);
};

/**
 * Default logger metadata
 */
export const getDefaultMeta = (config: LoggerConfig) => {
  return {
    service: config.service,
    environment: config.environment,
    mode: config.mode
  };
};
// src/utils/logging/core/logger.ts
/**
 * üîß CORE-SERVICES: Core Logger
 * 
 * Base logger implementation with core functionality
 * Follows SOLID principles with single responsibility
 * 
 * Classification: INTERNAL (service infrastructure)
 */

import { Logger } from 'winston';
import { 
  LogLevel, 
  ServiceOperation, 
  CoreServicesLogEntry, 
  ILogger, 
  ICoreServicesLogger,
  LoggerConfig 
} from './types';

/**
 * Base CoreServices Logger Class
 * Single Responsibility: Core logging functionality
 */
export class CoreServicesLogger implements ICoreServicesLogger {
  protected logger: Logger;
  protected config: LoggerConfig;
  
  constructor(logger: Logger, config: LoggerConfig) {
    this.logger = logger;
    this.config = config;
  }
  
  /**
   * Debug logging (verbose mode only)
   */
  debug(message: string, meta?: Partial<CoreServicesLogEntry>): void {
    this.log('DEBUG', message, meta);
  }
  
  /**
   * Info logging
   */
  info(message: string, meta?: Partial<CoreServicesLogEntry>): void {
    this.log('INFO', message, meta);
  }
  
  /**
   * Warning logging
   */
  warn(message: string, meta?: Partial<CoreServicesLogEntry>): void {
    this.log('WARN', message, meta);
  }
  
  /**
   * Error logging
   */
  error(message: string, meta?: Partial<CoreServicesLogEntry>): void {
    this.log('ERROR', message, meta);
  }
  
  /**
   * PDF generation logging
   */
  pdf(message: string, meta?: Partial<CoreServicesLogEntry>): void {
    this.log('INFO', message, {
      ...meta,
      operation: 'PDF_GENERATION'
    });
  }
  
  /**
   * ZPL label generation logging
   */
  zpl(message: string, meta?: Partial<CoreServicesLogEntry>): void {
    this.log('INFO', message, {
      ...meta,
      operation: 'ZPL_GENERATION'
    });
  }
  
  /**
   * Email operation logging
   */
  email(message: string, meta?: Partial<CoreServicesLogEntry>): void {
    this.log('INFO', message, {
      ...meta,
      operation: 'EMAIL_OPERATION'
    });
  }
  
  /**
   * Service container logging
   */
  container(message: string, meta?: Partial<CoreServicesLogEntry>): void {
    this.log('INFO', message, {
      ...meta,
      operation: 'SERVICE_CONTAINER'
    });
  }
  
  /**
   * Authentication logging
   */
  auth(message: string, meta?: Partial<CoreServicesLogEntry>): void {
    this.log('INFO', message, {
      ...meta,
      operation: 'AUTHENTICATION'
    });
  }
  
  /**
   * API request logging
   */
  request(message: string, meta?: Partial<CoreServicesLogEntry>): void {
    this.log('INFO', message, {
      ...meta,
      operation: 'API_REQUEST'
    });
  }
  
  /**
   * System operation logging
   */
  system(message: string, meta?: Partial<CoreServicesLogEntry>): void {
    this.log('INFO', message, {
      ...meta,
      operation: 'SYSTEM'
    });
  }
  
  /**
   * Performance logging with automatic duration calculation
   */
  performance(operation: ServiceOperation, startTime: number, meta?: Partial<CoreServicesLogEntry>): void {
    const duration = Date.now() - startTime;
    this.log('INFO', `${operation} completed`, {
      ...meta,
      operation,
      duration_ms: duration
    });
  }
  
  /**
   * Create child logger with default metadata
   */
  child(defaultMeta: Partial<CoreServicesLogEntry>): ICoreServicesLogger {
    const childLogger = this.logger.child(defaultMeta);
    return new CoreServicesLogger(childLogger, this.config);
  }
  
  /**
   * Check if current mode is verbose
   */
  isVerbose(): boolean {
    return this.config.mode === 'verbose';
  }
  
  /**
   * Core logging method - PROTECTED so subclasses can access it
   * This fixes the "Property 'log' is private" error
   */
  protected log(level: LogLevel, message: string, meta?: Partial<CoreServicesLogEntry>): void {
    this.logger.log(level, message, meta);
  }
  
  /**
   * Get logger configuration
   */
  getConfig(): LoggerConfig {
    return this.config;
  }
  
  /**
   * Get underlying Winston logger (for advanced usage)
   */
  getWinstonLogger(): Logger {
    return this.logger;
  }
}
// src/utils/logging/core/types.ts
/**
 * üîß CORE-SERVICES: Logger Types
 * 
 * Core types and interfaces for the logging system
 * Centralized type definitions following SOLID principles
 * 
 * Classification: INTERNAL (service infrastructure)
 */

/**
 * Log levels for core-services operations - Extended for legacy compatibility
 */
export type LogLevel = 'DEBUG' | 'INFO' | 'WARN' | 'ERROR' | 'TRACE' | 'FATAL' | 'CRITICAL' | 'ALERT' | 'EMERGENCY' | 'NOTICE' | 'AUDIT';

/**
 * Core-services specific log categories
 */
export type ServiceOperation = 
  | 'PDF_GENERATION' 
  | 'ZPL_GENERATION' 
  | 'EMAIL_OPERATION' 
  | 'SERVICE_CONTAINER' 
  | 'AUTHENTICATION' 
  | 'SYSTEM' 
  | 'API_REQUEST';

/**
 * Log modes for data sensitivity
 */
export type LogMode = 'normal' | 'verbose';

/**
 * Structured log entry interface
 */
export interface CoreServicesLogEntry {
  // Core message
  message: string;
  level?: LogLevel;
  
  // Service-specific context
  operation?: ServiceOperation;
  correlation_id?: string;
  trace_id?: string;
  
  // Performance metrics
  duration_ms?: number;
  file_size_bytes?: number;
  
  // Business context
  client_name?: string;
  user_id?: string;
  request_id?: string;
  
  // Technical context
  error_code?: string;
  stack?: string;
  endpoint?: string;
  method?: string;
  status_code?: number;
  
  // Service-specific data
  pdf_pages?: number;
  zpl_labels?: number;
  email_recipients?: number;
  attachment_count?: number;
  
  // Additional data (will be sanitized in normal mode)
  [key: string]: any;
}

/**
 * System metrics interface for performance monitoring
 */
export interface SystemMetrics {
  timestamp: string;
  memory: {
    rss: number;           // Resident Set Size (bytes)
    heapTotal: number;     // Total heap (bytes)
    heapUsed: number;      // Used heap (bytes)
    external: number;      // External memory (bytes)
    arrayBuffers: number;  // ArrayBuffers (bytes)
  };
  cpu: {
    userCPUTime: number;   // User CPU time (microseconds)
    systemCPUTime: number; // System CPU time (microseconds)
    cpuUsagePercent?: number; // CPU usage percentage (calculated)
  };
  process: {
    pid: number;
    ppid: number;
    uptime: number;        // Process uptime (seconds)
    uptimeFormatted: string; // Human readable uptime
    platform: string;
    nodeVersion: string;
  };
  system?: {
    totalMemory?: number;  // Total system memory (bytes)
    freeMemory?: number;   // Free system memory (bytes)
    loadAverage?: number[]; // Load average (Unix-like systems)
    cpuCount?: number;     // Number of CPU cores
  };
  gc?: {
    heapSizeLimit: number;
    totalHeapSizeExecutable: number;
    usedHeapSize: number;
  };
}

/**
 * Daily statistics interface for business metrics
 */
export interface DailyStats {
  date: string;
  period: {
    start: string;
    end: string;
  };
  operations: {
    emails_sent: number;
    emails_failed: number;
    pdfs_generated: number;
    pdfs_failed: number;
    zpl_labels_generated: number;
    zpl_labels_failed: number;
    total_requests: number;
    failed_requests: number;
  };
  performance: {
    avg_email_duration_ms: number;
    avg_pdf_duration_ms: number;
    avg_zpl_duration_ms: number;
    peak_memory_mb: number;
    avg_cpu_percent: number;
    max_concurrent_operations: number;
  };
  errors: {
    authentication_failures: number;
    service_errors: number;
    system_errors: number;
    total_errors: number;
  };
  system: {
    restarts: number;
    uptime_hours: number;
    total_uptime_hours: number;
  };
}

/**
 * Error types for statistics categorization
 */
export type ErrorType = 'auth' | 'service' | 'system';

/**
 * Logger configuration interface
 */
export interface LoggerConfig {
  mode: LogMode;
  level: LogLevel;
  environment: string;
  service: string;
  logsDirectory: string;
}

/**
 * Base logger interface for core functionality
 */
export interface ILogger {
  debug(message: string, meta?: Partial<CoreServicesLogEntry>): void;
  info(message: string, meta?: Partial<CoreServicesLogEntry>): void;
  warn(message: string, meta?: Partial<CoreServicesLogEntry>): void;
  error(message: string, meta?: Partial<CoreServicesLogEntry>): void;
  isVerbose(): boolean;
  child(defaultMeta: Partial<CoreServicesLogEntry>): ILogger;
}

/**
 * Enhanced logger interface with service-specific methods
 */
export interface ICoreServicesLogger extends ILogger {
  // Service-specific logging methods
  pdf(message: string, meta?: Partial<CoreServicesLogEntry>): void;
  zpl(message: string, meta?: Partial<CoreServicesLogEntry>): void;
  email(message: string, meta?: Partial<CoreServicesLogEntry>): void;
  container(message: string, meta?: Partial<CoreServicesLogEntry>): void;
  auth(message: string, meta?: Partial<CoreServicesLogEntry>): void;
  request(message: string, meta?: Partial<CoreServicesLogEntry>): void;
  system(message: string, meta?: Partial<CoreServicesLogEntry>): void;
  
  // Performance logging
  performance(operation: ServiceOperation, startTime: number, meta?: Partial<CoreServicesLogEntry>): void;
}

/**
 * Metrics collector interface
 */
export interface IMetricsCollector {
  collectMetrics(): SystemMetrics;
  logMetrics(): void;
  startMetricsCollection(intervalMs?: number): void;
  stopMetricsCollection(): void;
  getMetricsSummary(): string;
}

/**
 * Daily stats collector interface
 */
export interface IDailyStatsCollector {
  recordEmail(success: boolean, durationMs?: number): void;
  recordPdf(success: boolean, durationMs?: number): void;
  recordZpl(success: boolean, durationMs?: number): void;
  recordError(type: ErrorType): void;
  updatePerformanceMetrics(memoryMB: number, cpuPercent?: number): void;
  startOperation(): void;
  endOperation(): void;
  getDailySummary(stats?: DailyStats): string;
  getCurrentStats(): DailyStats;
  startDailyStats(): void;
  stopDailyStats(): void;
}

/**
 * Enhanced logger interface with metrics and stats
 */
export interface IEnhancedLogger extends ICoreServicesLogger {
  metrics: IMetricsCollector;
  dailyStats: IDailyStatsCollector;
  
  // System metrics
  systemMetrics(message: string, meta?: Partial<CoreServicesLogEntry>): void;
  
  // Lifecycle methods
  startMetrics(intervalMs?: number): void;
  stopMetrics(): void;
  
  // Daily stats methods
  getDailySummary(): string;
  getCurrentDailyStats(): DailyStats;
}
// src/utils/logging/formatter/humanMetrics.ts
/**
 * üß† CORE-SERVICES: Human-Friendly Metrics Formatter
 * 
 * Makes system metrics readable for humans instead of machines
 * Because nobody wants to calculate 296488960 bytes in their head
 */

import { SystemMetrics } from '../core/types';

/**
 * Convert bytes to human readable format
 */
function formatBytes(bytes: number): string {
  if (bytes === 0) return '0 B';
  
  const k = 1024;
  const sizes = ['B', 'KB', 'MB', 'GB'];
  const i = Math.floor(Math.log(bytes) / Math.log(k));
  
  return `${(bytes / Math.pow(k, i)).toFixed(1)} ${sizes[i]}`;
}

/**
 * Convert microseconds to milliseconds with proper formatting
 */
function formatCpuTime(microseconds: number): string {
  const milliseconds = microseconds / 1000;
  if (milliseconds < 1000) {
    return `${milliseconds.toFixed(1)}ms`;
  }
  return `${(milliseconds / 1000).toFixed(2)}s`;
}

/**
 * Format CPU percentage with proper rounding
 */
function formatCpuPercent(percent?: number): string {
  if (percent === undefined) return 'N/A';
  if (percent < 0.01) return '<0.01%';
  if (percent > 99.99) return '>99.99%';
  return `${percent.toFixed(2)}%`;
}

/**
 * Create human-readable metrics summary
 */
export function createHumanMetricsSummary(metrics: SystemMetrics): string {
  const memUsed = formatBytes(metrics.memory.heapUsed);
  const memTotal = formatBytes(metrics.memory.heapTotal);
  const memRss = formatBytes(metrics.memory.rss);
  const cpuPercent = formatCpuPercent(metrics.cpu.cpuUsagePercent);
  const uptime = metrics.process.uptimeFormatted;
  
  let summary = `üñ•Ô∏è  Memory: ${memUsed}/${memTotal} (RSS: ${memRss}) | ‚ö° CPU: ${cpuPercent} | ‚è±Ô∏è  Uptime: ${uptime}`;
  
  if (metrics.system) {
    const totalSysMem = formatBytes(metrics.system.totalMemory || 0);
    const freeSysMem = formatBytes(metrics.system.freeMemory || 0);
    const usedSysMem = formatBytes((metrics.system.totalMemory || 0) - (metrics.system.freeMemory || 0));
    summary += ` | üè† System: ${usedSysMem}/${totalSysMem} (${freeSysMem} free)`;
  }
  
  return summary;
}

/**
 * Create detailed human-readable metrics
 */
export function createDetailedHumanMetrics(metrics: SystemMetrics): any {
  return {
    timestamp: metrics.timestamp,
    memory: {
      heap_used: formatBytes(metrics.memory.heapUsed),
      heap_total: formatBytes(metrics.memory.heapTotal),
      rss: formatBytes(metrics.memory.rss),
      external: formatBytes(metrics.memory.external),
      array_buffers: formatBytes(metrics.memory.arrayBuffers)
    },
    cpu: {
      usage_percent: formatCpuPercent(metrics.cpu.cpuUsagePercent),
      user_time: formatCpuTime(metrics.cpu.userCPUTime),
      system_time: formatCpuTime(metrics.cpu.systemCPUTime)
    },
    process: {
      pid: metrics.process.pid,
      uptime: metrics.process.uptimeFormatted,
      platform: metrics.process.platform,
      node_version: metrics.process.nodeVersion
    },
    ...(metrics.system && {
      system: {
        total_memory: formatBytes(metrics.system.totalMemory || 0),
        free_memory: formatBytes(metrics.system.freeMemory || 0),
        used_memory: formatBytes((metrics.system.totalMemory || 0) - (metrics.system.freeMemory || 0)),
        cpu_cores: metrics.system.cpuCount || 'N/A'
      }
    })
  };
}

/**
 * Create ultra-compact metrics for logs
 */
export function createCompactMetrics(metrics: SystemMetrics): string {
  const memMB = Math.round(metrics.memory.heapUsed / 1024 / 1024);
  const cpuPercent = metrics.cpu.cpuUsagePercent?.toFixed(1) || 'N/A';
  const uptime = metrics.process.uptimeFormatted;
  
  return `${memMB}MB | ${cpuPercent}% | ${uptime}`;
}

/**
 * Format for console output with emojis and colors
 */
export function createConsoleMetrics(metrics: SystemMetrics): string {
  const memUsed = formatBytes(metrics.memory.heapUsed);
  const memTotal = formatBytes(metrics.memory.heapTotal);
  const cpuPercent = metrics.cpu.cpuUsagePercent || 0;
  const uptime = metrics.process.uptimeFormatted;
  
  // Add emoji indicators based on usage
  let memEmoji = 'üíö'; // Green
  const memUsage = (metrics.memory.heapUsed / metrics.memory.heapTotal) * 100;
  if (memUsage > 80) memEmoji = 'üî¥'; // Red
  else if (memUsage > 60) memEmoji = 'üü°'; // Yellow
  
  let cpuEmoji = 'üíö'; // Green  
  if (cpuPercent > 80) cpuEmoji = 'üî¥'; // Red
  else if (cpuPercent > 50) cpuEmoji = 'üü°'; // Yellow
  
  return `${memEmoji} Memory: ${memUsed}/${memTotal} (${memUsage.toFixed(1)}%) | ${cpuEmoji} CPU: ${formatCpuPercent(cpuPercent)} | ‚è∞ Uptime: ${uptime}`;
}
// src/utils/logging/index.ts
/**
 * üöÄ CORE-SERVICES: Logger System
 * 
 * Clean barrel exports for the logging system
 * Provides clean imports following SOLID principles
 * 
 * Classification: INTERNAL (service infrastructure)
 * 
 * Usage:
 * import logger from '@/utils/logging';
 * import { CoreServicesLogger, LogLevel } from '@/utils/logging';
 */

// Re-export all types for clean imports
export type {
  LogLevel,
  ServiceOperation,
  LogMode,
  CoreServicesLogEntry,
  SystemMetrics,
  DailyStats,
  ErrorType,
  LoggerConfig,
  ILogger,
  ICoreServicesLogger,
  IMetricsCollector,
  IDailyStatsCollector,
  IEnhancedLogger
} from './core/types';

// Re-export configuration utilities
export {
  getLogMode,
  getLogLevel,
  getServiceName,
  getEnvironment,
  ensureLogsDirectory,
  createLoggerConfig,
  sanitizeLogData,
  formatUptime,
  isValidLogLevel,
  isValidServiceOperation,
  getDefaultMeta,
  SENSITIVE_PATTERNS
} from './core/config';

// Re-export core logger class
export { CoreServicesLogger } from './core/logger';

// Import human-friendly formatters
import { 
  createHumanMetricsSummary, 
  createDetailedHumanMetrics, 
  createConsoleMetrics 
} from './formatter/humanMetrics';

// TODO: Import collectors when created  
// export { MetricsCollector } from './collectors/metricsCollector';
// export { DailyStatsCollector } from './collectors/dailyStatsCollector';

// TODO: Import enhanced logger when created
// export { EnhancedCoreServicesLogger } from './enhancedLogger';

// Temporary implementation for current compatibility
// This will be replaced when we move formatters and collectors to separate files
import { createLogger, format, transports, Logger } from 'winston';
import DailyRotateFile from 'winston-daily-rotate-file';
import { 
  createLoggerConfig, 
  sanitizeLogData, 
  getDefaultMeta,
  formatUptime 
} from './core/config';
import { 
  CoreServicesLogEntry, 
  SystemMetrics, 
  DailyStats, 
  IEnhancedLogger,
  IMetricsCollector,
  IDailyStatsCollector,
  ErrorType 
} from './core/types';
import { CoreServicesLogger } from './core/logger';
import path from 'path';

// Temporary formatters (will be moved to separate files)
const createStructuredFormat = () => {
  return format.printf((info) => {
    const config = createLoggerConfig();
    const {
      timestamp,
      level,
      message,
      operation,
      correlation_id,
      service = config.service,
      environment = config.environment,
      ...meta
    } = info;
    
    const sanitizedMeta = sanitizeLogData(meta, config.mode);
    
    const logEntry: any = {
      timestamp,
      level: level.toUpperCase(),
      service,
      environment,
      mode: config.mode,
      message
    };
    
    if (operation) {
      logEntry.operation = operation;
    }
    
    if (correlation_id) {
      logEntry.correlation_id = correlation_id;
    }
    
    if (Object.keys(sanitizedMeta).length > 0) {
      logEntry.metadata = sanitizedMeta;
    }
    
    return JSON.stringify(logEntry);
  });
};

const createConsoleFormat = () => {
  return format.printf((info) => {
    const config = createLoggerConfig();
    const {
      timestamp,
      level,
      message,
      operation,
      correlation_id,
      duration_ms,
      ...meta
    } = info;
    
    let output = `${timestamp} [${level.toUpperCase()}]`;
    
    if (config.mode === 'verbose') {
      output += ` [VERBOSE]`;
    }
    
    if (operation) {
      output += ` [${operation}]`;
    }
    
    if (correlation_id && typeof correlation_id === 'string') {
      output += ` [${correlation_id.substring(0, 8)}...]`;
    }
    
    output += `: ${message}`;
    
    if (duration_ms) {
      output += ` (${duration_ms}ms)`;
    }
    
    const sanitizedMeta = sanitizeLogData(meta, config.mode);
    if (config.mode === 'verbose' || level === 'error') {
      if (Object.keys(sanitizedMeta).length > 0) {
        output += `\n  üìã ${JSON.stringify(sanitizedMeta, null, 2)}`;
      }
    }
    
    return output;
  });
};

// Temporary collectors (will be moved to separate files)
class TempMetricsCollector implements IMetricsCollector {
  private previousCPUUsage: NodeJS.CpuUsage | null = null;
  private previousTimestamp: number | null = null;
  private metricsLogger: Logger;
  private intervalId: NodeJS.Timeout | null = null;
  private dailyStats: TempDailyStatsCollector;
  
  constructor(config: any, dailyStats: TempDailyStatsCollector) {
    this.dailyStats = dailyStats;
    this.metricsLogger = createLogger({
      levels: {
        ERROR: 0, WARN: 1, INFO: 2, DEBUG: 3
      },
      level: 'INFO',
      format: format.combine(
        format.timestamp({ format: 'YYYY-MM-DD HH:mm:ss.SSS' }),
        format.printf((info) => JSON.stringify(info))
      ),
      transports: [
        new DailyRotateFile({
          filename: path.join(config.logsDirectory, 'core-services-metrics-%DATE%.log'),
          datePattern: 'YYYY-MM-DD',
          zippedArchive: true,
          maxSize: '25m',
          maxFiles: '30d',
          level: 'INFO'
        })
      ],
      exitOnError: false
    });
  }
  
  collectMetrics(): SystemMetrics {
    const memUsage = process.memoryUsage();
    const cpuUsage = process.cpuUsage();
    const currentTime = Date.now();
    
    let cpuUsagePercent: number | undefined;
    if (this.previousCPUUsage && this.previousTimestamp) {
      const timeDiff = currentTime - this.previousTimestamp;
      const userDiff = cpuUsage.user - this.previousCPUUsage.user;
      const systemDiff = cpuUsage.system - this.previousCPUUsage.system;
      const totalCPUTime = (userDiff + systemDiff) / 1000;
      cpuUsagePercent = Math.min(100, (totalCPUTime / timeDiff) * 100);
    }
    
    this.previousCPUUsage = cpuUsage;
    this.previousTimestamp = currentTime;
    
    const uptimeSeconds = process.uptime();
    
    return {
      timestamp: new Date().toISOString(),
      memory: {
        rss: memUsage.rss,
        heapTotal: memUsage.heapTotal,
        heapUsed: memUsage.heapUsed,
        external: memUsage.external,
        arrayBuffers: memUsage.arrayBuffers
      },
      cpu: {
        userCPUTime: cpuUsage.user,
        systemCPUTime: cpuUsage.system,
        ...(cpuUsagePercent !== undefined && { cpuUsagePercent })
      },
      process: {
        pid: process.pid,
        ppid: process.ppid || 0,
        uptime: uptimeSeconds,
        uptimeFormatted: formatUptime(uptimeSeconds),
        platform: process.platform,
        nodeVersion: process.version
      }
    };
  }
  
  /**
   * Log current metrics and update daily stats - NOW WITH HUMAN-FRIENDLY FORMAT!
   */
  logMetrics(): void {
    const metrics = this.collectMetrics();
    
    // Human-friendly version for log file
    const humanMetrics = createDetailedHumanMetrics(metrics);
    this.metricsLogger.log('INFO', 'SYSTEM_METRICS', humanMetrics);
    
    // Update daily stats with performance data
    const memoryMB = Math.round(metrics.memory.heapUsed / 1024 / 1024);
    this.dailyStats.updatePerformanceMetrics(memoryMB, metrics.cpu.cpuUsagePercent);
  }
  
  startMetricsCollection(intervalMs: number = 30000): void {
    if (this.intervalId) return;
    this.logMetrics();
    this.intervalId = setInterval(() => this.logMetrics(), intervalMs);
  }
  
  stopMetricsCollection(): void {
    if (this.intervalId) {
      clearInterval(this.intervalId);
      this.intervalId = null;
    }
  }
  
  /**
   * Get human-readable metrics summary with emojis and proper units
   */
  getMetricsSummary(): string {
    const metrics = this.collectMetrics();
    return createConsoleMetrics(metrics);
  }
}

class TempDailyStatsCollector implements IDailyStatsCollector {
  private currentStats: DailyStats;
  
  constructor() {
    this.currentStats = this.initializeStats();
  }
  
  private initializeStats(): DailyStats {
    const now = new Date();
    return {
      date: now.toISOString().split('T')[0],
      period: { start: now.toISOString(), end: '' },
      operations: {
        emails_sent: 0, emails_failed: 0, pdfs_generated: 0, pdfs_failed: 0,
        zpl_labels_generated: 0, zpl_labels_failed: 0, total_requests: 0, failed_requests: 0
      },
      performance: {
        avg_email_duration_ms: 0, avg_pdf_duration_ms: 0, avg_zpl_duration_ms: 0,
        peak_memory_mb: 0, avg_cpu_percent: 0, max_concurrent_operations: 0
      },
      errors: { authentication_failures: 0, service_errors: 0, system_errors: 0, total_errors: 0 },
      system: { restarts: 0, uptime_hours: 0, total_uptime_hours: 0 }
    };
  }
  
  recordEmail(success: boolean, durationMs?: number): void {
    if (success) this.currentStats.operations.emails_sent++;
    else this.currentStats.operations.emails_failed++;
    this.currentStats.operations.total_requests++;
  }
  
  recordPdf(success: boolean, durationMs?: number): void {
    if (success) this.currentStats.operations.pdfs_generated++;
    else this.currentStats.operations.pdfs_failed++;
    this.currentStats.operations.total_requests++;
  }
  
  recordZpl(success: boolean, durationMs?: number): void {
    if (success) this.currentStats.operations.zpl_labels_generated++;
    else this.currentStats.operations.zpl_labels_failed++;
    this.currentStats.operations.total_requests++;
  }
  
  recordError(type: ErrorType): void {
    switch (type) {
      case 'auth': this.currentStats.errors.authentication_failures++; break;
      case 'service': this.currentStats.errors.service_errors++; break;
      case 'system': this.currentStats.errors.system_errors++; break;
    }
    this.currentStats.errors.total_errors++;
  }
  
  updatePerformanceMetrics(memoryMB: number, cpuPercent?: number): void {
    this.currentStats.performance.peak_memory_mb = Math.max(
      this.currentStats.performance.peak_memory_mb, memoryMB
    );
  }
  
  startOperation(): void {}
  endOperation(): void {}
  
  getDailySummary(): string {
    const s = this.currentStats;
    return `Emails: ${s.operations.emails_sent}, PDFs: ${s.operations.pdfs_generated}, ` +
           `ZPL: ${s.operations.zpl_labels_generated}, Errors: ${s.errors.total_errors}`;
  }
  
  getCurrentStats(): DailyStats { return this.currentStats; }
  startDailyStats(): void {}
  stopDailyStats(): void {}
}

// Enhanced logger with temporary implementation
class TempEnhancedLogger extends CoreServicesLogger implements IEnhancedLogger {
  public metrics: IMetricsCollector;
  public dailyStats: TempDailyStatsCollector;
  
  constructor(logger: Logger, config: any) {
    super(logger, config);
    this.dailyStats = new TempDailyStatsCollector();
    this.metrics = new TempMetricsCollector(config, this.dailyStats);
  }
  
  systemMetrics(message: string, meta?: Partial<CoreServicesLogEntry>): void {
    const summary = this.metrics.getMetricsSummary();
    this.system(`${message} - ${summary}`, meta);
  }
  
  startMetrics(intervalMs: number = 30000): void {
    this.metrics.startMetricsCollection(intervalMs);
    this.dailyStats.startDailyStats();
  }
  
  stopMetrics(): void {
    this.metrics.stopMetricsCollection();
    this.dailyStats.stopDailyStats();
  }
  
  getDailySummary(): string {
    return this.dailyStats.getDailySummary();
  }
  
  getCurrentDailyStats(): DailyStats {
    return this.dailyStats.getCurrentStats();
  }
  
  // Override methods to include stats tracking
  pdf(message: string, meta?: Partial<CoreServicesLogEntry>): void {
    const success = !message.toLowerCase().includes('error') && !message.toLowerCase().includes('failed');
    this.dailyStats.recordPdf(success, meta?.duration_ms);
    super.pdf(message, meta);
  }
  
  email(message: string, meta?: Partial<CoreServicesLogEntry>): void {
    const success = !message.toLowerCase().includes('error') && !message.toLowerCase().includes('failed');
    this.dailyStats.recordEmail(success, meta?.duration_ms);
    super.email(message, meta);
  }
  
  zpl(message: string, meta?: Partial<CoreServicesLogEntry>): void {
    const success = !message.toLowerCase().includes('error') && !message.toLowerCase().includes('failed');
    this.dailyStats.recordZpl(success, meta?.duration_ms);
    super.zpl(message, meta);
  }
  
  error(message: string, meta?: Partial<CoreServicesLogEntry>): void {
    const operation = meta?.operation;
    if (operation === 'AUTHENTICATION') {
      this.dailyStats.recordError('auth');
    } else if (operation && ['PDF_GENERATION', 'ZPL_GENERATION', 'EMAIL_OPERATION'].includes(operation)) {
      this.dailyStats.recordError('service');
    } else {
      this.dailyStats.recordError('system');
    }
    super.error(message, meta);
  }
}

// Create the Winston logger with custom levels
const createWinstonLogger = (): Logger => {
  const config = createLoggerConfig();
  
  // Define custom levels for Winston (UPPERCASE)
  const customLevels = {
    levels: {
      ERROR: 0,
      FATAL: 0,
      CRITICAL: 0,
      ALERT: 0,
      EMERGENCY: 0,
      WARN: 1,
      AUDIT: 1,
      INFO: 2,
      NOTICE: 2,
      DEBUG: 3,
      TRACE: 4
    },
    colors: {
      ERROR: 'red',
      FATAL: 'red', 
      CRITICAL: 'red',
      ALERT: 'red',
      EMERGENCY: 'red',
      WARN: 'yellow',
      AUDIT: 'yellow',
      INFO: 'green',
      NOTICE: 'green', 
      DEBUG: 'blue',
      TRACE: 'blue'
    }
  };
  
  return createLogger({
    levels: customLevels.levels,  // ‚Üê A√ëADIR ESTO
    level: config.level,
    format: format.combine(
      format.timestamp({ format: 'YYYY-MM-DD HH:mm:ss.SSS' }),
      format.errors({ stack: true }),
      createStructuredFormat()
    ),
    defaultMeta: getDefaultMeta(config),
    transports: [
      new transports.Console({
        level: config.level,
        format: format.combine(
          format.colorize({
            colors: customLevels.colors  // ‚Üê CAMBIAR ESTO
          }),
          format.timestamp({ format: 'HH:mm:ss.SSS' }),
          createConsoleFormat()
        )
      }),
      new DailyRotateFile({
        filename: path.join(config.logsDirectory, 'core-services-%DATE%.log'),
        datePattern: 'YYYY-MM-DD',
        zippedArchive: true,
        maxSize: '50m',
        maxFiles: '14d',
        level: 'INFO'  // ‚Üê CAMBIAR A MAY√öSCULAS
      }),
      new DailyRotateFile({
        filename: path.join(config.logsDirectory, 'core-services-error-%DATE%.log'),
        datePattern: 'YYYY-MM-DD',
        zippedArchive: true,
        maxSize: '25m',
        maxFiles: '30d',
        level: 'ERROR'  // ‚Üê CAMBIAR A MAY√öSCULAS
      })
    ],
    exceptionHandlers: [
      new DailyRotateFile({
        filename: path.join(config.logsDirectory, 'core-services-exceptions-%DATE%.log'),
        datePattern: 'YYYY-MM-DD',
        zippedArchive: true,
        maxSize: '25m',
        maxFiles: '30d'
      })
    ],
    rejectionHandlers: [
      new DailyRotateFile({
        filename: path.join(config.logsDirectory, 'core-services-rejections-%DATE%.log'),
        datePattern: 'YYYY-MM-DD',
        zippedArchive: true,
        maxSize: '25m',
        maxFiles: '30d'
      })
    ],
    exitOnError: false
  });
};

// Create and export the default logger instance
const winstonLogger = createWinstonLogger();
const config = createLoggerConfig();
const logger = new TempEnhancedLogger(winstonLogger, config);

// Log successful initialization
logger.systemMetrics('üöÄ Core-Services logger system initialized', {
  mode: config.mode,
  log_level: config.level,
  environment: config.environment,
  verbose_enabled: logger.isVerbose(),
  daily_summary: logger.getDailySummary()
});

// Export the configured logger as default
export default logger;

// Export the enhanced logger class for direct instantiation
export { TempEnhancedLogger as EnhancedCoreServicesLogger };
// src/utils/pdfSigner.ts
import fs from 'fs';
import { plainAddPlaceholder } from 'node-signpdf';

const { SignPdf } = require('node-signpdf');

interface SignPDFOptions {
  pdfPath: string;
  outputPath: string;
  certPath: string;
  certPassword?: string;
  type: 'p12' | 'pem';
}

export async function signPDF({
  pdfPath,
  outputPath,
  certPath,
  certPassword = '',
  type
}: SignPDFOptions): Promise<void> {
  
  // Validate required parameters
  if (!certPath) {
    throw new Error('Certificate path is required for PDF signing');
  }

  const pdfBuffer = fs.readFileSync(pdfPath);
  const pdfWithPlaceholder = plainAddPlaceholder({
    pdfBuffer,
    reason: 'Document signed electronically by CORE'
  });

  const signer = new SignPdf();

  if (type === 'p12') {
    const p12Buffer = fs.readFileSync(certPath);
    
    const signedPdf = signer.sign(pdfWithPlaceholder, p12Buffer, {
      passphrase: certPassword,
      timestampURL: 'http://timestamp.digicert.com'
    });
    
    fs.writeFileSync(outputPath, signedPdf);
    return;
  }

  if (type === 'pem') {
    const certContent = fs.readFileSync(certPath, 'utf8');
    const certMatch = certContent.match(/-----BEGIN CERTIFICATE-----[^-]+-----END CERTIFICATE-----/s);
    const keyMatch = certContent.match(/-----BEGIN (?:RSA )?PRIVATE KEY-----[^-]+-----END (?:RSA )?PRIVATE KEY-----/s);
    
    if (!certMatch || !keyMatch) {
      throw new Error('Invalid PEM file: certificate or key not found');
    }
    
    const certificate = Buffer.from(certMatch[0]);
    const key = Buffer.from(keyMatch[0]);
    
    const signedPdf = signer.sign(pdfWithPlaceholder, {
      key,
      cert: certificate
    });
    
    fs.writeFileSync(outputPath, signedPdf);
    return;
  }

  throw new Error('Unsupported certificate type');
}
// src/utils/windowsGpgFix.ts
// utils/windowsGpgFix.ts
import { execSync } from 'child_process';
import fs from 'fs';
import path from 'path';
import os from 'os';

/**
 * Fix GPG-agent issues on Windows for SOPS
 * This addresses the common gpg-agent connection problems on Windows
 */
export class WindowsGpgFix {
  
  // Updated fingerprint for new GPG key
  private static readonly REQUIRED_GPG_KEY = 'EC5A8F24358F986C983BA5F384F09B1D810A6590';
  
  /**
   * Setup GPG environment for Windows
   */
  static setupGpgEnvironment(): { gnupgHome: string; env: Record<string, string> } {
    const gnupgHome = process.env.GNUPGHOME || path.join(os.homedir(), '.gnupg');
    
    console.log(`üîß Setting up GPG environment for Windows...`);
    console.log(`üìÅ GNUPG Home: ${gnupgHome}`);
    
    // Kill any existing gpg-agent processes
    try {
      execSync('taskkill /f /im gpg-agent.exe 2>nul', { stdio: 'pipe' });
      console.log('üî™ Killed existing gpg-agent processes');
    } catch {
      // No existing processes, continue
    }
    
    // Set up Windows-specific environment
    const env = {
      ...process.env,
      GNUPGHOME: gnupgHome,
      GPG_TTY: 'CON',
      // Disable problematic keyboxd
      GPG_AGENT_INFO: '',
      // Force use of Windows paths
      PATH: process.env.PATH || ''
    };
    
    return { gnupgHome, env };
  }
  
  /**
   * Test GPG functionality
   */
  static testGpg(): boolean {
    try {
      console.log('üß™ Testing GPG functionality...');
      
      const { env } = this.setupGpgEnvironment();
      
      // Test 1: List secret keys
      const secretKeys = execSync('gpg --list-secret-keys --with-colons', { 
        encoding: 'utf8',
        env,
        stdio: 'pipe'
      });
      
      // Updated to use new GPG key fingerprint
      if (!secretKeys.includes(this.REQUIRED_GPG_KEY)) {
        console.error(`‚ùå Required GPG key not found: ${this.REQUIRED_GPG_KEY}`);
        console.log('Available keys:');
        console.log(secretKeys);
        return false;
      }
      
      console.log(`‚úÖ GPG key found: ${this.REQUIRED_GPG_KEY}`);
      
      // Test 2: Test gpg-agent
      try {
        execSync('gpg-connect-agent "GET_VERSION" /bye', { 
          env,
          stdio: 'pipe',
          timeout: 5000
        });
        console.log('‚úÖ gpg-agent is working');
      } catch {
        console.log('‚ö†Ô∏è gpg-agent connection issue, will try to fix');
        this.fixGpgAgent(env);
      }
      
      return true;
      
    } catch (error) {
      console.error('‚ùå GPG test failed:', (error as Error).message);
      return false;
    }
  }
  
  /**
   * Fix gpg-agent issues
   */
  private static fixGpgAgent(env: Record<string, string>): void {
    try {
      console.log('üîß Attempting to fix gpg-agent...');
      
      // Kill any hanging agents
      execSync('taskkill /f /im gpg-agent.exe 2>nul', { stdio: 'pipe' });
      
      // Start fresh gpg-agent
      execSync('gpg-agent --daemon --use-standard-socket', { 
        env,
        stdio: 'pipe',
        timeout: 5000
      });
      
      console.log('‚úÖ gpg-agent restarted');
      
    } catch (error) {
      console.warn('‚ö†Ô∏è Could not fix gpg-agent:', (error as Error).message);
    }
  }
  
  /**
   * Decrypt SOPS file with Windows-specific approach
   */
  static decryptSopsFile(sopsPath: string, secretsPath: string, passphrase: string): string {
    const { gnupgHome, env } = this.setupGpgEnvironment();
    
    console.log('üîê Attempting SOPS decryption with Windows-optimized approach...');
    
    // Method 1: Direct passphrase via stdin
    try {
      console.log('üîê Method 1: Direct passphrase input');
      
      const result = execSync(`echo ${passphrase}| "${sopsPath}" -d --output-type json "${secretsPath}"`, {
        encoding: 'utf8',
        env: {
          ...env,
          GPG_BATCH: '1',
          GPG_USE_AGENT: '0' // Disable agent for this operation
        },
        stdio: ['pipe', 'pipe', 'pipe'],
        timeout: 30000,
        shell: 'cmd.exe'
      });
      
      console.log('‚úÖ SOPS decryption successful (Method 1)');
      return result;
      
    } catch (error1) {
      console.log('‚ö†Ô∏è Method 1 failed, trying Method 2...');
      
      // Method 2: Use pinentry-mode loopback
      try {
        console.log('üîê Method 2: Pinentry loopback mode');
        
        const result = execSync(`"${sopsPath}" -d --output-type json "${secretsPath}"`, {
          encoding: 'utf8',
          env: {
            ...env,
            GPG_PASSPHRASE: passphrase,
            GPG_BATCH: '1'
          },
          input: passphrase,
          stdio: ['pipe', 'pipe', 'pipe'],
          timeout: 30000
        });
        
        console.log('‚úÖ SOPS decryption successful (Method 2)');
        return result;
        
      } catch (error2) {
        console.log('‚ö†Ô∏è Method 2 failed, trying Method 3...');
        
        // Method 3: Temporary script with passphrase
        try {
          console.log('üîê Method 3: Temporary script approach');
          
          const tempScript = path.join(os.tmpdir(), `decrypt-${Date.now()}.cmd`);
          const scriptContent = `@echo off
set GNUPGHOME=${gnupgHome}
set GPG_BATCH=1
set GPG_USE_AGENT=0
echo ${passphrase}| "${sopsPath}" -d --output-type json "${secretsPath}"`;
          
          fs.writeFileSync(tempScript, scriptContent);
          
          const result = execSync(`"${tempScript}"`, {
            encoding: 'utf8',
            stdio: ['pipe', 'pipe', 'pipe'],
            timeout: 30000
          });
          
          // Clean up immediately
          fs.unlinkSync(tempScript);
          
          console.log('‚úÖ SOPS decryption successful (Method 3)');
          return result;
          
        } catch (error3) {
          // Clean up script if it exists
          const tempScript = path.join(os.tmpdir(), `decrypt-${Date.now()}.cmd`);
          if (fs.existsSync(tempScript)) {
            fs.unlinkSync(tempScript);
          }
          
          console.error('‚ùå All SOPS decryption methods failed:');
          console.error('   Method 1 (stdin):', (error1 as Error).message.substring(0, 200));
          console.error('   Method 2 (loopback):', (error2 as Error).message.substring(0, 200));
          console.error('   Method 3 (script):', (error3 as Error).message.substring(0, 200));
          
          throw new Error('SOPS decryption failed with all methods. Check GPG setup and passphrase.');
        }
      }
    }
  }
}

// Export helper function for envConfig.ts
export function decryptSopsWithWindowsFix(sopsPath: string, secretsPath: string, passphrase: string): string {
  // Setup GPG environment first
  if (!WindowsGpgFix.testGpg()) {
    throw new Error('GPG environment is not properly set up');
  }
  
  // Decrypt using Windows-optimized methods
  return WindowsGpgFix.decryptSopsFile(sopsPath, secretsPath, passphrase);
}
// src/validators/iso27001EmailSchema.ts
// src/validators/iso27001EmailSchema.ts

import { z } from 'zod';

/**
 * ISO 27001 Compliant Email Request Schema
 * 
 * This schema validates email requests according to ISO 27001 standards:
 * - A.8.2.1: Information classification validation
 * - A.13.2.1: Information transfer format validation
 * - A.9.4.1: Information access restriction validation
 */

// ISO 27001 Annex A.8.2 - Information Classification Levels
const ISO27001ClassificationSchema = z.enum(['internal', 'confidential', 'restricted'], {
  errorMap: () => ({ 
    message: 'Classification must be one of: internal, confidential, restricted (ISO 27001 A.8.2.1)' 
  })
});

// Email attachment schema with file path validation
const EmailAttachmentSchema = z.object({
  name: z.string()
    .min(1, 'Attachment name is required')
    .max(255, 'Attachment name too long')
    .regex(/^[^<>:"/\\|?*]+$/, 'Invalid characters in attachment name'),
  path: z.string()
    .min(1, 'Attachment path is required')
    .max(500, 'Attachment path too long')
});

// Main ISO 27001 compliant email schema
export const ISO27001EmailRequestSchema = z.object({
  // Core email fields (A.13.2.1 - Information transfer)
  to: z.string()
    .email('Invalid email address format')
    .max(320, 'Email address too long'), // RFC 5321 limit
  
  subject: z.string()
    .min(1, 'Subject is required')
    .max(500, 'Subject too long'),
  
  body: z.string()
    .min(1, 'Email body is required')
    .max(50000, 'Email body too long'), // Reasonable limit for email content
  
  // Optional sender override
  from: z.string()
    .email('Invalid sender email address format')
    .max(320, 'Sender email address too long')
    .optional(),
  
  // Attachments with validation
  attachments: z.array(EmailAttachmentSchema)
    .max(10, 'Too many attachments (maximum 10)')
    .optional(),
  
  // ISO 27001 A.8.2.1 - Information classification (REQUIRED)
  classification: ISO27001ClassificationSchema,
  
  // Optional email priority
  importance: z.enum(['low', 'normal', 'high'])
    .default('normal'),
  
  // GDPR consent token (can be in header or body)
  gdpr_token: z.string()
    .min(1, 'GDPR token is required for compliance')
    .max(100, 'GDPR token too long')
    .optional() // Optional in body since it can come from headers
});

// Type inference for TypeScript
export type ISO27001EmailRequest = z.infer<typeof ISO27001EmailRequestSchema>;

/**
 * Validates email request against ISO 27001 compliance standards
 * 
 * @param payload - Email request payload to validate
 * @returns Validation result with detailed error information
 */
export const validateISO27001EmailRequest = (payload: unknown) => {
  return ISO27001EmailRequestSchema.safeParse(payload);
};

/**
 * Middleware function for Express routes to validate ISO 27001 email requests
 * 
 * @param req - Express request object
 * @param res - Express response object  
 * @param next - Express next function
 */
export const validateISO27001EmailMiddleware = (req: any, res: any, next: any) => {
  const result = validateISO27001EmailRequest(req.body);
  
  if (!result.success) {
    return res.status(400).json({
      error: 'ISO 27001 validation failed',
      details: result.error.issues,
      iso_control: 'A.8.2.1' // Information classification
    });
  }
  
  // Attach validated data to request
  req.validatedBody = result.data;
  next();
};
// src/validators/pdfRequestSchema.ts
import { z } from 'zod';

// Define the structure of core_report_info
export const coreReportInfoSchema = z.object({
  report_name: z.string().min(1),
  report_description: z.string().min(1),
  report_template: z.string().min(1),
  report_version: z.string().regex(/^\d+\.\d+\.\d+$/),
  report_file_name: z.string().min(1),
  report_out_mode: z.enum(['file', 'print']),
  barcode: z.string().optional() // ‚úÖ este es el cambio
});

// Main PDF request schema
export const pdfRequestSchema = z.object({
  core_report_info: coreReportInfoSchema
}).passthrough(); // allow additional properties

// src/validators/zplRequestSchema.ts
import { z } from 'zod';

// Define the structure of core_report_info
export const coreReportInfoSchema = z.object({
  report_name: z.string().min(1),
  report_description: z.string().min(1),
  report_template: z.string().min(1),
  report_version: z.string().regex(/^\d+\.\d+\.\d+$/),
  //report_file_name: z.string().min(1),
  //report_out_mode: z.enum(['file', 'print']),
  //barcode: z.string().optional() // ‚úÖ este es el cambio
});

// Main ZPL request schema
export const zplRequestSchema = z.object({
  core_report_info: coreReportInfoSchema
}).passthrough(); // allow additional properties



// src/config/users.json
{
    "admin": {
      "password": "$2a$12$XovTOF4pkAY7RgW2VabJceEgNQPk.TjDmWxbwno5fWaL2wgHmBwjy", 
      "role": "admin"
    }
  }
  
// deploy/windows/core-services.bat
@echo off
REM =====================================================
REM CORE-SERVICES - UNIFIED WINDOWS SERVICE MANAGER
REM Run as Administrator for install/uninstall operations
REM =====================================================

echo ================================================
echo CORE SERVICES - WINDOWS SERVICE MANAGER
echo ================================================
echo.

REM Get command from first argument
set COMMAND=%1
set GPG_PASS=%2

REM Show help if no command provided
if "%COMMAND%"=="" goto :show_help

REM Navigate to core-services root
cd /d "%~dp0\..\.."

REM Execute the appropriate command
if /i "%COMMAND%"=="install" goto :install
if /i "%COMMAND%"=="uninstall" goto :uninstall
if /i "%COMMAND%"=="start" goto :start
if /i "%COMMAND%"=="stop" goto :stop
if /i "%COMMAND%"=="restart" goto :restart
if /i "%COMMAND%"=="status" goto :status
if /i "%COMMAND%"=="help" goto :show_help

echo ERROR: Unknown command '%COMMAND%'
echo.
goto :show_help

REM =====================================================
REM INSTALL SERVICE
REM =====================================================
:install
echo [INSTALL] Starting Core Services installation...
echo.

REM Check if GPG passphrase was provided
if "%GPG_PASS%"=="" (
    echo ERROR: GPG passphrase is required for installation
    echo.
    echo Usage: core-services.bat install "your-gpg-passphrase"
    echo.
    echo Example:
    echo   core-services.bat install "my$ecureP@ssphrase123"
    echo.
    goto :end
)

REM Check if service already exists
sc query "Core Services" >nul 2>&1
if not errorlevel 1 (
    echo ERROR: Service already installed
    echo Run 'core-services.bat uninstall' first
    goto :end
)

echo [1/6] Verifying location...
if not exist "package.json" (
    echo ERROR: package.json not found
    echo Run this script from deploy/windows/ folder
    goto :end
)

echo [2/6] Checking Node.js...
node --version >nul 2>&1
if errorlevel 1 (
    echo ERROR: Node.js is not installed
    echo Download from: https://nodejs.org
    goto :end
)
node --version

echo [3/6] Building TypeScript project...
call npm run build
if errorlevel 1 (
    echo ERROR: Build failed
    goto :end
)

echo [4/6] Installing production dependencies...
call npm install --production --silent

echo [5/6] Installing node-windows...
call npm install node-windows --silent

echo [6/6] Creating Windows service with GPG passphrase...
echo GPG passphrase will be securely stored in the service configuration

REM Create a temporary JS file to handle the complex service creation
echo const Service = require('node-windows').Service; > temp_install.js
echo const path = require('path'); >> temp_install.js
echo. >> temp_install.js
echo const svc = new Service({ >> temp_install.js
echo   name: 'Core Services', >> temp_install.js
echo   description: 'Core Services API - Email, PDF, ZPL generation', >> temp_install.js
echo   script: path.join(__dirname, 'dist', 'scripts', 'start.js'), >> temp_install.js
echo   nodeOptions: ['--max-old-space-size=1024'], >> temp_install.js
echo   env: [ >> temp_install.js
echo     { name: 'NODE_ENV', value: 'production' }, >> temp_install.js
echo     { name: 'CLIENT_ID', value: 'core-dev' }, >> temp_install.js
echo     { name: 'GPG_PASSPHRASE', value: '%GPG_PASS%' }, >> temp_install.js
echo     { name: 'HOST', value: '0.0.0.0' }, >> temp_install.js
echo     { name: 'LOG_LEVEL', value: 'INFO' } >> temp_install.js
echo   ] >> temp_install.js
echo }); >> temp_install.js
echo. >> temp_install.js
echo svc.on('install', () =^> { >> temp_install.js
echo   console.log('‚úÖ Service installed successfully'); >> temp_install.js
echo   console.log('üìù GPG passphrase has been configured'); >> temp_install.js
echo   console.log('üîê The passphrase is stored securely in Windows service registry'); >> temp_install.js
echo   console.log('üöÄ Starting service...'); >> temp_install.js
echo   setTimeout(() =^> svc.start(), 2000); >> temp_install.js
echo }); >> temp_install.js
echo. >> temp_install.js
echo svc.on('start', () =^> { >> temp_install.js
echo   console.log('‚úÖ Service started successfully'); >> temp_install.js
echo   console.log('üåê API available at: http://localhost:3001'); >> temp_install.js
echo   console.log(''); >> temp_install.js
echo   console.log('================================================'); >> temp_install.js
echo   console.log('INSTALLATION COMPLETED'); >> temp_install.js
echo   console.log('================================================'); >> temp_install.js
echo   console.log(''); >> temp_install.js
echo   console.log('The GPG passphrase has been permanently configured.'); >> temp_install.js
echo   console.log('The service will use it automatically on every restart.'); >> temp_install.js
echo   console.log(''); >> temp_install.js
echo   console.log('To verify installation:'); >> temp_install.js
echo   console.log('  core-services.bat status'); >> temp_install.js
echo   console.log(''); >> temp_install.js
echo   console.log('To change the GPG passphrase later:'); >> temp_install.js
echo   console.log('  1. Uninstall: core-services.bat uninstall'); >> temp_install.js
echo   console.log('  2. Reinstall: core-services.bat install "new-passphrase"'); >> temp_install.js
echo }); >> temp_install.js
echo. >> temp_install.js
echo svc.on('error', err =^> { >> temp_install.js
echo   console.error('‚ùå Installation error:', err.message); >> temp_install.js
echo }); >> temp_install.js
echo. >> temp_install.js
echo svc.install(); >> temp_install.js

REM Execute the installation
node temp_install.js

REM Clean up temp file
del temp_install.js

goto :end

REM =====================================================
REM UNINSTALL SERVICE
REM =====================================================
:uninstall
echo [UNINSTALL] Removing Core Services...
echo.

set /p confirm="Are you sure? (Y/N): "
if /i not "%confirm%"=="Y" (
    echo Cancelled
    goto :end
)

echo Stopping service...
net stop "Core Services" >nul 2>&1

echo Installing node-windows if needed...
npm install node-windows --silent >nul 2>&1

echo Uninstalling service...
node -e "const Service=require('node-windows').Service;const path=require('path');const svc=new Service({name:'Core Services',script:path.join(__dirname,'dist','scripts','start.js')});svc.on('uninstall',()=>{console.log('‚úÖ Service uninstalled successfully');console.log('üîê GPG passphrase configuration has been removed')});svc.uninstall();"

echo.
echo ‚úÖ Uninstall completed
echo üîê All configuration (including GPG passphrase) has been removed
goto :end

REM =====================================================
REM START SERVICE
REM =====================================================
:start
echo [START] Starting Core Services...
net start "Core Services"
if errorlevel 1 (
    echo ERROR: Could not start service
    echo.
    echo Possible causes:
    echo - Service is not installed (run: core-services.bat install "gpg-passphrase")
    echo - Service is already running
    echo - Check Event Viewer for details
) else (
    echo ‚úÖ Service started
    echo üîê Using stored GPG passphrase
    timeout /t 3 /nobreak >nul
    echo.
    echo Testing API...
    curl -s http://localhost:3001/health >nul 2>&1
    if errorlevel 1 (
        echo ‚ö†Ô∏è  API not responding yet (give it 30-60 seconds)
    ) else (
        echo ‚úÖ API is working
    )
)
goto :end

REM =====================================================
REM STOP SERVICE
REM =====================================================
:stop
echo [STOP] Stopping Core Services...
net stop "Core Services"
if errorlevel 1 (
    echo ERROR: Could not stop service
    echo Service might not be running or not installed
) else (
    echo ‚úÖ Service stopped
)
goto :end

REM =====================================================
REM RESTART SERVICE
REM =====================================================
:restart
echo [RESTART] Restarting Core Services...
net stop "Core Services" >nul 2>&1
timeout /t 5 /nobreak >nul
net start "Core Services"
if errorlevel 1 (
    echo ERROR: Could not restart service
) else (
    echo ‚úÖ Service restarted
    echo üîê Using stored GPG passphrase
    timeout /t 10 /nobreak >nul
    curl -s http://localhost:3001/health >nul 2>&1
    if errorlevel 1 (
        echo ‚ö†Ô∏è  API starting up...
    ) else (
        echo ‚úÖ API is working
    )
)
goto :end

REM =====================================================
REM STATUS CHECK
REM =====================================================
:status
echo [STATUS] Checking Core Services...
echo.

REM Check if service exists
sc query "Core Services" >nul 2>&1
if errorlevel 1 (
    echo ‚ùå Service not installed
    echo.
    echo To install:
    echo   core-services.bat install "your-gpg-passphrase"
    echo.
    goto :end
)

REM Show service status
echo Service Status:
sc query "Core Services" | find "STATE"
echo.

REM Check GPG configuration
echo Configuration:
echo - GPG passphrase: [CONFIGURED - Hidden for security]
echo - Client ID: core-dev
echo - Port: 3001
echo.

REM Test API
echo Testing API...
curl -s -w "Response time: %%{time_total}s\n" http://localhost:3001/health 2>nul
if errorlevel 1 (
    echo ‚ùå API not responding
    echo.
    echo Troubleshooting:
    echo - Wait 30-60 seconds if service just started
    echo - Check Event Viewer for errors
    echo - Verify GPG passphrase is correct
) else (
    echo ‚úÖ API is working
)
echo.

echo Logs: Event Viewer ^> Application ^> "Core Services"
goto :end

REM =====================================================
REM SHOW HELP
REM =====================================================
:show_help
echo Usage: core-services.bat [command] [options]
echo.
echo Commands:
echo   install "passphrase" - Install service with GPG passphrase
echo   uninstall           - Remove Core Services
echo   start               - Start the service
echo   stop                - Stop the service
echo   restart             - Restart the service
echo   status              - Check service status
echo   help                - Show this help
echo.
echo Examples:
echo   core-services.bat install "my$ecureP@ssphrase123"
echo   core-services.bat status
echo   core-services.bat restart
echo.
echo Notes:
echo   - The GPG passphrase is required only during installation
echo   - It will be stored securely in the Windows service configuration
echo   - The service will use it automatically on every restart
echo   - To change the passphrase, uninstall and reinstall the service
echo.

:end
pause
// deploy/windows/install-service.bat
@echo off
REM =====================================================
REM CORE-SERVICES WINDOWS SERVICE INSTALLER
REM Works from any directory - Run as Administrator
REM =====================================================

echo ================================================
echo CORE-SERVICES - WINDOWS SERVICE INSTALLER
echo ================================================

REM Get current directory (where this script is located)
set "SCRIPT_DIR=%~dp0"
set "SCRIPT_DIR=%SCRIPT_DIR:~0,-1%"

REM Navigate to core-services root (two levels up from deploy/windows/)
set "SERVICE_DIR=%SCRIPT_DIR%\..\.."
cd /d "%SERVICE_DIR%"

echo Installing from: %CD%
echo.

echo [1/6] Verifying location...
if not exist "package.json" (
    echo ERROR: package.json not found
    echo This script must be run from deploy/windows/ folder
    echo Current directory: %CD%
    pause
    exit /b 1
)

echo [2/6] Checking Node.js installation...
node --version >nul 2>&1
if errorlevel 1 (
    echo ERROR: Node.js is not installed
    echo Download from: https://nodejs.org
    pause
    exit /b 1
)
node --version

echo [3/6] Building TypeScript project...
npm run build
if errorlevel 1 (
    echo ERROR: Failed to build project
    echo Make sure 'npm run build' works correctly
    pause
    exit /b 1
)

echo [4/6] Installing production dependencies...
npm install --production --silent

echo [5/6] Installing node-windows...
npm install node-windows --silent

echo [6/6] Creating Windows service...
REM Create service using inline Node.js command
node -e "const Service=require('node-windows').Service;const path=require('path');console.log('Creating service...');const svc=new Service({name:'Core Services',description:'Core Services API - Email, PDF, ZPL generation service',script:path.join(__dirname,'dist','app.js'),nodeOptions:['--max-old-space-size=1024'],env:[{name:'NODE_ENV',value:'production'},{name:'CONFIG_MODE',value:'standalone'},{name:'CLIENT_ID',value:'core-dev'},{name:'GPG_PASSPHRASE',value:'CHANGE_THIS_GPG_PASSPHRASE'}],logOnAs:{domain:'workgroup',account:'LocalSystem',password:''}});svc.on('install',()=>{console.log('‚úÖ Service installed successfully');console.log('üöÄ Starting service...');setTimeout(()=>svc.start(),2000);});svc.on('start',()=>{console.log('üü¢ Service started successfully');console.log('üåê API available at: http://localhost:3001');console.log('üìä View logs in: Event Viewer > Application');console.log('');console.log('================================================');console.log('INSTALLATION COMPLETED');console.log('================================================');});svc.on('error',err=>{console.error('‚ùå Error:',err.message);});if(!require('fs').existsSync(path.join(__dirname,'dist','app.js'))){console.error('‚ùå ERROR: dist/app.js not found');console.error('   Run: npm run build');process.exit(1);}svc.install();"

echo.
echo ================================================
echo SERVICE INSTALLATION COMPLETED
echo ================================================
echo Location: %CD%
echo.
echo ‚ö†Ô∏è IMPORTANT: Update GPG_PASSPHRASE
echo   1. Open services.msc
echo   2. Find "Core Services" ‚Üí Properties
echo   3. Log On tab ‚Üí Environment variables
echo   4. Change GPG_PASSPHRASE value
echo.
echo To check service status:
echo   services.msc ‚Üí "Core Services"
echo.
echo To view logs:
echo   Event Viewer ‚Üí Application ‚Üí Filter by "Core Services"
echo.
echo To manage the service use:
echo   start-service.bat
echo   stop-service.bat
echo   restart-service.bat
echo   status-service.bat
echo.
echo To test API:
echo   http://localhost:3001/health
echo.
pause
// deploy/windows/restart-service.bat
REM =====================================================
REM start-service.bat
REM =====================================================
@echo off
echo Starting Core Services...
net start "Core Services"
if errorlevel 1 (
    echo ERROR: Could not start service
    echo Check services.msc for details
) else (
    echo ‚úÖ Service started successfully
    echo üåê API available at: http://localhost:3001/health
    timeout /t 3 /nobreak >nul
    echo üîç Testing API...
    curl -s http://localhost:3001/health >nul 2>&1
    if errorlevel 1 (
        echo ‚ö†Ô∏è API not responding yet, give it a few more seconds
    ) else (
        echo ‚úÖ API working correctly
    )
)
pause

REM =====================================================
REM stop-service.bat
REM =====================================================
@echo off
echo Stopping Core Services...
net stop "Core Services"
if errorlevel 1 (
    echo ERROR: Could not stop service
    echo Service might already be stopped
) else (
    echo ‚úÖ Service stopped successfully
)
pause

REM =====================================================
REM restart-service.bat
REM =====================================================
@echo off
echo ================================================
echo RESTARTING CORE SERVICES
echo ================================================
echo.

echo [1/3] Stopping service...
net stop "Core Services" >nul 2>&1

echo [2/3] Waiting 5 seconds...
timeout /t 5 /nobreak >nul

echo [3/3] Starting service...
net start "Core Services"
if errorlevel 1 (
    echo ERROR: Could not restart service
    echo Check services.msc or Event Viewer for details
    pause
    exit /b 1
)

echo ‚úÖ Service restarted successfully
echo.
echo üîç Verifying API response...
timeout /t 10 /nobreak >nul

curl -s http://localhost:3001/health >nul 2>&1
if errorlevel 1 (
    echo ‚ö†Ô∏è Service started but API not responding
    echo Check Event Viewer for error details
    echo Service might need more time to initialize
) else (
    echo ‚úÖ API working correctly
    echo üåê http://localhost:3001/health
)
echo.
pause

REM =====================================================
REM status-service.bat
REM =====================================================
@echo off
echo ================================================
echo CORE SERVICES STATUS
echo ================================================
echo Current directory: %CD%
echo.

REM Check if service exists
sc query "Core Services" >nul 2>&1
if errorlevel 1 (
    echo ‚ùå Service not installed
    echo Run install-service.bat first
    goto :end
)

REM Show service status
echo üîç Service status:
sc query "Core Services" | find "STATE"

REM Test API
echo.
echo üîç Testing API...
curl -s -w "Response time: %%{time_total}s\n" http://localhost:3001/health 2>nul
if errorlevel 1 (
    echo ‚ùå API not responding
    echo.
    echo Possible causes:
    echo - Service is stopped
    echo - Service is starting up
    echo - Configuration error
    echo.
    echo Check Event Viewer for more details
) else (
    echo ‚úÖ API working correctly
)

:end
echo.
echo For detailed logs:
echo   Event Viewer ^> Application ^> Filter by "Core Services"
echo.
echo For service management:
echo   services.msc ^> "Core Services"
echo.
pause

REM =====================================================
REM uninstall-service.bat
REM =====================================================
@echo off
echo ================================================
echo ‚ö†Ô∏è UNINSTALLING CORE SERVICES
echo ================================================
echo Current directory: %CD%
echo.

REM Verify we're in the correct location
cd /d "%~dp0\..\.."
if not exist "package.json" (
    echo ERROR: Run from deploy/windows/ folder
    echo package.json not found in: %CD%
    pause
    exit /b 1
)

set /p confirm="Are you sure you want to uninstall the service? (Y/N): "
if /i not "%confirm%"=="Y" (
    echo Operation cancelled
    pause
    exit /b
)

echo.
echo [1/3] Stopping service...
net stop "Core Services" >nul 2>&1

echo [2/3] Checking node-windows...
npm list node-windows >nul 2>&1
if errorlevel 1 (
    echo Installing node-windows...
    npm install node-windows --silent
)

echo [3/3] Uninstalling service...
REM Uninstall service using inline Node.js command
node -e "const Service=require('node-windows').Service;const path=require('path');console.log('Uninstalling service...');const svc=new Service({name:'Core Services',script:path.join(__dirname,'dist','app.js')});svc.on('uninstall',()=>{console.log('‚úÖ Service uninstalled successfully');console.log('üßπ Cleanup completed');console.log('');console.log('The Core Services service has been removed from the system');console.log('You can manually delete this folder if desired');});svc.on('error',err=>{console.error('‚ùå Uninstall error:',err.message);});svc.uninstall();"

echo.
echo ‚úÖ Uninstallation process completed
echo.
pause
// deploy/windows/start-service.bat
@echo off
REM =====================================================
REM CORE-SERVICES - CHECK SERVICE STATUS
REM Run from deploy/windows/ folder
REM =====================================================

echo ================================================
echo CORE SERVICES STATUS CHECK
echo ================================================
echo Current directory: %CD%
echo Timestamp: %DATE% %TIME%
echo.

REM Check if service exists
echo üîç Checking if service is installed...
sc query "Core Services" >nul 2>&1
if errorlevel 1 (
    echo ‚ùå Service not installed
    echo.
    echo To install the service:
    echo   run install-service.bat as Administrator
    echo.
    goto :end
)

echo ‚úÖ Service is installed
echo.

REM Show detailed service status
echo üîç Service status:
for /f "tokens=4" %%i in ('sc query "Core Services" ^| find "STATE"') do set SERVICE_STATE=%%i
echo    State: %SERVICE_STATE%

sc query "Core Services" | find "STATE"
echo.

REM Test API if service is running
if /i "%SERVICE_STATE%"=="RUNNING" (
    echo üåê Testing API connectivity...
    
    REM Test with timeout and response time
    curl -s -w "   Response time: %%{time_total}s" -m 10 http://localhost:3001/health 2>nul
    if errorlevel 1 (
        echo ‚ùå API not responding
        echo.
        echo Possible issues:
        echo - Service is starting up ^(wait 30-60 seconds^)
        echo - Configuration error
        echo - Port 3001 is blocked
        echo - Application crashed after service start
        echo.
        echo Check Event Viewer for error details
    ) else (
        echo.
        echo ‚úÖ API is working correctly
        echo üåê Full API URL: http://localhost:3001/health
    )
) else (
    echo ‚ö†Ô∏è Service is not running - skipping API test
    echo.
    echo To start the service:
    echo   start-service.bat
)

echo.
echo üìä Additional Information:
echo    Service name: Core Services
echo    Display name: Core Services
echo    Port: 3001
echo    Startup type: Automatic
echo.

:end
echo üìã Management Commands:
echo    start-service.bat    - Start the service
echo    stop-service.bat     - Stop the service  
echo    restart-service.bat  - Restart the service
echo    status-service.bat   - This status check
echo.
echo üìä Monitoring:
echo    services.msc                           - Windows Service Manager
echo    Event Viewer ^> Application ^> Core Services - Service logs
echo    http://localhost:3001/health           - API health check
echo.
pause
// deploy/windows/status-service.bat
REM status-service.bat  
@echo off
echo ================================================
echo STATUS OF CORE SERVICES
echo ================================================
echo Current Folder: %CD%
echo.

REM Verificar si el servicio existe
sc query "Core Services" >nul 2>&1
if errorlevel 1 (
    echo ‚ùå Service "Core Services" not installed.
    echo Execute install-service.bat to install it.
    goto :end
)

REM Mostrar estado del servicio
echo üîç Service status:
sc query "Core Services" | find "STATE"

REM Probar la API
echo.
echo üîç Checking API health...
curl -s -w "Response time: %%{time_total}s\n" http://localhost:3001/health 2>nul
if errorlevel 1 (
    echo ‚ùå API is not responding.
    echo.
    echo Possible reasons:
    echo - Service is not running
    echo - Service is starting
    echo - Configuration error
    echo - Check Event Viewer for more details
) else (
    echo ‚úÖ API funcionando correctamente
)

:end
echo.
echo To see logs:
echo   Event Viewer ^> Application ^> Filter by "Core Services"
echo.
echo To manage the service:
echo   services.msc ^> "Core Services"
echo.
pause
// deploy/windows/stop-service.bat
REM stop-service.bat  
@echo off
echo Stopping Core Services...
net stop "Core Services"
if errorlevel 1 (
    echo ERROR: Service could not be stopped
    echo Service may be already stopped or does not exist.
    echo If you want to uninstall the service, run:
    echo uninstall-service.bat
    echo If you want to start the service again, run:
    echo start-service.bat
    echo You can also check the service status using:
    echo sc query "Core Services"    
) else (
    echo ‚úÖ Service stopped successfully.
    echo You can now safely uninstall the service if needed.
    echo If you want to uninstall the service, run:
    echo uninstall-service.bat
    echo If you want to start the service again, run:
    echo start-service.bat
)
pause
// deploy/windows/uninstall-service.bat
@echo off
REM =====================================================
REM CORE-SERVICES WINDOWS SERVICE UNINSTALLER
REM Run as Administrator from deploy/windows/ folder
REM =====================================================

echo ================================================
echo ‚ö†Ô∏è UNINSTALLING CORE SERVICES
echo ================================================

REM Get current directory and navigate to core-services root
set "SCRIPT_DIR=%~dp0"
cd /d "%SCRIPT_DIR%\..\.."

echo Current directory: %CD%
echo.

REM Verify we're in the correct location
if not exist "package.json" (
    echo ERROR: package.json not found
    echo This script must be run from deploy/windows/ folder
    echo Current directory: %CD%
    pause
    exit /b 1
)

echo Found core-services project at: %CD%
echo.

set /p confirm="Are you sure you want to uninstall the Core Services Windows Service? (Y/N): "
if /i not "%confirm%"=="Y" (
    echo Operation cancelled
    pause
    exit /b
)

echo.
echo [1/3] Stopping service...
net stop "Core Services" >nul 2>&1
if errorlevel 1 (
    echo Service was already stopped or not found
) else (
    echo Service stopped successfully
)

echo [2/3] Checking node-windows dependency...
npm list node-windows >nul 2>&1
if errorlevel 1 (
    echo Installing node-windows...
    npm install node-windows --silent
    if errorlevel 1 (
        echo ERROR: Failed to install node-windows
        echo Make sure npm is working correctly
        pause
        exit /b 1
    )
) else (
    echo node-windows is available
)

echo [3/3] Uninstalling Windows service...
REM Uninstall service using inline Node.js command (no separate .js files needed)
node -e "const Service=require('node-windows').Service;const path=require('path');console.log('üóëÔ∏è Uninstalling Core Services...');const svc=new Service({name:'Core Services',script:path.join(__dirname,'dist','app.js')});svc.on('uninstall',()=>{console.log('‚úÖ Service uninstalled successfully');console.log('üßπ Windows Service cleanup completed');console.log('');console.log('================================================');console.log('UNINSTALLATION COMPLETED');console.log('================================================');console.log('');console.log('The Core Services Windows Service has been removed');console.log('from the system. You can manually delete this folder');console.log('if you no longer need the application.');console.log('');});svc.on('error',err=>{console.error('‚ùå Uninstall error:',err.message);console.error('You may need to remove the service manually using:');console.error('sc delete \"Core Services\"');});svc.uninstall();"

if errorlevel 1 (
    echo.
    echo ‚ö†Ô∏è Uninstall script encountered an error
    echo You may need to remove the service manually:
    echo   sc delete "Core Services"
    echo.
    pause
    exit /b 1
)

echo.
echo ‚úÖ Uninstallation process completed successfully
echo.
echo The "Core Services" Windows Service has been removed.
echo Event Viewer logs may still contain historical entries.
echo.
echo To completely remove the application:
echo   1. Delete this entire folder
echo   2. Remove any shortcuts or references
echo.
pause
